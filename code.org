
#+PROPERTY: header-args:bash+ :comments both :noweb yes :eval no-export
#+PROPERTY: header-args:bash+ :session (concat "*" (file-name-sans-extension (buffer-name)) "-shell*")
#+PROPERTY: header-args:bash+ :tangle-mode (identity #o544) :shebang #!/usr/bin/env bash
#+PROPERTY: header-args:jupyter-python :kernel TMB :session TMB

* Sectors

Here "sectors" refers both to the citet:zwally_2012 outlines and the citet:mouginot_2019_glacier basins and regions.

Procedure to map the sectors to the SMB grids:
+ Sectors are in EPSG:4326 projection, and need to be re-projected to the SMB projection
  + HIRHAM :: rotated pole
  + MAR :: Oblique stereographic 
  + RACMO :: EPSG:3413

SMB grids do not match sectors perfectly. When the sectors are larger, that is fine - whatever the SMB is in the sector interior is the modeled SMB. It is problematic where the sectors are smaller than the model domain. Because the largest SMB losses are at the edge, if some RCM pixels are outside of a sector and not counted, this would introduce large errors.
+ For each SMB, determine the mask
  + HIRHAM ::
  + MAR ::
  + RACMO :: No mask provided. Look at summer season and assume any cell with any values != 0 is in the model domain, and all cells summed = 0 is outside
+ For all model domain cells outside of a sector, enlarge the sector to include them, and join the new sector cells to the nearest sector cell that is inside the model domain

** Mouginot 2019

#+NAME: import_mouginot
#+BEGIN_SRC bash

log_info "Loading Mouginot 2019"

g.mapset -c Mouginot_2019

v.import input=${DATADIR}/Mouginot_2019/Greenland_Basins_PS_v1.4.2.shp output=basins snap=1

# remove peripheral ice caps
db.execute sql="DELETE FROM basins WHERE name LIKE 'ICE_CAPS_%'"

v.db.addcolumn map=basins columns="SUBREGION1_num INT,cat_ INT"

db.execute sql="UPDATE basins SET cat_=cat"

# convert NW NO NE CW CE SW SE based on clock
db.execute sql="UPDATE basins SET SUBREGION1_num=11 WHERE SUBREGION1='NW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=12 WHERE SUBREGION1='NO'"
db.execute sql="UPDATE basins SET SUBREGION1_num=1 WHERE SUBREGION1='NE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=3 WHERE SUBREGION1='CE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=9 WHERE SUBREGION1='CW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=5 WHERE SUBREGION1='SE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=7 WHERE SUBREGION1='SW'"
#+END_SRC
** Zwally 2012

#+NAME: import_zwally
#+BEGIN_SRC bash

log_info "Loading Zwally 2012"

g.mapset -c Zwally_2012
v.import input=${DATADIR}/Zwally_2012/sectors output=sectors snap=1
#+END_SRC

* HIRHAM
:PROPERTIES:
:header-args:bash+: :tangle HIRHAM.sh
:END:

** Init

#+BEGIN_SRC bash :tangle no
grass -c EPSG:4326 G_HIRHAM

# from gridOBC_SD.nc
ncdump -v rlon ~/data/HIRHAM/gridOBC_SD.nc
ncdump -v rlat ~/data/HIRHAM/gridOBC_SD.nc
#+END_SRC

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC


** Domain

#+BEGIN_SRC bash
g.region w=-13.65 e=1.3 s=-9.8 n=12.2 res=0:03
g.region w=w-0.025 e=e+0.025 s=s-0.025 n=n+0.025 -p
g.region save=RCM

g.region res=0:0.5 # ~1 km grid cell resolution
g.region save=sectors
#+END_SRC

*** Test 

#+BEGIN_SRC bash :tangle no
r.in.gdal -ol input=NetCDF:${DATADIR}/HIRHAM/daily/DarcySensitivity_RP810_Daily2D_GL2LIN_Darcy_60m_liqCL_wh1_smb_HydroYr_2012_2013_DM_SD.nc:smb band=200 output=smb
r.region map=smb region=RCM

v.in.ogr -o input=./dat/Zrot.gpkg output=Z
v.in.ogr -o input=./dat/Mrot.gpkg output=M

d.mon start=wx0
r.colors map=smb color=viridis
d.erase
d.rast smb
d.vect Z fill_color=none
d.vect M fill_color=none
d.grid 1:0:0
#+END_SRC


** Convert sectors to rotated pole

+ https://lists.osgeo.org/pipermail/grass-user/2011-October/062180.html
+ This no longer works in GRASS 7.8 https://lists.osgeo.org/pipermail/grass-user/2020-November/081828.html
+ Therefore, the following is done 1x in a VM (Ubuntu 18.04) running GRASS 7.4

#+BEGIN_SRC bash :results verbatim :tangle no
rm -fR Gnorm Grot

grass -c EPSG:4326 Gnorm

DATADIR=~/data

v.import input=${DATADIR}/Zwally_2012/sectors output=Z
v.import snap=1 input=${DATADIR}/Mouginot_2019/Greenland_Basins_PS_v1.4.2.shp output=M

cat << EOF > ./Gnorm/PERMANENT/PROJ_INFO
name: General Oblique Transformation
datum: wgs84
towgs84: 0.000,0.000,0.000
proj: ob_tran
o_proj: latlon
ellps: wgs84
a: 6378137.0000000000
es: 0.0066943800
f: 298.2572235630
lat_0: 0.0000000000
lon_0: 180.0000000000
o_lat_p: 18.0
o_lon_p: -200.0
EOF

# rotated_pole:grid_north_pole_latitude = 18. ;
# rotated_pole:grid_north_pole_longitude = -200.

cat << EOF > ./Gnorm/PERMANENT/PROJ_UNITS
unit: degree
units: degrees
meters: .0174532925
EOF

grass -e -c EPSG:4326 Grot
grass ./Grot/PERMANENT

v.proj location=Gnorm input=Z
v.proj location=Gnorm input=M

# g.region vector=Z,M
# d.mon start=wx0
# d.erase
# d.vect Z
# d.vect M
# d.grid 1:0:0

v.out.ogr input=Z output=./dat/Zrot.gpkg
v.out.ogr input=M output=./dat/Mrot.gpkg
#+END_SRC

** Load sectors

This should be:

#+BEGIN_SRC bash :tangle no
<<import_mouginot>>
<<import_zwally>>
#+END_SRC

But we're loading different files that have been converted to the rotated pole, so here I **duplicate** those code blocks but change the input filename.

#+BEGIN_SRC bash
log_info "Loading Zwally 2012"

g.mapset -c Zwally_2012
v.in.ogr -o input=Zwally_2012_HIRHAM.gpkg output=sectors

v.db.dropcolumn map=sectors columns="cat_"
v.db.renamecolumn map=sectors column=cat__1,cat_
#+END_SRC

#+BEGIN_SRC bash

log_info "Loading Mouginot 2019"

g.mapset -c Mouginot_2019

v.in.ogr -o input=Mouginot_2019_HIRHAM.gpkg output=basins

# remove peripheral ice caps
db.execute sql="DELETE FROM basins WHERE name LIKE 'ICE_CAPS_%'"

v.db.addcolumn map=basins columns="SUBREGION1_num INT"

# convert NW NO NE CW CE SW SE based on clock
db.execute sql="UPDATE basins SET SUBREGION1_num=11 WHERE SUBREGION1='NW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=12 WHERE SUBREGION1='NO'"
db.execute sql="UPDATE basins SET SUBREGION1_num=1 WHERE SUBREGION1='NE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=3 WHERE SUBREGION1='CE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=9 WHERE SUBREGION1='CW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=5 WHERE SUBREGION1='SE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=7 WHERE SUBREGION1='SW'"

g.mapset PERMANENT
#+END_SRC
 

** Find model ice domain

+ Use all cells != 0 for sum of 2020 JAS as model domain

#+BEGIN_SRC bash
r.in.gdal -ol input="NetCDF:${DATADIR}/HIRHAM/ZwallyMasks_all_SD.nc:glacmask" output=mask
r.region map=mask region=RCM
r.mapcalc "mask_ice_all = if(mask == 1, 1, null())"

r.clump input=mask_ice_all output=mask_ice_clump
main_clump=$(r.stats -c -n mask_ice_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if(mask_ice_clump == ${main_clump}, 1, null())"
#+END_SRC

** Expand sectors to cover model domain

We'll develop this here on the HIRHAM data 1x, but do so generically so that when working with MAR and RACMO we can just <<expand_sectors>>. The one requirement is that we expect a "mask_ice" raster in the PERMANENT mapset which defines the ice domain.

#+NAME: expand_sectors
#+BEGIN_SRC bash :tangle no
<<expand_zwally>>
<<expand_mouginot>>
#+END_SRC

*** Zwally

#+NAME: expand_zwally
#+BEGIN_SRC bash

log_info "Expanding Zwally sectors to cover RCM domain"

g.mapset Zwally_2012
g.region region=sectors

v.to.rast input=sectors output=sectors use=attr attribute_column=cat_
# r.mapcalc "outside = if(isnull(sectors) & not(isnull(mask_ice)), 1, null())"

# Works fine if limited to contiguous cells (hence main_clump).
# Deosn't work great for distal islands (e.g. NE GL).
# Probably need to jump to v.distance or r.distance if we want to assign these to sectors.
r.grow.distance input=sectors value=value
r.mapcalc "sectors_e = if(mask_ice, int(value), null())" # sectors_enlarged
#+END_SRC


*** Mouginot

#+NAME: expand_mouginot
#+BEGIN_SRC bash

log_info "Expanding Mouginot basins to cover RCM domain"

g.mapset Mouginot_2019
g.region region=sectors

v.to.rast input=basins output=basins use=attr attribute_column=cat_ labelcolumn=SUBREGION1
# r.mapcalc "outside = if(isnull(basins) & mask_ice, 1, null())"
r.grow.distance input=basins value=value_b
r.mapcalc "basins_e = if(mask_ice, int(value_b), null())"
r.category map=basins separator=":" > ./tmp/basins_cats
r.category map=basins_e separator=":" rules=./tmp/basins_cats

v.to.rast input=basins output=regions use=attr attribute_column=SUBREGION1_num labelcolumn=SUBREGION1
# r.mapcalc "outside = if(isnull(regions) & mask_ice, 1, null())"
r.grow.distance input=regions value=value_r
r.mapcalc "regions_e = if(mask_ice, int(value_r), null())"
r.category map=regions separator=":" > ./tmp/region_cats
r.category map=regions_e separator=":" rules=./tmp/region_cats
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_HIRHAM/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:




* MAR
:PROPERTIES:
:header-args:bash+: :tangle MAR.sh
:END:
** Init

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC

** INFO MAR projection

From XF:

#+BEGIN_QUOTE
These outputs are on the MAR native grid using a Stereographic Oblique Projection with 70.5°N, 40°W as center.
#+END_QUOTE

#+BEGIN_SRC bash :tangle no
grass -e -c G_tmp --exec g.proj -c location=G_MAR proj4="+proj=sterea +lat_0=90 +lat_ts=70.5 +lon_0=-40 +k=1 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs"
rm -fR G_tmp
grass ./G_MAR/PERMANENT
#+END_SRC

** Find domain boundary

#+BEGIN_QUOTE
Y21_155 and X12_84 are the X/Y axis from MAR after a selection by NOAA
FERRET of a smaller domain covering Greenland only within the MAR 
initial integration domain.
The centre of the projection in the reduced grid corresponds to the
pixel (39,60).

The dimension of the original grid is 95 x 165 and the centre of the
projection is the pixel (50,80).
I'm not a specialist of the projections and I can't help to resolve
your problem. But as it comes from a old Fortran code into, it is
likely that there are some errors explaining the differences you have.
#+END_QUOTE

** Set up GRASS location

#+BEGIN_SRC bash
C=$(m.proj -i coordinates=-40,70.5) # center of center grid cell
echo $C ## 0.00|-2198452.92|0.00
# The following numbers come from $c(y) and the X12_84 and Y21_155 arrays in the MAR NetCDF files
g.region e=680000 w=-760000 s=$(( -1180000 - 2198452 )) n=$(( 1500000 - 2198452 )) rows=134 cols=72 -pl
g.region e=e+10000 w=w-10000 s=s-10000 n=n+10000 -pl # adjust from cell center to edges
g.region save=RCM

g.region res=1000 -p
g.region save=sectors
#+END_SRC

** Ice mask
#+BEGIN_SRC bash
r.external -o source=NetCDF:${DATADIR}/MAR/daily/MARv3.11-20km-daily-NCEP-NCARv1-2000.nc:MSK output=mask
r.region map=mask region=RCM
r.colors map=mask color=haxby

r.mapcalc "mask_ice_all = if(mask == 0, null(), 1)"

r.clump input=mask_ice_all output=mask_clump
main_clump=$(r.stats -c -n mask_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if((mask_clump == ${main_clump}) & (mask > 0.5), 1, null())"
#+END_SRC

** Sectors

#+BEGIN_SRC bash
<<import_zwally>>
<<expand_zwally>>

<<import_mouginot>>
<<expand_mouginot>>

g.mapset PERMANENT
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_MAR/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:



* RACMO
:PROPERTIES:
:header-args:bash+: :tangle RACMO.sh
:END:

** Set up GRASS location

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC

#+BEGIN_SRC bash :tangle no
cdo -sinfo ${DATADIR}/RACMO/daily/smb_rec.2020_JAS.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.DD.nc
# x : -638956 to 856044 by 1000 km
# y : -3354596 to -655596 by 1000 km
#+END_SRC

#+BEGIN_SRC bash
g.region w=-638956 e=856044 s=-3354596 n=-655596 res=1000 -p
g.region n=n+500 s=s-500 w=w-500 e=e+500 res=1000 -p
g.region save=RCM

g.region res=1000 -p
g.region save=sectors
#+END_SRC

** Ice mask
#+BEGIN_SRC bash
r.external -o source=NetCDF:${DATADIR}/RACMO/Icemask_Topo_Iceclasses_lon_lat_average_1km.nc:Promicemask output=mask
r.region map=mask region=RCM
r.colors map=mask color=haxby

r.mapcalc "mask_ice_all = if(mask >= 2, 1, null())"

r.clump input=mask_ice_all output=mask_clump
main_clump=$(r.stats -c -n mask_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if((mask_clump == ${main_clump}) & (mask > 0.5), 1, null())"
#+END_SRC

** Reproject sectors to RCM grid

+ Nothing to do here because RACMO on EPSG:3413 projection.

** Sectors

#+BEGIN_SRC bash
<<import_zwally>>
<<expand_zwally>>

<<import_mouginot>>
<<expand_mouginot>>

g.mapset PERMANENT
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_RACMO/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:




* SMB to sectors
** Print dates

#+BEGIN_SRC jupyter-python :tangle nc_dates.py
import xarray as xr
import sys

f = sys.argv[1]

ds = xr.open_dataset(f)

if 'time' in ds.variables:
    tvar = 'time'
if 'TIME' in ds.variables:
    tvar = 'TIME'
    
t = [str(_)[0:10] for _ in ds[tvar].values]
for _ in t: print(_)
#+END_SRC


** HIRHAM
#+BEGIN_SRC bash :results verbatim :tangle HIRHAM_to_sectors.sh

<<init_bash>>

RCM=HIRHAM
mkdir -p tmp/${RCM}

dir=${DATADIR}/${RCM}/daily
f=$(ls ${dir}/*.nc|head -n1) # debug

for f in ${dir}/*.nc; do
  dates=$(python ./nc_dates.py ${f})
  band=0
  d=1985-09-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 1 ))

    log_info "HIRHAM: ${d}"

    if [[ -e ./tmp/${RCM}/Z_${d}.bsv ]]; then continue; fi

    var=smb
    if [[ ${f} == *"SMBmodel"* ]]; then var=gld; fi

    r.in.gdal -ol input="NetCDF:${f}:${var}" band=${band} output=smb --o --q
    r.region map=smb region=RCM
    
    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Z_${d}.bsv

    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mb_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mr_${d}.bsv
    
  done
done
#+END_SRC

** MAR
#+BEGIN_SRC bash :results verbatim :tangle MAR_to_sectors.sh

<<init_bash>>

RCM=MAR
mkdir -p tmp/${RCM}

dir=${DATADIR}/${RCM}/daily
f=$(ls ${dir}/MARv3.11*.nc|head -n1) # debug


for f in ${dir}/MARv3.11*.nc; do
  dates=$(python ./nc_dates.py ${f})
  band=0
  d=1986-01-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 1 ))

    log_info "MAR: ${d}"

    if [[ -e ./tmp/${RCM}/Z_${d}.bsv ]]; then continue; fi

    r.in.gdal -o input="NetCDF:${f}:SMB" band=${band} output=smb_raw --o --q
    r.region map=smb_raw region=RCM
    r.mapcalc "smb = if(mask > 50, smb_raw * (mask/100), null())" --o

    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Z_${d}.bsv

    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mb_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mr_${d}.bsv
    
  done
done
#+END_SRC


** RACMO
#+BEGIN_SRC bash :results verbatim :tangle RACMO_to_sectors.sh

<<init_bash>>

RCM=RACMO
mkdir -p tmp/${RCM}

dir=${DATADIR}/${RCM}/daily
f=$(ls ${dir}/*.nc|head -n1) # debug

for f in ${dir}/*.nc; do
  dates=$(python ./nc_dates.py ${f})
  band=0
  d=1989-07-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 1 ))

    log_info "RACMO: ${d}"

    if [[ -e ./tmp/${RCM}/Z_${d}.bsv ]]; then continue; fi

    var=SMB_rec
    if [[ ${f} == *"ERA5"* ]]; then var=smb_rec; fi

    r.in.gdal -o input="NetCDF:${f}:${var}" band=${band} output=smb --o  --q
    r.region map=smb region=RCM

    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Z_${d}.bsv

    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mb_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/Mr_${d}.bsv
    
  done
done
#+END_SRC


** Merge

Raw (daily) BSV merged to one
#+BEGIN_SRC bash
for RCM in HIRHAM MAR RACMO; do
  for ROI in Z Mb Mr; do
    head -n1 ./tmp/${RCM}/${ROI}_2000-01-01.bsv > ./tmp/${RCM}_${ROI}.bsv
    tail -q -n1 ./tmp/${RCM}/${ROI}_*.bsv >> ./tmp/${RCM}_${ROI}.bsv
  done
done
#+END_SRC

#+RESULTS:

BSV to NetCDF

#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import numpy as np
import subprocess
import datetime
import glob
from tqdm import tqdm

time = pd.date_range(start = "1986-01-01",
                     end = datetime.datetime.now() + datetime.timedelta(days = 10),
                     freq = "D")

ds = xr.Dataset()
ds["time"] = (("time"), time)
ds["time"].attrs["cf_role"] = "timeseries_id"
ds["time"].attrs["standard_name"] = "time"
ds["time"].attrs["axis"] = "T"

ds["sector"] = pd.read_csv("./tmp/HIRHAM_Z.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(np.int)
ds["sector"].attrs["standard_name"] = "Zwally 2012 sectors"

ds["region"] = pd.read_csv("./tmp/HIRHAM_Mr.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(np.int)
ds["region"].attrs["standard_name"] = "Mouginot 2019 regions"

ds["basin"] = pd.read_csv("./tmp/HIRHAM_Mb.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(np.int)
ds["basin"].attrs["standard_name"] = "Mouginot 2019 basins"




df = pd.read_csv("./tmp/HIRHAM_Z.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_HIRHAM_Z"] = (("time", "sector"), df.values)
ds["SMB_HIRHAM_Z"].attrs["long_name"] = "Surface mass balance"
ds["SMB_HIRHAM_Z"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_HIRHAM_Z"].attrs["units"] = "Gt yr-1"
ds["SMB_HIRHAM_Z"].attrs["coordinates"] = "time sector"

df = pd.read_csv("./tmp/HIRHAM_Mr.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_HIRHAM_Mr"] = (("time", "region"), df.values)
ds["SMB_HIRHAM_Mr"].attrs["long_name"] = "Surface mass balance"
ds["SMB_HIRHAM_Mr"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_HIRHAM_Mr"].attrs["units"] = "Gt yr-1"
ds["SMB_HIRHAM_Mr"].attrs["coordinates"] = "time region"

df = pd.read_csv("./tmp/HIRHAM_Mb.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_HIRHAM_Mb"] = (("time", "basin"), df.values)
ds["SMB_HIRHAM_Mb"].attrs["long_name"] = "Surface mass balance"
ds["SMB_HIRHAM_Mb"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_HIRHAM_Mb"].attrs["units"] = "Gt yr-1"
ds["SMB_HIRHAM_Mb"].attrs["coordinates"] = "time region"


df = pd.read_csv("./tmp/MAR_Z.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_MAR_Z"] = (("time", "sector"), df.values)
ds["SMB_MAR_Z"].attrs["long_name"] = "Surface mass balance"
ds["SMB_MAR_Z"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_MAR_Z"].attrs["units"] = "Gt yr-1"
ds["SMB_MAR_Z"].attrs["coordinates"] = "time sector"

df = pd.read_csv("./tmp/MAR_Mr.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_MAR_Mr"] = (("time", "region"), df.values)
ds["SMB_MAR_Mr"].attrs["long_name"] = "Surface mass balance"
ds["SMB_MAR_Mr"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_MAR_Mr"].attrs["units"] = "Gt yr-1"
ds["SMB_MAR_Mr"].attrs["coordinates"] = "time region"

df = pd.read_csv("./tmp/MAR_Mb.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_MAR_Mb"] = (("time", "basin"), df.values)
ds["SMB_MAR_Mb"].attrs["long_name"] = "Surface mass balance"
ds["SMB_MAR_Mb"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_MAR_Mb"].attrs["units"] = "Gt yr-1"
ds["SMB_MAR_Mb"].attrs["coordinates"] = "time region"


df = pd.read_csv("./tmp/RACMO_Z.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_RACMO_Z"] = (("time", "sector"), df.values)
ds["SMB_RACMO_Z"].attrs["long_name"] = "Surface mass balance"
ds["SMB_RACMO_Z"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_RACMO_Z"].attrs["units"] = "Gt yr-1"
ds["SMB_RACMO_Z"].attrs["coordinates"] = "time sector"

df = pd.read_csv("./tmp/RACMO_Mr.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_RACMO_Mr"] = (("time", "region"), df.values)
ds["SMB_RACMO_Mr"].attrs["long_name"] = "Surface mass balance"
ds["SMB_RACMO_Mr"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_RACMO_Mr"].attrs["units"] = "Gt yr-1"
ds["SMB_RACMO_Mr"].attrs["coordinates"] = "time region"

df = pd.read_csv("./tmp/RACMO_Mb.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
ds["SMB_RACMO_Mb"] = (("time", "basin"), df.values)
ds["SMB_RACMO_Mb"].attrs["long_name"] = "Surface mass balance"
ds["SMB_RACMO_Mb"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
ds["SMB_RACMO_Mb"].attrs["units"] = "Gt yr-1"
ds["SMB_RACMO_Mb"].attrs["coordinates"] = "time region"







ds.attrs["featureType"] = "timeSeries"
ds.attrs["title"] = "Greenland mass balance"
ds.attrs["summary"] = "Greenland mass balance"
ds.attrs["keywords"] = "Greenland; Mass; Mass balance"
# ds.attrs["Conventions"] = "CF-1.8"
ds.attrs["source"] = "git commit: " + subprocess.check_output(["git", "describe", "--always"]).strip().decode('UTF-8')
# ds.attrs["comment"] = "TODO"
# ds.attrs["acknowledgment"] = "TODO"
# ds.attrs["license"] = "TODO"
# ds.attrs["date_created"] = datetime.datetime.now().strftime("%Y-%m-%d")
ds.attrs["creator_name"] = "Ken Mankoff"
ds.attrs["creator_email"] = "kdm@geus.dk"
ds.attrs["creator_url"] = "http://kenmankoff.com"
ds.attrs["institution"] = "GEUS"
# ds.attrs["time_coverage_start"] = "TODO"
# ds.attrs["time_coverage_end"] = "TODO"
# ds.attrs["time_coverage_resolution"] = "TODO"
ds.attrs["references"] = "10.22008/promice/mass_balance"
ds.attrs["product_version"] = 1.0

comp = dict(zlib=True, complevel=9)
encoding = {var: comp for var in ds.data_vars} # all

ds.to_netcdf('./TMB/mb.nc', mode='w', encoding=encoding)

#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

ValueErrorTraceback (most recent call last)
<ipython-input-194-529e49efdd5f> in <module>
     51 
     52 
---> 53 df = pd.read_csv("./tmp/MAR_Z.bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
     54 ds["SMB_MAR_Z"] = (("time", "sector"), df.values)
     55 ds["SMB_MAR_Z"].attrs["long_name"] = "Surface mass balance"

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    307         @wraps(func)
    308         def wrapper(*args, **kwargs) -> Callable[..., Any]:
--> 309             return func(*args, **kwargs)
    310 
    311         kind = inspect.Parameter.POSITIONAL_OR_KEYWORD

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/frame.py in reindex(self, *args, **kwargs)
   4034         kwargs.pop("axis", None)
   4035         kwargs.pop("labels", None)
-> 4036         return super().reindex(**kwargs)
   4037 
   4038     def drop(

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/generic.py in reindex(self, *args, **kwargs)
   4459 
   4460         # perform the reindex on the axes
-> 4461         return self._reindex_axes(
   4462             axes, level, limit, tolerance, method, fill_value, copy
   4463         ).__finalize__(self, method="reindex")

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/frame.py in _reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   3880         index = axes["index"]
   3881         if index is not None:
-> 3882             frame = frame._reindex_index(
   3883                 index, method, copy, level, fill_value, limit, tolerance
   3884             )

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/frame.py in _reindex_index(self, new_index, method, copy, level, fill_value, limit, tolerance)
   3899             new_index, method=method, level=level, limit=limit, tolerance=tolerance
   3900         )
-> 3901         return self._reindex_with_indexers(
   3902             {0: [new_index, indexer]},
   3903             copy=copy,

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/generic.py in _reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)
   4522 
   4523             # TODO: speed up on homogeneous DataFrame objects
-> 4524             new_data = new_data.reindex_indexer(
   4525                 index,
   4526                 indexer,

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/internals/managers.py in reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate)
   1274         # some axes don't allow reindexing with dups
   1275         if not allow_dups:
-> 1276             self.axes[axis]._can_reindex(indexer)
   1277 
   1278         if axis >= self.ndim:

~/local/miniconda3/envs/TMB/lib/python3.8/site-packages/pandas/core/indexes/base.py in _can_reindex(self, indexer)
   3287         # trying to reindex on an axis with duplicates
   3288         if not self.is_unique and len(indexer):
-> 3289             raise ValueError("cannot reindex from a duplicate axis")
   3290 
   3291     def reindex(self, target, method=None, level=None, limit=None, tolerance=None):

ValueError: cannot reindex from a duplicate axis
#+end_example
:END:

#+BEGIN_SRC jupyter-python
# df_HIRHAM_Z = pd.read_csv('./tmp/HIRHAM_z.bsv', index_col=0, parse_dates=True)
# df_err = pd.read_csv('./out/' + csvfile + '_err.csv', index_col=0, parse_dates=True)
# df_coverage = pd.read_csv('./out/' + csvfile + '_coverage.csv', index_col=0, parse_dates=True)

# meta = pd.read_csv("./out/gate_meta.csv")

# ds = xr.Dataset()

# ds["time"] = (("time"), df_D.index)
# ds["time"].attrs["cf_role"] = "timeseries_id"
# ds["time"].attrs["standard_name"] = "time"
# # ds["time"].attrs["units"] = "day of year"
# # ds["time"].attrs["calendar"] = "julian"
# ds["time"].attrs["axis"] = "T"

# ds["gate"] = (("gate"), df_D.columns.astype(np.int32))
# ds["gate"].attrs["long_name"] = "Gate"
# ds["gate"].attrs["standard_name"] = "N/A"

# ds["discharge"] = (("gate", "time"), df_D.T.values.astype(np.float32))
# ds["discharge"].attrs["long_name"] = "Discharge"
# ds["discharge"].attrs["standard_name"] = "land_ice_mass_tranport_due_to_calving_and_ice_front_melting"
# ds["discharge"].attrs["units"] = "Gt yr-1"
# ds["discharge"].attrs["coordinates"] = "time gate"

# ds["err"] = (("gate", "time"), df_err.T.values.astype(np.float32))
# ds["err"].attrs["long_name"] = "Error"
# ds["err"].attrs["standard_name"] = "Uncertainty"
# ds["err"].attrs["units"] = "Gt yr-1"
# ds["err"].attrs["coordinates"] = "time gate"

# ds["coverage"] = (("gate", "time"), df_coverage.T.values.astype(np.float32))
# ds["coverage"].attrs["long_name"] = "Coverage"
# ds["coverage"].attrs["standard_name"] = "Coverage"
# # ds["coverage"].attrs["units"] = "-"
# ds["coverage"].attrs["coordinates"] = "time gate"

# ds["mean_x"] = (("gate"), meta.mean_x.astype(np.int32))
# ds["mean_x"].attrs["long_name"] = "Mean x coordinate of gate in EPSG:3413"
# ds["mean_x"].attrs["standard_name"] = "Mean x"

# ds["mean_y"] = (("gate"), meta.mean_y.astype(np.int32))
# ds["mean_y"].attrs["long_name"] = "Mean y coordinate of gate in EPSG:3413"
# ds["mean_y"].attrs["standard_name"] = "Mean y"

# ds["mean_lon"] = (("gate"), meta.lon.astype(np.float32))
# ds["mean_lon"].attrs["long_name"] = "Mean lon coordinate of gate"
# ds["mean_lon"].attrs["standard_name"] = "Longitude"

# ds["mean_lat"] = (("gate"), meta.lat.astype(np.float32))
# ds["mean_lat"].attrs["long_name"] = "Mean lat coordinate of gate"
# ds["mean_lat"].attrs["standard_name"] = "Latitude"

# ds["sector"] = (("gate"), meta.sector.astype(np.int32))
# ds["sector"].attrs["long_name"] = "Mouginot 2019 sector containing gate"

# ds["region"] = (("gate"), meta.region)
# ds["region"].attrs["long_name"] = "Mouginot 2019 region containing gate"

# ds["Zwally_2012"] = (("gate"), meta.Zwally_2012)
# ds["Zwally_2012"].attrs["long_name"] = "Zwally 2012 sector containing gate"

# ds["name_Bjørk"] = (("gate"), meta.Bjork_2015)
# ds["name_Bjørk"].attrs["long_name"] = "Nearest name from Bjørk (2015)"

# ds["name_Mouginot"] = (("gate"), meta.Mouginot_2019)
# ds["name_Mouginot"].attrs["long_name"] = "Nearest name from Mouginot (2019)"

# ds.attrs["featureType"] = "timeSeries"
# ds.attrs["title"] = "Greenland discharge"
# ds.attrs["summary"] = "Greenland discharge per gate"
# ds.attrs["keywords"] = "Greenland; Ice Discharge; Calving; Submarine Melt"
# # ds.attrs["Conventions"] = "CF-1.8"
# ds.attrs["source"] = "git commit: " + subprocess.check_output(["git", "describe", "--always"]).strip().decode('UTF-8')
# # ds.attrs["comment"] = "TODO"
# # ds.attrs["acknowledgment"] = "TODO"
# # ds.attrs["license"] = "TODO"
# # ds.attrs["date_created"] = datetime.datetime.now().strftime("%Y-%m-%d")
# ds.attrs["creator_name"] = "Ken Mankoff"
# ds.attrs["creator_email"] = "kdm@geus.dk"
# ds.attrs["creator_url"] = "http://kenmankoff.com"
# ds.attrs["institution"] = "GEUS"
# # ds.attrs["time_coverage_start"] = "TODO"
# # ds.attrs["time_coverage_end"] = "TODO"
# # ds.attrs["time_coverage_resolution"] = "TODO"
# ds.attrs["references"] = "10.22008/promice/ice_discharge"
# ds.attrs["product_version"] = 2.0

# comp = dict(zlib=True, complevel=9)
# encoding = {var: comp for var in ds.data_vars} # all

# ds.to_netcdf('./out/gate.nc', mode='w', encoding=encoding)

#+END_SRC

#+RESULTS:
:   0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 10356.31it/s]HIRHAM
: MAR
: RACMO
: 


* MMB to ROI
** Mankoff 2020
#+BEGIN_SRC jupyter-python :tangle D_to_sector.py
import xarray as xr

ds = xr.open_dataset("/home/kdm/data/Mankoff_2020/ice/latest/gate.nc")

ds_Z = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat","sector"])\
          .groupby("Zwally_2012")\
          .sum()

ds_Mb = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat"])\
          .groupby("sector")\
          .sum()

ds_Mr = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat","sector"])\
          .groupby("region")\
          .sum()

print(ds_Z['discharge'].to_dataframe().unstack().T.head())
#+END_SRC

#+RESULTS:
#+begin_example
Zwally_2012                  11     12     13         21         31         32         33  \
          time                                                                              
discharge 1986-04-15  15.504001  4.844  0.803  21.589001  15.722001  11.582000  34.144005   
          1986-05-15  14.807000  4.844  0.803  21.708000  15.662001  11.475000  34.777004   
          1986-06-15  18.068998  4.843  0.803  21.833000  16.401001  11.731999  36.752995   
          1986-07-15  19.074999  4.844  0.803  19.969000  16.516001  11.771999  35.920002   
          1986-08-15  18.014999  4.844  0.803  21.770000  16.478001  11.165000  36.254002   

Zwally_2012                  41         42         43         50     61     62         71  \
          time                                                                              
discharge 1986-04-15  40.632000  62.001999  40.166000  29.128998  6.737  1.972  23.174000   
          1986-05-15  44.097000  60.396004  39.656998  29.122000  6.740  1.931  30.070000   
          1986-06-15  44.028000  58.748009  39.128002  29.115999  6.743  1.887  30.006001   
          1986-07-15  45.708996  58.844002  38.619003  29.113003  6.746  1.845  29.944000   
          1986-08-15  43.092999  62.721001  38.094006  29.105003  6.749  1.803  29.879999   

Zwally_2012                  72         81        82  
          time                                        
discharge 1986-04-15  46.439999  83.981003  7.387001  
          1986-05-15  46.694004  84.958000  7.380000  
          1986-06-15  46.781002  87.003983  7.375000  
          1986-07-15  46.840996  89.250000  7.368001  
          1986-08-15  45.588997  88.822014  7.361000  
#+end_example


** Mouginot 2019

#+BEGIN_SRC jupyter-python
df = pd.read_excel('/home/kdm/data/Mouginot_2019/pnas.1904242116.sd02.xlsx', sheet_name=1)

c0 = 29 # Column containing 1986
c1 = 61 # Last column
r0 = 8  # sub-table start
r1 = 15 # sub-table stop
D =  pd.DataFrame(index = df.iloc[r0:r1][df.columns[1]],
                  data = df.iloc[r0:r1][df.columns[c0:(c1 + 1)]].values,
                  columns = df.iloc[r0-1][df.columns[c0:(c1 + 1)]].astype(np.int)).T
D.index.name = "Date"
D

c2  = 96
c3 = 128
D_err =  pd.DataFrame(index = df.iloc[r0:r1][df.columns[1]],
                      data = df.iloc[r0:r1][df.columns[c2:(c3 + 1)]].values,
                      columns = df.iloc[r0-1][df.columns[c2:(c3 + 1)]].astype(np.int)).T
D_err.index.name = "Date"
D_err







# r0 = 18  # sub-table start
# r1 = 15 # sub-table stop
# D =  pd.DataFrame(index = df.iloc[r0:r1][df.columns[1]],
#                   data = df.iloc[r0:r1][df.columns[c0:(c1 + 1)]].values,
#                   columns = df.iloc[r0-1][df.columns[c0:(c1 + 1)]].astype(np.int)).T
# D.index.name = "Date"
# D

# c2  = 96
# c3 = 128
# D_err =  pd.DataFrame(index = df.iloc[r0:r1][df.columns[1]],
#                       data = df.iloc[r0:r1][df.columns[c2:(c3 + 1)]].values,
#                       columns = df.iloc[r0-1][df.columns[c2:(c3 + 1)]].astype(np.int)).T
# D_err.index.name = "Date"
# D_err









#+END_SRC

#+RESULTS:
| Date |      SW |      CW |      NW |      NO |      NE |      CE |      SE |
|------+---------+---------+---------+---------+---------+---------+---------|
| 1986 | 6.39625 | 4.72754 | 2.38358 | 1.63729 | 3.90171 | 3.58464 | 6.26399 |
| 1987 | 5.75199 | 4.96941 | 2.45813 | 1.56809 | 3.18116 | 3.80129 |  5.9118 |
| 1988 |    5.16 | 4.71652 |  2.4065 | 1.59757 | 2.88738 |  3.2834 | 6.06374 |
| 1989 | 4.62963 | 4.79985 | 2.42505 | 1.47593 | 2.18272 | 3.70168 | 5.17736 |
| 1990 | 4.17212 | 4.83942 | 2.48977 | 1.43793 | 2.14466 | 3.76175 | 5.27879 |
| 1991 | 3.82206 | 4.85328 | 2.39979 | 1.48182 | 2.26422 | 3.91059 | 5.47042 |
| 1992 | 3.65411 | 4.55641 | 2.43593 | 1.44005 | 2.14691 | 5.30484 |  5.6089 |
| 1993 | 3.64839 | 4.69709 | 2.39624 |  1.6146 | 2.30081 | 5.26971 | 7.09088 |
| 1994 | 3.64255 | 4.60775 | 2.54011 | 1.44794 | 2.10937 | 5.74289 | 6.89653 |
| 1995 | 3.62894 | 4.39499 | 2.61485 | 1.56702 | 2.11486 |  4.9321 | 6.82442 |
| 1996 | 3.51352 | 4.68193 | 2.54174 |  1.4639 | 2.00207 | 5.19833 | 5.48101 |
| 1997 | 3.46357 | 4.36486 | 2.64467 |  1.5321 | 2.01441 | 6.37345 | 5.67094 |
| 1998 |   3.415 | 4.61574 | 2.36702 | 1.66198 | 1.99784 | 6.26478 | 5.58793 |
| 1999 | 3.35585 | 4.53863 | 2.32118 |   1.677 | 1.98387 |  6.7796 | 5.52591 |
| 2000 | 3.39966 |  5.8818 | 3.42818 | 1.51398 | 2.04081 | 4.12785 | 7.05655 |
| 2001 | 3.67663 | 6.12703 | 2.69904 | 1.53292 | 1.95297 | 4.12014 | 5.64725 |
| 2002 | 4.27788 | 4.28093 | 2.72628 | 1.60927 | 2.15423 | 4.04651 | 6.89033 |
| 2003 | 4.26662 | 6.71444 | 2.49513 |  1.7854 | 2.20818 | 4.17898 | 6.86905 |
| 2004 | 3.89778 | 6.91594 | 2.63926 | 1.79659 | 2.04922 | 4.34439 | 6.18362 |
| 2005 | 3.71913 | 6.81152 | 2.61378 | 1.65053 | 2.08252 | 7.49266 | 6.26086 |
| 2006 |  4.5809 |   6.767 | 2.61186 | 1.77084 | 2.02779 | 8.20212 |  5.7107 |
| 2007 | 5.51983 | 6.70576 | 2.56404 | 1.77298 | 2.06704 |  7.3861 | 5.59826 |
| 2008 | 4.85753 |  6.9526 | 2.63584 | 1.71374 | 2.08598 | 7.18677 | 5.86512 |
| 2009 | 4.61921 | 6.66087 | 2.69473 | 1.51692 |  2.0466 | 6.53872 | 6.13972 |
| 2010 | 4.30496 | 7.35043 | 2.65021 | 1.61146 | 2.19988 | 6.33188 | 6.25135 |
| 2011 | 4.72156 | 6.77581 |  2.8259 | 1.79008 | 2.20814 | 6.52244 | 6.63427 |
| 2012 | 4.76317 | 7.29886 | 2.85012 | 1.77425 | 2.35668 |  6.8472 | 6.51387 |
| 2013 | 4.59133 | 7.45694 | 2.77906 | 1.94955 | 2.18406 | 6.47556 | 6.06294 |
| 2014 | 4.12034 | 7.56739 | 2.96806 | 2.09472 | 2.33947 |  6.2547 | 6.03204 |
| 2015 | 4.32102 | 7.41726 | 3.00687 | 1.91911 | 2.45207 | 6.00333 | 6.12812 |
| 2016 | 4.28208 | 7.33231 | 2.98299 | 1.78163 | 2.48866 | 5.81663 | 5.87333 |
| 2017 | 3.86859 | 6.32904 |  3.3492 | 1.76588 | 2.49688 | 5.90265 | 5.82881 |
| 2018 | 4.06431 | 6.00724 | 3.54655 | 1.89799 | 2.72033 | 6.49952 | 5.95446 |



* BMB
** GHF
** Velocity
** Surface runoff
:PROPERTIES:
:header-args:bash+: :tangle VHD.sh
:END:

*** Source contribution map

+ Determine VHD routing map as per citet:mankoff_2017_VHD
+ For each source cell, estimate the contribution to that sector VHD per unit mass of water
  + That is, from the water source, the integrated VHD between source and outlet.
  + Rather than doing full routing for each and every interior cell, the integrated VHD per cell can be estimated as the difference between the source pressure+elevation and the basal pressure+elevation terms.
+ Then, use this source map applied to each day of runoff from one of the RCMs.

Do this work in the RACMO location because it is EPSG:3413 and so is BedMachine (used for the basal routing)


*** Import BedMachine v3
+ from [[textcite:Morlighem:2017BedMachine][Morlighem /et al./ (2017)]]
#+BEGIN_SRC bash :results verbatim
g.mapset -c BedMachine

for var in $(echo mask surface thickness bed errbed); do
  echo $var
  # r.external source=netCDF:${DATADIR}/Morlighem_2017/BedMachineGreenland-2020-07-16.nc:${var} output=${var}
  r.external source=netCDF:${DATADIR}/Morlighem_2017/BedMachineGreenland-2017-09-20.nc:${var} output=${var}
done

g.region raster=surface
g.region save=BedMachine

r.colors map=mask color=haxby
r.mapcalc "mask_ice_0 = if(mask == 2, 1, null())"
#+END_SRC

*** Expand Mask

The ice mask needs to be expanded so that land terminating glaciers need 1 grid cell outside the ice domain are included in the model, so that the discharge location has 0 thickness (0 pressure term) and all pressure energy is released. Only gravitational potential energy remains. Submarine discharge remains pressurized.

#+BEGIN_SRC bash
r.grow input=mask_ice_0 output=mask_ice_1 radius=1.5 new=1
r.mapcalc "mask_ice = if((mask == 0) | (mask == 3), null(), mask_ice_1)"
#+END_SRC

*** Expand ROIs to BedMachine ice domain

Copied from <<expand_zwally>> and <<expand_mouginot>>, but run here so that it is in the BedMachine mapset

#+BEGIN_SRC bash
r.grow.distance input=sectors@Zwally_2012 value=value_Z
r.mapcalc "sectors_e = if(mask_ice, int(value_Z), null())"

r.grow.distance input=regions@Mouginot_2019 value=value_Mr
r.mapcalc "regions_e = if(mask_ice, int(value_Mr), null())"

r.grow.distance input=basins@Mouginot_2019 value=value_Mb
r.mapcalc "basins_e = if(mask_ice, int(value_Mb), null())"
#+END_SRC

*** COMMENT Fill in small holes

Also fills in nunatuks.

This is done because hydrologic routing will terminate at domain boundaries, even if they're inland. We want to route to the ice edge.

#+BEGIN_SRC bash :results verbatim
r.colors map=mask color=haxby
r.mapcalc "not_ice = if(isnull(mask) ||| (mask != 2), 1, 0)"

# No mask, NULLS are not clumped
r.clump input=not_ice output=clumps
# d.rast clumps
main_clump=$(r.stats -c -n clumps sort=desc | head -n1 | cut -d" " -f1)
r.mask -i raster=clumps maskcats=${main_clump} --o

r.mapcalc "all_ice = 1"
r.clump input=all_ice output=clumps2
# d.rast clumps2
main_clump=$(r.stats -c -n clumps2 sort=desc | head -n1 | cut -d" " -f1)
r.mask raster=clumps2 maskcats=${main_clump} --o

r.mapcalc "mask_ice = MASK"
# ice mask with no islands

# original mask ice
r.mask -r
r.mapcalc "mask_ice_islands = if(mask == 2, 1, null())"
#+END_SRC
#+RESULTS:


*** Hydropotential head

#+BEGIN_SRC bash :results verbatim
log_info "Calculating subglacial head with k = 1.0"
r.mapcalc "head = mask_ice * bed + 1 * 0.917 * thickness"
#+END_SRC

**** Streams

After calculating the head, we use 3rd party tools to get the flow direction and streams

#+NAME: streams
#+BEGIN_SRC bash :results verbatim
THRESH=300
log_warn "Using threshold: ${THRESH}"
log_info "r.stream.extract..."

r.stream.extract elevation=head threshold=${THRESH} memory=16384 direction=dir stream_raster=streams stream_vector=streams
#+END_SRC

**** Outlets

+ The flow direction =dir= is negative where flow leaves the domain. These are the outlets.
+ Encode each outlet with a unique id

#+NAME: outlets
#+BEGIN_SRC bash :results verbatim
log_info "Calculating outlets"
r.mapcalc "outlets_1 = if(dir < 0, 1, null())"
r.out.xyz input=outlets_1 | \
    cat -n | \
    tr '\t' '|' | \
    cut -d"|" -f1-3 | \
    v.in.ascii input=- output=outlets_uniq separator=pipe \
        columns="x int, y int, cat int" x=2 y=3 cat=1
#+END_SRC

**** Basins

Using =r.stream.basins=, we can get basins for every outlet.

#+NAME: basins
#+BEGIN_SRC bash :results verbatim
log_info "r.stream.basins..."

r.stream.basins -m direction=dir points=outlets_uniq basins=basins_uniq memory=16384 --verbose
#+END_SRC


*** Find outlet (x,y) for each inland cell
**** Extract the x and y coordinates of each outlet
#+BEGIN_SRC bash :results verbatim
log_info "Exporting outlet coordinates"

r.out.xyz input=outlets | awk -F'|' '{print \$3, \$1}' > ./tmp/outlets_x
r.out.xyz input=outlets | awk -F'|' '{print \$3, \$2}' > ./tmp/outlets_y
#+END_SRC

**** Basins map where pixel encodes the x and y coordinate of its outlet

+ First encode the outlet location as the category information of each basin, rather than as the value for each (x,y) cell.
  + This means we encode n basin bits of info (20,000?) rather than n grid cells bits of info (4.5M?)
+ Then create new rasters where the grid cells contain the outlet info directly. Easier for math further down in the code.

#+BEGIN_SRC bash :results verbatim

log_info "Encoding outlet coordinates"

g.copy basins,cat_x
g.copy basins,cat_y

r.category map=cat_x rules=./tmp/outlets_x separator=space
r.category map=cat_y rules=./tmp/outlets_y separator=space

r.mapcalc "outlet_x = @cat_x"
r.mapcalc "outlet_y = @cat_y"
#+END_SRC



*** More complex stats in Python                        :noexport:
**** Export to CSV

#+BEGIN_SRC bash :results verbatim

log_info "Exporting cells..."
log_warn "Reducing resolution."

LIST=$(g.list type=raster pattern='^outlet_[x|y]$' separator=,)
echo "x,y,${LIST}" | tr "," "|" > ./tmp/cells_xy.bsv
g.region res=1000 -a
r.out.xyz input=${LIST} output=- >> ./tmp/cells_xy.bsv
g.region BedMachine
#+END_SRC


**** Import & process in Python

#+BEGIN_SRC jupyter-python :tangle VHD.py
import pandas as pd
import numpy as np

df_xy = pd.read_csv("./tmp/cells_xy.bsv", delimiter="|", usecols=['x','y']).astype(np.int)

header = pd.read_csv("./tmp/cells_xy.bsv", delimiter="|", nrows=1)
cols_in = header.columns[['ice' in _ for _ in header.columns]]
df = pd.read_csv("./tmp/cells_xy.bsv", delimiter="|", usecols=cols_in)

multi_hdr = [np.array([_.split("_")[1] for _ in cols_in]).astype(np.int), 
             [_.split("_")[2] for _ in cols_in]]
df.columns = pd.MultiIndex.from_arrays(multi_hdr, names=['k','coord'])

k = np.unique(multi_hdr[0])

for kk in k:

    # mean distance for this k =
    # all other columns minus this column squared for x
    # also for y
    # square root of all that
    # take the mean of all columns

    df[(kk,'avg_dist')] = (((df.xs('x', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('x', level='coord', axis='columns')[kk]))**2 + (df.xs('y', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('y', level='coord', axis='columns')[kk]))**2)**0.5).mean(axis='columns')

    # same for max, except last step

    df[(kk,'max_dist')] = (((df.xs('x', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('x', level='coord', axis='columns')[kk]))**2 + (df.xs('y', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('y', level='coord', axis='columns')[kk]))**2)**0.5).max(axis='columns')

    # df[(kk,'std_dist')] = (((df.xs('x', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('x', level='coord', axis='columns')[kk]))**2 + (df.xs('y', level='coord', axis='columns').drop(kk, axis='columns').apply(lambda c: c-df.xs('y', level='coord', axis='columns')[kk]))**2)**0.5).std(axis='columns')

df_out = pd.DataFrame()
df_out['avg_avg'] = df.xs('avg_dist', level='coord', axis='columns').mean(axis='columns')
# df_out['avg_max'] = df.xs('max_dist', level='coord', axis='columns').mean(axis='columns')
# df_out['avg_std'] = df.xs('std_dist', level='coord', axis='columns').mean(axis='columns')

# df_out['max_avg'] = df.xs('avg_dist', level='coord', axis='columns').max(axis='columns')
df_out['max_max'] = df.xs('max_dist', level='coord', axis='columns').max(axis='columns')
# df_out['max_std'] = df.xs('std_dist', level='coord', axis='columns').max(axis='columns')

# df_out['std_avg'] = df.xs('avg_dist', level='coord', axis='columns').std(axis='columns')
# df_out['std_max'] = df.xs('max_dist', level='coord', axis='columns').std(axis='columns')
# df_out['std_std'] = df.xs('std_dist', level='coord', axis='columns').std(axis='columns')

df_out = df_out.merge(df_xy, left_index=True, right_index=True)
df_out.to_csv("./tmp/cells_xy_ice_stats.bsv", sep="|", float_format='%d')
#+END_SRC


**** Import Python results back to GRASS

#+BEGIN_SRC bash :results verbatim

# run python code above to calculate basin outlet statistics

log_info "Calculating cell outlet statistics..."

python ./VHD.py 

log_info "Re-importing rasters."
log_warn "Reduced resolution."

g.region res=1000 -a
r.in.xyz input=./tmp/cells_xy_ice_stats.bsv output=ice_max method=mean x=4 y=5 z=3 skip=1
r.in.xyz input=./tmp/cells_xy_ice_stats.bsv output=ice_avg method=mean x=4 y=5 z=2 skip=1

# r.in.xyz input=./tmp/cells_xy_land_stats.bsv output=land_max method=mean x=4 y=5 z=3 skip=1
# r.in.xyz input=./tmp/cells_xy_land_stats.bsv output=land_avg method=mean x=4 y=5 z=2 skip=1
#+END_SRC


*** Find outlet pressure head and elevation head for each inland cell

*** Determine integrated VHD contribution from each inland cell per unit water

*** NOTDONE Volumetric flow through each cell              :grass:

#+BEGIN_SRC bash :results verbatim
r.mask raster=mask_ice_1 --o # CHECKME

r.mapcalc 'phi_z = 1000 * 9.8 * bed@BedMachine' --o --q
r.mapcalc 'phi_p = 0.9 * (917 * 9.8 * (surface@BedMachine - bed@BedMachine))' --o --q
r.mapcalc 'phi = phi_z + phi_p' --o --q

# r.watershed doesn't work with extreme values larger than "normal"
# terrain (presumably due to buffer overflow), so scale things from Pa to kPa.
r.mapcalc 'phi_1E4 = phi / 10000' --o  --q # [Pa] to [kpa]
# See: https://trac.osgeo.org/grass/ticket/2935

r.watershed -s -m -a elevation=phi_1E4 accumulation=V_1000 drainage=fdir
#+END_SRC
#+RESULTS:

*** NOTDONE Convert from mmWE to mWE                       :grass:

We left runoff in its original [mmWE] form and therefore accumulation is also in [mmWE], because if we converted to [m] earlier, the small numbers appeared to cause underflow somewhere (probably in =r.watershed=?).

#+BEGIN_SRC bash :results verbatim
r.mapcalc 'V = V_1000 / 1000'
#+END_SRC
#+RESULTS:
*** NOTDONE \nabla h, h_z, h_p
#+BEGIN_SRC bash :results verbatim

r.mask mask_ice_1 --o # CHECKME
r.mapcalc "phi = if(isnull(phi), 0, phi)" --o
r.mapcalc "delta_phi = 0" --o
r.mapcalc "delta_phi_z = 0" --o
r.mapcalc "delta_phi_p = 0" --o
r.mapcalc "fdir_abs = abs(fdir)" --o
for dir in $(seq 8); do 
  if [[ ${dir} == '1' ]]; then r=-1; c=1;  fi # 45 CCW from E; /°
  if [[ ${dir} == '2' ]]; then r=-1; c=0;  fi # 90 °|
  if [[ ${dir} == '3' ]]; then r=-1; c=-1; fi # 135 °\
  if [[ ${dir} == '4' ]]; then r=0;  c=-1; fi # 180 .-
  if [[ ${dir} == '5' ]]; then r=1;  c=-1; fi # 225 ./
  if [[ ${dir} == '6' ]]; then r=1;  c=0;  fi # 270 .|
  if [[ ${dir} == '7' ]]; then r=1;  c=1;  fi # 315 \.
  if [[ ${dir} == '8' ]]; then r=0;  c=1;  fi # 360 ->

  r.mapcalc "delta_phi = if(fdir_abs == ${dir}, phi - phi[${r},${c}], delta_phi)" --o
  r.mapcalc "delta_phi_z = if(fdir_abs == ${dir}, phi_z - phi_z[${r},${c}], delta_phi_z)" --o
  r.mapcalc "delta_phi_p = if(fdir_abs == ${dir}, phi_p - phi_p[${r},${c}], delta_phi_p)" --o
  echo ${dir}
done

# r.mapcalc "delta_phi = if(delta_phi < 0, 0, delta_phi)" --o
#+END_SRC
#+RESULTS:
*** NOTDONE Energy dissipated at each point

0.36 is the scaling due to the changing phase change temperature (see Python code embedded in Methods section)

V is in [m^3]
h should be in [m] since we used unique phi_1E4 for flow routing
     See https://trac.osgeo.org/grass/ticket/2935
result is [J] per grid cell per year

#+BEGIN_SRC bash :results verbatim
r.mask mask_ice_1 --o # CHECKME
r.mapcalc "VHD = V * ( delta_phi - 0.36 * delta_phi_p )" --o
#+END_SRC


* IMBIE

This spreadsheet contains the IMBIE-2019 datasets for Greenland, which includes data on the annual rate of change and cumulative change in Greenland’s ice sheet mass, its surface mass balance and ice discharge anomalies, and their estimated uncertainty. The data are expressed in units of rate of mass change (Gigatons per year – sheet 1, columns B, C, F, G, J and K) mass (Gigatons – sheet 1, columns D, E, H, I, L and M) and in units of equivalent mean global sea level rise (millimetres per year – sheet 2, columns B, C, F, G, J and K, and millimetres – sheet 2, columns D, E, H, I, L and M).

#+BEGIN_SRC jupyter-python
import pandas as pd
df = pd.read_excel("/home/kdm/data/IMBIE/imbie_dataset_greenland_dynamics-2020_02_28.xlsx", sheet_name=0, index_col=0, usecols=(0,1,2,3,4,5,6,9,10))\
       .rename(columns={"Rate of ice sheet mass change (Gt/yr)":"mb",
                        "Rate of ice sheet mass change uncertainty (Gt/yr)":"mb_err",
                        "Cumulative ice sheet mass change (Gt)" : "mb_cum",
	                "Cumulative ice sheet mass change uncertainty (Gt)" : "mb_cum_err",
                        "Rate of mass balance anomaly (Gt/yr)":"smb",
                        "Rate of mass balance anomaly uncertainty (Gt/yr)":"smb_err",
                        "Rate of ice dynamics anomaly (Gt/yr)":"D",
                        "Rate of ice dyanamics anomaly uncertainty (Gt/yr)":"D_err"})

df['index'] = pd.to_datetime('1980-01-01') + pd.to_timedelta((df.index-1980) * 365, unit="D")
# Appears that these fractional dates are supposed to be start-of-month? I think so...
# Let's hard-code this.
df['index'] = [pd.to_datetime(y + '-' + m + '-01') for y in df.index.astype(np.int).unique().astype(np.str) for m in np.arange(1,13).astype(np.str)]
df.tail(30)
#+END_SRC

#+RESULTS:
|    Year |       mb |  mb_err |   mb_cum | mb_cum_err |      smb | smb_err |        D |   D_err | index               |
|---------+----------+---------+----------+------------+----------+---------+----------+---------+---------------------|
|  2016.5 | -231.338 | 62.4089 | -3704.86 |     330.44 | -174.559 | 86.9118 | -56.7792 | 106.998 | 2016-07-01 00:00:00 |
| 2016.58 | -269.403 | 65.6814 | -3727.31 |    330.984 | -174.559 | 86.9118 | -94.8442 | 108.939 | 2016-08-01 00:00:00 |
| 2016.67 | -269.403 | 65.6814 | -3749.76 |    331.526 | -174.559 | 86.9118 | -94.8442 | 108.939 | 2016-09-01 00:00:00 |
| 2016.75 | -269.403 | 65.6814 | -3772.21 |    332.068 | -174.559 | 86.9118 | -94.8442 | 108.939 | 2016-10-01 00:00:00 |
| 2016.83 | -269.403 | 65.6814 | -3794.66 |    332.609 | -174.559 | 86.9118 | -94.8442 | 108.939 | 2016-11-01 00:00:00 |
| 2016.92 | -269.403 | 65.6814 | -3817.11 |    333.149 | -174.559 | 86.9118 | -94.8442 | 108.939 | 2016-12-01 00:00:00 |
|    2017 |  -84.583 | 75.4958 | -3824.16 |    333.861 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-01-01 00:00:00 |
| 2017.08 |  -84.583 | 75.4958 | -3831.21 |    334.572 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-02-01 00:00:00 |
| 2017.17 |  -84.583 | 75.4958 | -3838.26 |    335.281 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-03-01 00:00:00 |
| 2017.25 |  -84.583 | 75.4958 | -3845.31 |    335.988 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-04-01 00:00:00 |
| 2017.33 |  -84.583 | 75.4958 | -3852.36 |    336.694 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-05-01 00:00:00 |
| 2017.42 |  -84.583 | 75.4958 |  -3859.4 |    337.399 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-06-01 00:00:00 |
|  2017.5 |  -84.583 | 75.4958 | -3866.45 |    338.102 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-07-01 00:00:00 |
| 2017.58 |  -84.583 | 75.4958 |  -3873.5 |    338.804 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-08-01 00:00:00 |
| 2017.67 |  -84.583 | 75.4958 | -3880.55 |    339.504 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-09-01 00:00:00 |
| 2017.75 |  -84.583 | 75.4958 |  -3887.6 |    340.203 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-10-01 00:00:00 |
| 2017.83 |  -84.583 | 75.4958 | -3894.65 |      340.9 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-11-01 00:00:00 |
| 2017.92 |  -84.583 | 75.4958 |  -3901.7 |    341.596 |  32.2868 | 110.905 |  -116.87 | 134.163 | 2017-12-01 00:00:00 |
|    2018 | -75.4444 | 66.0924 | -3907.98 |    342.129 |  45.6531 | 77.0317 | -121.097 | 101.499 | 2018-01-01 00:00:00 |
| 2018.08 |   -143.8 |    56.8 | -3919.97 |    342.521 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-02-01 00:00:00 |
| 2018.17 |   -143.8 |    56.8 | -3931.95 |    342.914 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-03-01 00:00:00 |
| 2018.25 |   -143.8 |    56.8 | -3943.93 |    343.305 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-04-01 00:00:00 |
| 2018.33 |   -143.8 |    56.8 | -3955.92 |    343.697 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-05-01 00:00:00 |
| 2018.42 |   -143.8 |    56.8 |  -3967.9 |    344.088 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-06-01 00:00:00 |
|  2018.5 |   -143.8 |    56.8 | -3979.88 |    344.478 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-07-01 00:00:00 |
| 2018.58 |   -143.8 |    56.8 | -3991.87 |    344.868 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-08-01 00:00:00 |
| 2018.67 |   -143.8 |    56.8 | -4003.85 |    345.258 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-09-01 00:00:00 |
| 2018.75 |   -143.8 |    56.8 | -4015.83 |    345.647 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-10-01 00:00:00 |
| 2018.83 |   -143.8 |    56.8 | -4027.82 |    346.035 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-11-01 00:00:00 |
| 2018.92 |   -143.8 |    56.8 |  -4039.8 |    346.424 |  45.6531 | 77.0317 | -189.453 | 95.7085 | 2018-12-01 00:00:00 |


* Environment
** Python
*** Anaconda environment

**** Create
#+BEGIN_SRC bash
env=TMB
conda create -n ${env} python=3.8 xarray pandas matplotlib jupyter tabulate pint uncertainties scipy cfchecker geopandas
conda activate ${env}
python -m ipykernel install --user --name=${env}

# anconda install matplotlib xarray rasterio simplekml 
# pip install uncertainties

#+END_SRC
**** Share

#+BEGIN_SRC bash :cmdline -i :results drawer verbatim :exports both
conda env export --name TMB | tee environment.yml
#+END_SRC

#+RESULTS:
:results:
name: TMB
channels:
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_gnu
  - argon2-cffi=20.1.0=py38h25fe258_2
  - async_generator=1.10=py_0
  - attrs=20.3.0=pyhd3deb0d_0
  - backcall=0.2.0=pyh9f0ad1d_0
  - backports=1.0=py_2
  - backports.functools_lru_cache=1.6.1=py_0
  - bleach=3.2.1=pyh9f0ad1d_0
  - bzip2=1.0.8=h516909a_3
  - c-ares=1.16.1=h516909a_3
  - ca-certificates=2020.11.8=ha878542_0
  - certifi=2020.11.8=py38h578d9bd_0
  - cfchecker=4.0.0=py_0
  - cffi=1.14.3=py38h1bdcb99_1
  - cftime=1.3.0=py38h0b5ebd8_0
  - cfunits=3.3.0=pyh9f0ad1d_0
  - curl=7.71.1=he644dc0_8
  - cycler=0.10.0=py_2
  - dbus=1.13.6=hfdff14a_1
  - decorator=4.4.2=py_0
  - defusedxml=0.6.0=py_0
  - entrypoints=0.3=pyhd8ed1ab_1003
  - expat=2.2.9=he1b5a44_2
  - fontconfig=2.13.1=h7e3eb15_1002
  - freetype=2.10.4=h7ca028e_0
  - future=0.18.2=py38h578d9bd_2
  - gettext=0.19.8.1=hf34092f_1004
  - glib=2.66.2=h58526e2_0
  - gst-plugins-base=1.14.5=h0935bb2_2
  - gstreamer=1.14.5=h36ae1b5_2
  - hdf4=4.2.13=hf30be14_1003
  - hdf5=1.10.6=nompi_h1022a3e_1110
  - icu=67.1=he1b5a44_0
  - importlib-metadata=2.0.0=py_1
  - importlib_metadata=2.0.0=1
  - importlib_resources=3.3.0=py38h578d9bd_0
  - ipykernel=5.3.4=py38h81c977d_1
  - ipython=7.19.0=py38h81c977d_0
  - ipython_genutils=0.2.0=py_1
  - ipywidgets=7.5.1=pyh9f0ad1d_1
  - jedi=0.17.2=py38h578d9bd_1
  - jinja2=2.11.2=pyh9f0ad1d_0
  - jpeg=9d=h36c2ea0_0
  - jsonschema=3.2.0=py_2
  - jupyter=1.0.0=py_2
  - jupyter_client=6.1.7=py_0
  - jupyter_console=6.2.0=py_0
  - jupyter_core=4.6.3=py38h578d9bd_2
  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0
  - kiwisolver=1.3.1=py38h82cb98a_0
  - krb5=1.17.1=hfafb76e_3
  - lcms2=2.11=hcbb858e_1
  - ld_impl_linux-64=2.35.1=hed1e6ac_0
  - libblas=3.9.0=2_openblas
  - libcblas=3.9.0=2_openblas
  - libclang=10.0.1=default_hde54327_1
  - libcurl=7.71.1=hcdd3856_8
  - libedit=3.1.20191231=he28a2e2_2
  - libev=4.33=h516909a_1
  - libevent=2.1.10=hcdb4288_3
  - libffi=3.2.1=he1b5a44_1007
  - libgcc-ng=9.3.0=h5dbcf3e_17
  - libgfortran-ng=9.3.0=he4bcb1c_17
  - libgfortran5=9.3.0=he4bcb1c_17
  - libglib=2.66.2=hbe7bbb4_0
  - libgomp=9.3.0=h5dbcf3e_17
  - libiconv=1.16=h516909a_0
  - liblapack=3.9.0=2_openblas
  - libllvm10=10.0.1=he513fc3_3
  - libnetcdf=4.7.4=nompi_hefab0ff_106
  - libnghttp2=1.41.0=h8cfc5f6_2
  - libopenblas=0.3.12=pthreads_h4812303_1
  - libpng=1.6.37=h21135ba_2
  - libpq=12.3=h5513abc_2
  - libsodium=1.0.18=h36c2ea0_1
  - libssh2=1.9.0=hab1572f_5
  - libstdcxx-ng=9.3.0=h2ae2ef3_17
  - libtiff=4.1.0=h4f3a223_6
  - libuuid=2.32.1=h14c3975_1000
  - libwebp-base=1.1.0=h36c2ea0_3
  - libxcb=1.13=h14c3975_1002
  - libxkbcommon=0.10.0=he1b5a44_0
  - libxml2=2.9.10=h68273f3_2
  - lz4-c=1.9.2=he1b5a44_3
  - markupsafe=1.1.1=py38h8df0ef7_2
  - matplotlib=3.3.3=py38h578d9bd_0
  - matplotlib-base=3.3.3=py38h5c7f4ab_0
  - mistune=0.8.4=py38h25fe258_1002
  - mysql-common=8.0.21=2
  - mysql-libs=8.0.21=hf3661c5_2
  - nbclient=0.5.1=py_0
  - nbconvert=6.0.7=py38h578d9bd_3
  - nbformat=5.0.8=py_0
  - ncurses=6.2=h58526e2_4
  - nest-asyncio=1.4.3=pyhd8ed1ab_0
  - netcdf4=1.5.4=nompi_py38hec8b9af_103
  - notebook=6.1.5=py38h578d9bd_0
  - nspr=4.29=he1b5a44_1
  - nss=3.59=h2c00c37_0
  - numpy=1.19.4=py38hf0fd68c_1
  - olefile=0.46=pyh9f0ad1d_1
  - openssl=1.1.1h=h516909a_0
  - packaging=20.4=pyh9f0ad1d_0
  - pandas=1.1.4=py38h0ef3d22_0
  - pandoc=2.11.1.1=h36c2ea0_0
  - pandocfilters=1.4.2=py_1
  - parso=0.7.1=pyh9f0ad1d_0
  - pcre=8.44=he1b5a44_0
  - pexpect=4.8.0=pyh9f0ad1d_2
  - pickleshare=0.7.5=py_1003
  - pillow=8.0.1=py38h70fbd49_0
  - pint=0.16.1=py_0
  - pip=20.2.4=py_0
  - prometheus_client=0.9.0=pyhd3deb0d_0
  - prompt-toolkit=3.0.8=pyha770c72_0
  - prompt_toolkit=3.0.8=hd8ed1ab_0
  - pthread-stubs=0.4=h14c3975_1001
  - ptyprocess=0.6.0=py_1001
  - pycparser=2.20=pyh9f0ad1d_2
  - pygments=2.7.2=py_0
  - pyparsing=2.4.7=pyh9f0ad1d_0
  - pyqt=5.12.3=py38ha8c2ead_4
  - pyrsistent=0.17.3=py38h25fe258_1
  - python=3.8.6=h852b56e_0_cpython
  - python-dateutil=2.8.1=py_0
  - python_abi=3.8=1_cp38
  - pytz=2020.4=pyhd8ed1ab_0
  - pyzmq=20.0.0=py38h1d1b12f_1
  - qt=5.12.9=h1f2b2cb_0
  - qtconsole=4.7.7=pyh9f0ad1d_0
  - qtpy=1.9.0=py_0
  - readline=8.0=he28a2e2_2
  - scipy=1.5.3=py38hb2138dd_0
  - send2trash=1.5.0=py_0
  - setuptools=49.6.0=py38h924ce5b_2
  - six=1.15.0=pyh9f0ad1d_0
  - sqlite=3.33.0=h4cf870e_1
  - tabulate=0.8.7=pyh9f0ad1d_0
  - terminado=0.9.1=py38h32f6830_1
  - testpath=0.4.4=py_0
  - tk=8.6.10=hed695b0_1
  - tornado=6.1=py38h25fe258_0
  - traitlets=5.0.5=py_0
  - udunits2=2.2.27.6=h4e0c4b3_1001
  - uncertainties=3.1.4=py_0
  - wcwidth=0.2.5=pyh9f0ad1d_2
  - webencodings=0.5.1=py_1
  - wheel=0.35.1=pyh9f0ad1d_0
  - widgetsnbextension=3.5.1=py38h578d9bd_4
  - xarray=0.16.1=py_0
  - xorg-libxau=1.0.9=h14c3975_0
  - xorg-libxdmcp=1.1.3=h516909a_0
  - xz=5.2.5=h516909a_1
  - zeromq=4.3.3=h58526e2_2
  - zipp=3.4.0=py_0
  - zlib=1.2.11=h516909a_1010
  - zstd=1.4.5=h6597ccf_2
  - pip:
    - pyqt5-sip==4.19.18
    - pyqtchart==5.12
    - pyqtwebengine==5.12.1
prefix: /home/kdm/local/miniconda3/envs/TMB

:end:

*** Packages
#+NAME: init_py
#+BEGIN_SRC jupyter-python
import numpy as np
import pandas as pd
import xarray as xr
#+END_SRC

#+RESULTS: init

*** Graphics
#+NAME: init_graphics
#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt

from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)
matplotlib.pyplot.xkcd()
#+END_SRC

#+RESULTS: init_graphics

*** Data Dir

+ I set =DATADIR= as a =bash= environment variable in my login scripts.
+ This is so that Python babel blocks can also easily get that property.

#+NAME: get_DATADIR
#+BEGIN_SRC jupyter-python
import os
DATADIR = os.environ['DATADIR']
#+END_SRC

Example:
#+BEGIN_SRC jupyter-python :tangle no
<<get_DATADIR>>
print(DATADIR)
#+END_SRC

** Bash
#+NAME: init_bash
#+BEGIN_SRC bash :results verbatim
set -o nounset
set -o pipefail

# set -o errexit

### uncomment the above line when doing initial run. When rerunning and
### counting on GRASS failing w/ overwrite issues (speed increase), the
### line above must be commented

red='\033[0;31m'; orange='\033[0;33m'; green='\033[0;32m'; nc='\033[0m' # No Color
log_info() { echo -e "${green}[$(date --iso-8601=seconds)] [INFO] ${@}${nc}"; }
log_warn() { echo -e "${orange}[$(date --iso-8601=seconds)] [WARN] ${@}${nc}"; }
log_err() { echo -e "${red}[$(date --iso-8601=seconds)] [ERR] ${@}${nc}" >&2; }

# trap ctrl_c INT # trap ctrl-c and call ctrl_c()
# ctrl_c() { log_err "CTRL-C. Cleaning up"; }

debug() { if [[ debug:- == 1 ]]; then log_warn "debug:"; echo $@; fi; }

#+END_SRC

** GRASS config

https://grass.osgeo.org/grass74/manuals/variables.html

| GRASS_VERBOSE |                                                                |
|---------------+----------------------------------------------------------------|
|            -1 | complete silence (also errors and warnings are discarded)      |
|             0 | only errors and warnings are printed                           |
|             1 | progress and important messages are printed (percent complete) |
|             2 | all module messages are printed                                |
|             3 | additional verbose messages are printed                        |

#+NAME: init_grass
#+BEGIN_SRC bash :results verbatim :tangle no
export GRASS_VERBOSE=3
# export GRASS_MESSAGE_FORMAT=silent

if [ -z ${DATADIR+x} ]; then
    echo "DATADIR environment varible is unset."
    echo "Fix with: \"export DATADIR=/path/to/data\""
    exit 255
fi

set -x # print commands to STDOUT before running them
#+END_SRC

* Makefile
:PROPERTIES:
:CUSTOM_ID: sec:makefile
:END:

This code, and all code files in this project, are derived products tangled from the sob.org source file.

#+BEGIN_SRC makefile :tangle Makefile :tangle-mode (identity #o444)
PROMICE_MB: all
# dist

mar_proj = "+proj=sterea +lat_0=90 +lat_ts=70.5 +lon_0=-40 +k=1 +x_0=0 +y_0=0 	+datum=WGS84 +units=m +no_defs"

all: FORCE
	mkdir -p tmp dat

	# set up
	grass -e -c EPSG:4326 G_HIRHAM
	grass ./G_HIRHAM/PERMANENT/ --exec ./HIRHAM.sh
	
	grass -e -c G_tmp --exec g.proj -c location=G_MAR proj4=$(mar_proj)
	rm -fR G_tmp
	grass ./G_MAR/PERMANENT --exec ./MAR.sh

	grass -e -c EPSG:3413 G_RACMO
	grass ./G_RACMO/PERMANENT --exec ./RACMO.sh

	make SMB
	make dist

SMB: FORCE
	# partition RCM by Zwally sectors, Mouginot basins, and Mouginot regions
	grass ./G_HIRHAM/PERMANENT --exec ./HIRHAM_to_sectors.sh
	grass ./G_MAR/PERMANENT --exec ./MAR_to_sectors.sh
	grass ./G_RACMO/PERMANENT --exec ./RACMO_to_sectors.sh

dist: FORCE
	echo "Not done yet"
	# create end-user data product
	# parallel --bar "python bsv2netcdf.py {}" ::: $(ls dat/*_runoff_*.bsv)

# dist: freshwater.zip
# 	zip -r freshwater.zip freshwater

FORCE: # dummy target

clean_all:
	rm -fR G_RACMO G_HIRHAM G_MAR G_tmp tmp dat PROMICE_MB

clean_SMB:
	rm -fR tmp/HIRHAM tmp/MAR tmp/RACMO 
	
#+END_SRC
