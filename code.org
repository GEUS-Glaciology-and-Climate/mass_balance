
#+PROPERTY: header-args:bash+ :comments both :noweb yes :eval no-export
#+PROPERTY: header-args:bash+ :session (concat "*" (file-name-sans-extension (buffer-name)) "-shell*")
#+PROPERTY: header-args:bash+ :tangle-mode (identity #o544) :shebang #!/usr/bin/env bash
#+PROPERTY: header-args:jupyter-python :kernel TMB :session TMB :noweb yes :comments both

* ROIs

ROI refers both to the citet:zwally_2012 sectors and the citet:mouginot_2019_glacier basins and regions.

Procedure to map the ROIs to the SMB grids:
+ ROIs are in EPSG:4326 projection, and need to be re-projected to the SMB projection
  + HIRHAM :: rotated pole
  + MAR :: Oblique stereographic 
  + RACMO :: EPSG:3413

SMB grids do not match ROIs perfectly. When the ROIs are larger, that is fine - whatever the SMB is in the sector interior is the modeled SMB. It is problematic where the ROIs are smaller than the model domain. Because the largest SMB losses are at the edge, if some RCM pixels are outside of a ROI and not counted, this would introduce large errors.
+ For each SMB, determine the mask
  + HIRHAM ::
  + MAR ::
  + RACMO :: No mask provided. Look at summer season and assume any cell with any values != 0 is in the model domain, and all cells summed = 0 is outside
+ For all model domain cells outside of a ROI, enlarge the ROI to include them, and join the new ROI cells to the nearest ROI cell that is inside the model domain

** Mouginot 2019

#+NAME: import_mouginot
#+BEGIN_SRC bash

log_info "Loading Mouginot 2019"

g.mapset -c Mouginot_2019

v.import input=${DATADIR}/Mouginot_2019/Greenland_Basins_PS_v1.4.2.shp output=basins snap=1

# remove peripheral ice caps
db.execute sql="DELETE FROM basins WHERE name LIKE 'ICE_CAPS_%'"

v.db.addcolumn map=basins columns="SUBREGION1_num INT,cat_ INT"

db.execute sql="UPDATE basins SET cat_=cat"

# convert NW NO NE CW CE SW SE based on clock
db.execute sql="UPDATE basins SET SUBREGION1_num=11 WHERE SUBREGION1='NW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=12 WHERE SUBREGION1='NO'"
db.execute sql="UPDATE basins SET SUBREGION1_num=1 WHERE SUBREGION1='NE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=3 WHERE SUBREGION1='CE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=9 WHERE SUBREGION1='CW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=5 WHERE SUBREGION1='SE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=7 WHERE SUBREGION1='SW'"
#+END_SRC

** Zwally 2012

#+NAME: import_zwally
#+BEGIN_SRC bash

log_info "Loading Zwally 2012"

g.mapset -c Zwally_2012
v.import input=${DATADIR}/Zwally_2012/sectors output=sectors snap=1
#+END_SRC

* HIRHAM
:PROPERTIES:
:header-args:bash+: :tangle HIRHAM.sh
:END:

** Init

#+BEGIN_SRC bash :tangle no
grass -c EPSG:4326 G_HIRHAM

# from gridOBC_SD.nc
ncdump -v rlon ~/data/HIRHAM/gridOBC_SD.nc
ncdump -v rlat ~/data/HIRHAM/gridOBC_SD.nc
#+END_SRC

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC


** Domain

#+BEGIN_SRC bash
g.region w=-13.65 e=1.3 s=-9.8 n=12.2 res=0:03
g.region w=w-0.025 e=e+0.025 s=s-0.025 n=n+0.025 -p
g.region save=RCM

g.region res=0:0.5 # ~1 km grid cell resolution
g.region save=sectors --o
#+END_SRC

*** Test 

#+BEGIN_SRC bash :tangle no
r.in.gdal -ol input=NetCDF:${DATADIR}/HIRHAM/daily/DarcySensitivity_RP810_Daily2D_GL2LIN_Darcy_60m_liqCL_wh1_smb_HydroYr_2012_2013_DM_SD.nc:smb band=200 output=smb
r.region map=smb region=RCM

v.in.ogr -o input=./dat/Zrot.gpkg output=Z
v.in.ogr -o input=./dat/Mrot.gpkg output=M

d.mon start=wx0
r.colors map=smb color=viridis
d.erase
d.rast smb
d.vect Z fill_color=none
d.vect M fill_color=none
d.grid 1:0:0
#+END_SRC


** Convert sectors to rotated pole

+ https://lists.osgeo.org/pipermail/grass-user/2011-October/062180.html
+ This no longer works in GRASS 7.8 https://lists.osgeo.org/pipermail/grass-user/2020-November/081828.html
+ Therefore, the following is done 1x in a VM (Ubuntu 18.04) running GRASS 7.4

#+BEGIN_SRC bash :results verbatim :tangle no
rm -fR Gnorm Grot

grass -c EPSG:4326 Gnorm

DATADIR=~/data

v.import input=${DATADIR}/Zwally_2012/sectors output=Z
v.import snap=1 input=${DATADIR}/Mouginot_2019/Greenland_Basins_PS_v1.4.2.shp output=M

cat << EOF > ./Gnorm/PERMANENT/PROJ_INFO
name: General Oblique Transformation
datum: wgs84
towgs84: 0.000,0.000,0.000
proj: ob_tran
o_proj: latlon
ellps: wgs84
a: 6378137.0000000000
es: 0.0066943800
f: 298.2572235630
lat_0: 0.0000000000
lon_0: 180.0000000000
o_lat_p: 18.0
o_lon_p: -200.0
EOF

# rotated_pole:grid_north_pole_latitude = 18. ;
# rotated_pole:grid_north_pole_longitude = -200.

cat << EOF > ./Gnorm/PERMANENT/PROJ_UNITS
unit: degree
units: degrees
meters: .0174532925
EOF

grass -e -c EPSG:4326 Grot
grass ./Grot/PERMANENT

v.proj location=Gnorm input=Z
v.proj location=Gnorm input=M

# g.region vector=Z,M
# d.mon start=wx0
# d.erase
# d.vect Z
# d.vect M
# d.grid 1:0:0

v.out.ogr input=Z output=./dat/Zrot.gpkg
v.out.ogr input=M output=./dat/Mrot.gpkg
#+END_SRC

** Load sectors

This should be:

#+BEGIN_SRC bash :tangle no
<<import_mouginot>>
<<import_zwally>>
#+END_SRC

But we're loading different files that have been converted to the rotated pole, so here I **duplicate** those code blocks but change the input filename.

#+BEGIN_SRC bash
log_info "Loading Zwally 2012"

g.mapset -c Zwally_2012
v.in.ogr -o input=Zwally_2012_HIRHAM.gpkg output=sectors

v.db.dropcolumn map=sectors columns="cat_"
v.db.renamecolumn map=sectors column=cat__1,cat_
#+END_SRC

#+BEGIN_SRC bash

log_info "Loading Mouginot 2019"

g.mapset -c Mouginot_2019

v.in.ogr -o input=Mouginot_2019_HIRHAM.gpkg output=basins

# remove peripheral ice caps
db.execute sql="DELETE FROM basins WHERE name LIKE 'ICE_CAPS_%'"

v.db.addcolumn map=basins columns="SUBREGION1_num INT"

# convert NW NO NE CW CE SW SE based on clock
db.execute sql="UPDATE basins SET SUBREGION1_num=11 WHERE SUBREGION1='NW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=12 WHERE SUBREGION1='NO'"
db.execute sql="UPDATE basins SET SUBREGION1_num=1 WHERE SUBREGION1='NE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=3 WHERE SUBREGION1='CE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=9 WHERE SUBREGION1='CW'"
db.execute sql="UPDATE basins SET SUBREGION1_num=5 WHERE SUBREGION1='SE'"
db.execute sql="UPDATE basins SET SUBREGION1_num=7 WHERE SUBREGION1='SW'"

g.mapset PERMANENT
#+END_SRC
 

** Find model ice domain

+ Use all cells != 0 for sum of 2020 JAS as model domain

#+BEGIN_SRC bash
r.in.gdal -ol input="NetCDF:${DATADIR}/HIRHAM/ZwallyMasks_all_SD.nc:glacmask" output=mask
r.region map=mask region=RCM
r.mapcalc "mask_ice_all = if(mask == 1, 1, null())"

r.clump input=mask_ice_all output=mask_ice_clump
main_clump=$(r.stats -c -n mask_ice_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if(mask_ice_clump == ${main_clump}, 1, null())"
#+END_SRC

** Expand sectors to cover model domain

We'll develop this here on the HIRHAM data 1x, but do so generically so that when working with MAR and RACMO we can just <<expand_sectors>>. The one requirement is that we expect a "mask_ice" raster in the PERMANENT mapset which defines the ice domain.

#+NAME: expand_sectors
#+BEGIN_SRC bash :tangle no
<<expand_zwally>>
<<expand_mouginot>>
#+END_SRC

*** Zwally

#+NAME: expand_zwally
#+BEGIN_SRC bash

log_info "Expanding Zwally sectors to cover RCM domain"

g.mapset Zwally_2012
g.region region=sectors

v.to.rast input=sectors output=sectors use=attr attribute_column=cat_
# r.mapcalc "outside = if(isnull(sectors) & not(isnull(mask_ice)), 1, null())"

# Works fine if limited to contiguous cells (hence main_clump).
# Deosn't work great for distal islands (e.g. NE GL).
# Probably need to jump to v.distance or r.distance if we want to assign these to sectors.
r.grow.distance input=sectors value=value
r.mapcalc "sectors_e = if(mask_ice, int(value), null())" # sectors_enlarged
r.to.vect input=sectors_e output=sectors_e type=area
#+END_SRC


*** Mouginot

#+NAME: expand_mouginot
#+BEGIN_SRC bash

log_info "Expanding Mouginot basins to cover RCM domain"

g.mapset Mouginot_2019
g.region region=sectors

v.to.rast input=basins output=basins use=attr attribute_column=cat_ labelcolumn=SUBREGION1
# r.mapcalc "outside = if(isnull(basins) & mask_ice, 1, null())"
r.grow.distance input=basins value=value_b
r.mapcalc "basins_e = if(mask_ice, int(value_b), null())"
r.category map=basins separator=":" > ./tmp/basins_cats
r.category map=basins_e separator=":" rules=./tmp/basins_cats
r.to.vect input=basins_e output=basins_e type=area

v.to.rast input=basins output=regions use=attr attribute_column=SUBREGION1_num labelcolumn=SUBREGION1
# r.mapcalc "outside = if(isnull(regions) & mask_ice, 1, null())"
r.grow.distance input=regions value=value_r
r.mapcalc "regions_e = if(mask_ice, int(value_r), null())"
r.category map=regions separator=":" > ./tmp/region_cats
r.category map=regions_e separator=":" rules=./tmp/region_cats
r.to.vect input=regions_e output=regions_e type=area
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_HIRHAM/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:




* MAR
:PROPERTIES:
:header-args:bash+: :tangle MAR.sh
:END:
** Init

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC


** Set up GRASS location

#+BEGIN_SRC bash :tangle no :results verbatim
cdo sinfov ${DATADIR}/MAR/3.12/MAR-2000.nc | head -n20
#+END_SRC

#+RESULTS:
#+begin_example
Warning (cdfInqContents): Coordinates variable time can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable time can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
Warning (cdfInqContents): Coordinates variable y can't be assigned!
Warning (cdfInqContents): Coordinates variable x can't be assigned!
   File format : NetCDF4 classic zip
    -1 : Institut Source   T Steptype Levels Num    Points Num Dtype : Parameter name
     1 : unknown  unknown  v instant       2   1     10336   1  F32z : smb           
     2 : unknown  unknown  v instant       2   1     10336   1  F32z : ru            
     3 : unknown  unknown  c instant       1   2     10336   1  F32z : lon           
     4 : unknown  unknown  c instant       1   2     10336   1  F32z : lat           
     5 : unknown  unknown  c instant       1   2     10336   1  F32z : sh            
     6 : unknown  unknown  c instant       1   2     10336   1  F32z : msk           
     7 : unknown  unknown  c instant       1   2     10336   1  F32z : srf           
   Grid coordinates :
     1 : projection               : points=10336 (76x136)
                          mapping : polar_stereographic
                                x : -640 to 860 by 20 km
                                y : -3347.928 to -647.9277 by 20 km
   Vertical coordinates :
     1 : generic                  : levels=2
                           sector : 1 to 2 level
     2 : surface                  : levels=1
   Time coordinate :  366 steps
     RefTime =  1993-09-01 00:00:00  Units = days  Calendar = standard
#+end_example

#+BEGIN_SRC bash
g.region w=-640000 e=860000 s=-3347928 n=-647928 res=20000 -p
g.region w=w-10000 e=e+10000 s=s-10000 n=n+10000 res=20000 -p # adjust from cell center to edges
g.region save=RCM
g.region res=1000 -p
g.region save=sectors
#+END_SRC

** Ice mask
#+BEGIN_SRC bash
r.external -o source=NetCDF:${DATADIR}/MAR/3.12/MAR-2000.nc:msk output=mask
r.region map=mask region=RCM
r.colors map=mask color=haxby

# r.mapcalc "mask_ice_all = if(mask == 0, null(), 1)"
r.mapcalc "mask_ice_1 = if(mask >= 50, 1, null())"
r.grow input=mask_ice_1 output=mask_ice_all radius=3 new=1

r.clump input=mask_ice_all output=mask_clump
main_clump=$(r.stats -c -n mask_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if((mask_clump == ${main_clump}) & (mask > 0.5), 1, null())"
#+END_SRC

** Sectors

#+BEGIN_SRC bash
<<import_zwally>>
<<expand_zwally>>

<<import_mouginot>>
<<expand_mouginot>>

g.mapset PERMANENT
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_MAR/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:



* RACMO
:PROPERTIES:
:header-args:bash+: :tangle RACMO.sh
:END:

** Set up GRASS location

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC

#+BEGIN_SRC bash :tangle no :results verbatim
cdo -sinfo ${DATADIR}/RACMO/daily/smb_rec.2020_JAS.BN_RACMO2.3p2_ERA5_3h_FGRN055.1km.DD.nc | head -n 9
# x : -638956 to 856044 by 1000 km
# y : -3354596 to -655596 by 1000 km
#+END_SRC

#+RESULTS:
#+begin_example
cdo    sinfo: Processed 3 variables over 92 timesteps [0.00s 50MB].
   File format : NetCDF
    -1 : Institut Source   T Steptype Levels Num    Points Num Dtype : Parameter ID
     1 : unknown  unknown  c instant       1   1   4039200   1  F32  : -1            
     2 : unknown  unknown  c instant       1   1   4039200   1  F32  : -2            
     3 : unknown  unknown  v instant       1   1   4039200   1  F32  : -3            
   Grid coordinates :
     1 : generic                  : points=4039200 (1496x2700)
                                x : -638956 to 856044 by 1000 km
                                y : -3354596 to -655596 by 1000 km
#+end_example

#+BEGIN_SRC bash
g.region w=-638956 e=856044 s=-3354596 n=-655596 res=1000 -p
g.region n=n+500 s=s-500 w=w-500 e=e+500 res=1000 -p
g.region save=RCM

g.region res=1000 -p
g.region save=sectors
#+END_SRC

** Ice mask
#+BEGIN_SRC bash
r.external -o source=NetCDF:${DATADIR}/RACMO/Icemask_Topo_Iceclasses_lon_lat_average_1km.nc:Promicemask output=mask
r.region map=mask region=RCM
r.colors map=mask color=haxby

r.mapcalc "mask_ice_all = if(mask >= 2, 1, null())"

r.clump input=mask_ice_all output=mask_clump
main_clump=$(r.stats -c -n mask_clump sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ice = if((mask_clump == ${main_clump}) & (mask > 0.5), 1, null())"
#+END_SRC

** Reproject sectors to RCM grid

+ Nothing to do here because RACMO on EPSG:3413 projection.

** Sectors

#+BEGIN_SRC bash
<<import_zwally>>
<<expand_zwally>>

<<import_mouginot>>
<<expand_mouginot>>

g.mapset PERMANENT
#+END_SRC

** Test location alignment

#+BEGIN_SRC bash :tangle no
grass ./G_RACMO/PERMANENT
g.mapset PERMANENT
d.mon start=wx0
d.erase

d.rast mask_ice
# d.vect basins@Mouginot_2019 fill_color=none
# d.vect sectors@Zwally_2012 fill_color=none

d.rast sectors_e@Zwally_2012
d.rast basins_e@Mouginot_2019
d.rast regions_e@Mouginot_2019
#+END_SRC

#+RESULTS:




* SMB to ROIs
** Print dates

#+BEGIN_SRC jupyter-python :tangle nc_dates.py
import xarray as xr
import sys

f = sys.argv[1]

ds = xr.open_dataset(f)

if 'time' in ds.variables:
    tvar = 'time'
if 'TIME' in ds.variables:
    tvar = 'TIME'
    
t = [(str(_)[0:10]) for _ in ds[tvar].values]
for _ in t: print(_)
#+END_SRC

** HIRHAM

#+BEGIN_SRC bash :results verbatim :tangle SMB_HIRHAM_ROI.sh
<<init_bash>>

RCM=HIRHAM
mkdir -p tmp/${RCM}

dir=${DATADIR}/${RCM}/daily
f=$(ls ${dir}/*.nc|head -n1) # debug

if [ -z ${OP+x} ]; then
  f_list=$(ls ${dir}/*.nc) ## initial
  log_info "Initial run. Processing all files"
else
  f_list=$(ls ${dir}/*.nc | tail -n 30) ## operational
  log_warn "OP set to ${OP}. Processing subset of files"
fi

for f in ${f_list}; do
  dates=$(python ./nc_dates.py ${f})
  band=0
  d=1985-09-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 1 ))

    log_info "HIRHAM: ${d}"

    if [[ -e ./tmp/${RCM}/sector_${d}.bsv ]]; then continue; fi

    var=smb
    if [[ ${f} == *"SMBmodel"* ]]; then var=gld; fi

    r.in.gdal -ol input="NetCDF:${f}:${var}" band=${band} output=smb --o --q
    r.region map=smb region=RCM
    
    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/sector_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/region_${d}.bsv

    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/basin_${d}.bsv
    
  done
done

# log_info "HIRHAM ROI areas"
# r.in.gdal -ol input="NetCDF:${DATADIR}/HIRHAM/ZwallyMasks_all_SD.nc:cellarea" output=area
# r.region map=area region=RCM

# r.univar -t --q map=area zones=sectors_e@Zwally_2012 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/HIRHAM_sector_area.bsv

# r.univar -t --q map=area zones=basins_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/HIRHAM_basin_area.bsv

# r.univar -t --q map=area zones=regions_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/HIRHAM_region_area.bsv
#+END_SRC

** MAR
#+BEGIN_SRC bash :results verbatim :tangle SMB_MAR_ROI.sh

<<init_bash>>

RCM=MAR
mkdir -p tmp/${RCM}

dir=${DATADIR}/MAR/3.12
f=$(ls ${dir}/MAR-????.nc|head -n1) # debug

if [ -z ${OP+x} ]; then
  f_list=$(ls ${dir}/*.nc) ## initial
  log_info "Initial run. Processing all files"
else
  f_list=$(ls ${dir}/*.nc | tail -n 2) ## operational
  log_warn "OP set to ${OP}. Processing subset of files"
fi

for f in ${f_list}; do
  dates=$(python ./nc_dates.py ${f})

  ###
  ### "band = -1" and "band = band + 2"
  ### This code is necessary because the smb raster in the NetCDF files
  ### has dimensions (time, sector, y, x) where sector, per Xavier, is:
  ### "k=1 permanent ice, k=2 tundra". GRASS exposes these dimensions as
  ### even and odd band numbers, so 1 is Jan 1 ice, and 2 is jan 1 tundra
  ### and 3 is Jan 2 ice, etc.
  ###
  
  band=-1  
  d=1986-01-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 2 ))

    log_info "${RCM}: ${d}"

    if [[ -e ./tmp/${RCM}/sector_${d}.bsv ]]; then continue; fi

    r.in.gdal -o input="NetCDF:${f}:smb" band=${band} output=smb_raw --o --q
    r.region map=smb_raw region=RCM
    r.mapcalc "smb = if(mask > 50, smb_raw * (mask/100), null())" --o

    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/sector_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/region_${d}.bsv

    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/basin_${d}.bsv
    
  done
done

# log_info "${RCM} ROI areas"
# r.in.gdal -o input="NetCDF:${DATADIR}/MAR/3.12/MAR_basins.nc:AREA" output=area
# r.region map=area region=RCM

# r.univar -t --q map=area zones=sectors_e@Zwally_2012 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_sector_area.bsv

# r.univar -t --q map=area zones=basins_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_basin_area.bsv

# r.univar -t --q map=area zones=regions_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_region_area.bsv

#+END_SRC


** RACMO
#+BEGIN_SRC bash :results verbatim :tangle SMB_RACMO_ROI.sh

<<init_bash>>

RCM=RACMO
mkdir -p tmp/${RCM}

dir=${DATADIR}/${RCM}/daily
f=$(ls ${dir}/*.nc|head -n1) # debug

if [ -z ${OP+x} ]; then
  f_list=$(ls ${dir}/*.nc) ## initial
  log_info "Initial run. Processing all files"
else
  f_list=$(ls ${dir}/*.nc | tail -n 2) ## operational
  log_warn "OP set to ${OP}. Processing subset of files"
fi

for f in ${f_list}; do
  dates=$(python ./nc_dates.py ${f})
  band=0
  d=1989-07-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 1 ))

    log_info "${RCM}: ${d}"

    if [[ -e ./tmp/${RCM}/sector_${d}.bsv ]]; then continue; fi

    var=SMB_rec
    if [[ ${f} == *"ERA5"* ]]; then var=smb_rec; fi

    r.in.gdal -o input="NetCDF:${f}:${var}" band=${band} output=smb --o  --q
    r.region map=smb region=RCM

    r.univar -t --q map=smb zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/sector_${d}.bsv

    r.univar -t --q map=smb zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/region_${d}.bsv
    
    r.univar -t --q map=smb zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/${RCM}/basin_${d}.bsv

  done
done


# log_info "${RCM} ROI areas"
# r.region map=area region=RCM

# r.univar -t --q map=area zones=sectors_e@Zwally_2012 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_sector_area.bsv

# r.univar -t --q map=area zones=basins_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_basin_area.bsv

# r.univar -t --q map=area zones=regions_e@Mouginot_2019 \
# | cut -d"|" -f1,13 \
# | datamash -t"|" transpose \
# | sed s/^sum/${d}/ \
# > ./tmp/${RCM}_region_area.bsv
	
#+END_SRC


** Create data file
*** Merge from daily to single file for each RCM and ROI

#+BEGIN_SRC bash :tangle smb_merge.sh
<<init_bash>>

for RCM in HIRHAM MAR RACMO; do
  for ROI in sector region basin; do
    log_info ${RCM} ${ROI}
    head -n1 ./tmp/${RCM}/${ROI}_2000-01-01.bsv > ./tmp/${RCM}_${ROI}.bsv
    tail -q -n1 ./tmp/${RCM}/${ROI}_*.bsv >> ./tmp/${RCM}_${ROI}.bsv
  done
done
#+END_SRC

#+RESULTS:

*** BSV to NetCDF

For HIRHAM, resolution is in degrees not meters.
+ 1 degree of latitude @ equator is 110574 m => 0.5 minutes or 0:30 seconds or 0.008333 degrees = 0.008333 * 110574 = 921.413142
+ 1 degree of latitude @ 10 °N is 110608 m => 110608 * 0.0083333 = 921.7296464 m

#+NAME: smb_bsv2nc
#+BEGIN_SRC jupyter-python :tangle smb_bsv2nc.py
import pandas as pd
import xarray as xr
import numpy as np
import datetime
import os
import uncertainties
from uncertainties import unumpy

time = pd.date_range(start = "1986-01-01",
                     end = datetime.datetime.utcnow().date() + datetime.timedelta(days = 7),
                     freq = "D")

smb = xr.Dataset()
smb["time"] = (("time"), time)

smb["sector"] = pd.read_csv("./tmp/HIRHAM_sector.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)

rstr = {11:'NW', 12:'NO', 1:'NE', 3:'CE', 9:'CW', 5:'SE', 7:'SW'}
rnum = pd.read_csv("./tmp/HIRHAM_region.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)
smb["region"] = [rstr[n] for n in rnum]
# smb["basin"] = pd.read_csv("./tmp/HIRHAM_basin.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)

def bsv2nc(smb, bsv, dim):
    df = pd.read_csv("./tmp/" + bsv + ".bsv", delimiter="|", index_col=0, parse_dates=True).reindex(time)
    df.columns = df.columns.astype(np.int64)
    if dim == "region":
        df.columns = [rstr[n] for n in df.columns]
    missing = smb[dim].values[~pd.Series(smb[dim].values).isin(df.columns)]
    if len(missing) > 0:
        df[missing] = np.nan
        df = df.reindex(sorted(df.columns), axis=1)
 
    if 'HIRHAM' in bsv:
        grid = 912 * 912
        error = 0.15
    if 'MAR' in bsv:
        grid = 1000 * 1000
        error = 0.15
    if 'RACMO' in bsv:
        grid = 1000 * 1000
        error = 0.15
        
    #       mm->m  grid  m^3->kg  kg->Gt
    CONV = 1E-3  * grid * 1E3    / 1E12
    df = df * CONV
    
    smb[bsv] = (("time", dim), df)
    smb[bsv + '_err'] = (("time", dim), df*error)
    return smb

smb = bsv2nc(smb, "HIRHAM_sector", "sector")
smb = bsv2nc(smb, "HIRHAM_region", "region")
# smb = bsv2nc(smb, "HIRHAM_basin", "basin")

smb = bsv2nc(smb, "MAR_sector", "sector")
smb = bsv2nc(smb, "MAR_region", "region")
# smb = bsv2nc(smb, "MAR_basin", "basin")

smb = bsv2nc(smb, "RACMO_sector", "sector")
smb = bsv2nc(smb, "RACMO_region", "region")
# smb = bsv2nc(smb, "RACMO_basin", "basin")



err = unumpy.uarray([1,1,1], [0.1, 0.15, 0.7])
print('{:.4f}'.format(err.sum()))

### create SMB from individual SMBs
# for roi in ['sector','region','basin']:
for roi in ['sector','region']:
    mean = smb[['HIRHAM_'+roi, 'MAR_'+roi, 'RACMO_'+roi]].to_array(dim='m').mean('m')
    s = 'smb_' + roi
    smb[s] = (('time',roi), mean)
    # smb[s].attrs["long_name"] = "Surface mass balance (RCM mean)"
    # smb[s].attrs["standard_name"] = "land_ice_mass_tranport"
    # smb[s].attrs["units"] = "Gt d-1"
    # smb[s].attrs["coordinates"] = "time region"

    s = s + '_err'
    smb[s] = (('time',roi), mean * 0.15)
    # smb[s].attrs["long_name"] = "Surface mass balance (RCM mean) uncertainty"
    # smb[s].attrs["standard_name"] = "land_ice_mass_tranport"
    # smb[s].attrs["units"] = "Gt d-1"
    # smb[s].attrs["coordinates"] = "time region"

smb['smb'] = (('time'), smb['smb_sector'].sum(dim='sector'))
smb['smb_err'] = (('time'), smb['smb_sector'].sum(dim='sector') * 0.09)

fn = './tmp/smb.nc'
if os.path.exists(fn): os.remove(fn)
smb.to_netcdf(fn)
#+END_SRC

#+RESULTS: smb_bsv2nc
: 3.0000+/-0.7228


Test:

#+BEGIN_SRC jupyter-python
import xarray as xr
ds = xr.open_dataset('./tmp/smb.nc')
ds[['smb','HIRHAM_sector','MAR_sector','RACMO_sector']].sum(dim='sector').to_dataframe().plot()
ds[['smb','HIRHAM_sector','MAR_sector','RACMO_sector']].sum(dim='sector').to_dataframe().head()
#+END_SRC

#+RESULTS:
| time                |     smb | HIRHAM_sector | MAR_sector | RACMO_sector |
|---------------------+---------+---------------+------------+--------------|
| 1986-01-01 00:00:00 |  2.3474 |       2.41357 |    2.34658 |      2.28207 |
| 1986-01-02 00:00:00 | 3.20165 |       3.24823 |    3.35151 |      3.00523 |
| 1986-01-03 00:00:00 | 4.22387 |       4.19169 |     4.1993 |      4.28062 |
| 1986-01-04 00:00:00 |  1.5914 |       1.66981 |    1.82024 |      1.28414 |
| 1986-01-05 00:00:00 | 1.97892 |       1.97788 |    2.44208 |      1.51681 |

* MMB to ROI

Already done in each of the MMB products
+ citet:mankoff_2020_solid includes ROI for each gate
+ citet:mouginot_2019_forty is provided on Zwally sector

* BMB to ROI
:PROPERTIES:
:header-args:bash+: :tangle BMB.sh
:END:

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC

** GF
*** Import 
#+BEGIN_SRC bash
g.mapset -c GF

r.external -o source=NetCDF:${DATADIR}/Karlsson_2021/basalmelt.nc:gfmelt output=GF_ann
g.region raster=GF_ann

r.mapcalc "GF = GF_ann / 365"
#+END_SRC
*** Apply

#+BEGIN_SRC bash
r.univar -t --q map=GF zones=sectors_e@Zwally_2012 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_GF_sector.bsv

r.univar -t --q map=GF zones=basins_e@Mouginot_2019 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_GF_basin.bsv

r.univar -t --q map=GF zones=regions_e@Mouginot_2019 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_GF_region.bsv
#+END_SRC

** Velocity
*** Import
#+BEGIN_SRC bash
g.mapset -c vel

r.external -o source=NetCDF:${DATADIR}/Karlsson_2021/basalmelt.nc:fricmelt output=vel_ann
g.region raster=vel_ann

r.mapcalc "vel = vel_ann / 365"
#+END_SRC

*** Apply

TODO: Convert vel from NBK units to...  m melt per grid cell?

#+BEGIN_SRC bash
r.univar -t --q map=vel zones=sectors_e@Zwally_2012 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_vel_sector.bsv

r.univar -t --q map=vel zones=basins_e@Mouginot_2019 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_vel_basin.bsv

r.univar -t --q map=vel zones=regions_e@Mouginot_2019 \
  | cut -d"|" -f1,13 \
  | datamash -t"|" transpose \
  | sed s/^sum/daily/ \
  > ./tmp/BMB_vel_region.bsv
#+END_SRC

** VHD
*** Setup
**** Source contribution map

+ Determine VHD routing map as per citet:mankoff_2017_VHD
+ For each source cell, estimate the contribution to that sector VHD per unit mass of water
  + That is, from the water source, the integrated VHD between source and outlet.
  + Rather than doing full routing for each and every interior cell, the integrated VHD per cell can be estimated as the difference between the source pressure+elevation and the basal pressure+elevation terms.
+ Then, use this source map applied to each day of runoff from one of the RCMs.

Do the initial work in BedMachine (3413), then re-project into MAR because that is the RCM with future data.

**** Import BedMachine v4
+ from [[textcite:Morlighem:2017BedMachine][Morlighem /et al./ (2017)]]
+ See https://github.com/GEUS-Glaciology-and-Climate/ice_discharge/issues/26

#+BEGIN_SRC bash :results verbatim
log_info "Importing BedMachine"

g.mapset -c VHD

for var in $(echo mask surface bed thickness); do
  echo $var
  r.external source=${DATADIR}/Morlighem_2017/BMv4_3413/${var}.tif output=${var}
done

g.region raster=surface
g.region res=1000
g.region save=BedMachine

r.colors map=mask color=haxby
r.mapcalc "mask_ice_0 = if(mask == 2, 1, null())"
#+END_SRC

**** Expand Mask

The ice mask needs to be expanded so that land terminating glaciers contain 1 grid cell outside the ice domain. This is so that the discharge location has 0 thickness (0 pressure term) and all pressure energy is released. Only gravitational potential energy remains. Submarine discharge remains pressurized by the ice thickness.

#+BEGIN_SRC bash
r.grow input=mask_ice_0 output=mask_ice_1 radius=1.5 new=1
r.mapcalc "mask_01 = if((mask == 0) | (mask == 3), null(), mask_ice_1)"
#+END_SRC

**** Fill in small holes

Also fills in nunatuks.

This is done because hydrologic routing will terminate at domain boundaries, even if they're inland. We want to route to the ice edge, because eventually that is where all the water goes.

#+BEGIN_SRC bash :results verbatim
r.colors map=mask color=haxby
r.mapcalc "not_ice = if(isnull(mask_01) ||| (mask != 2), 1, 0)"

# No mask, NULLS are not clumped
r.clump input=not_ice output=clumps
# d.rast clumps
main_clump=$(r.stats -c -n clumps sort=desc | head -n1 | cut -d" " -f1)
r.mask -i raster=clumps maskcats=${main_clump} --o

r.mapcalc "all_ice = 1"
r.clump input=all_ice output=clumps2
# d.rast clumps2
main_clump=$(r.stats -c -n clumps2 sort=desc | head -n1 | cut -d" " -f1)
r.mask raster=clumps2 maskcats=${main_clump} --o

r.mapcalc "mask_ice = MASK"
# ice mask with no islands

# # original mask ice
# r.mask -r
# r.mapcalc "mask_ice_islands = if(mask == 2, 1, null())"
#+END_SRC
#+RESULTS:


**** Hydropotential head

#+BEGIN_SRC bash :results verbatim
log_info "Calculating subglacial head with k = 1.0"
r.mapcalc "pressure = 1 * 0.917 * thickness"
r.mapcalc "head = mask_ice * bed + pressure"
#+END_SRC

***** Streams

After calculating the head, we use 3rd party tools to get the flow direction and streams

#+NAME: streams
#+BEGIN_SRC bash :results verbatim
THRESH=300
log_warn "Using threshold: ${THRESH}"
log_info "r.stream.extract..."

r.stream.extract elevation=head threshold=${THRESH} memory=16384 direction=dir stream_raster=streams stream_vector=streams
#+END_SRC

***** Outlets

+ The flow direction =dir= is negative where flow leaves the domain. These are the outlets.
+ Encode each outlet with a unique id

#+NAME: outlets
#+BEGIN_SRC bash :results verbatim
log_info "Calculating outlets"
r.mapcalc "outlets_1 = if(dir < 0, 1, null())"
r.out.xyz input=outlets_1 | \
    cat -n | \
    tr '\t' '|' | \
    cut -d"|" -f1-3 | \
    v.in.ascii input=- output=outlets_uniq separator=pipe \
        columns="x int, y int, cat int" x=2 y=3 cat=1
#+END_SRC

***** Basins

Using =r.stream.basins=, we can get basins for every outlet.

#+NAME: basins
#+BEGIN_SRC bash :results verbatim
log_info "r.stream.basins..."

r.stream.basins -m direction=dir points=outlets_uniq basins=basins_uniq memory=16384 --verbose
#+END_SRC

**** Change in head between each cell and its outlet

#+BEGIN_SRC bash
r.stream.distance -o stream_rast=outlets_1 direction=dir elevation=head method=downstream difference=delta_head
r.stream.distance -o stream_rast=outlets_1 direction=dir elevation=bed method=downstream difference=delta_z_head
r.stream.distance -o stream_rast=outlets_1 direction=dir elevation=pressure method=downstream difference=delta_p_head
#+END_SRC

The effective head change is the change in the elevation head and the change in the pressure head, minus the change in pressure head that is occupied with the effect of changing pressure on the changing phase transition temperature (PTT; citet:mankoff_2017_VHD).

The relationship between changing pressure and changing PTT is,

#+NAME: eq:PTT
\begin{equation}
PTT = C_T C_p \rho_w,
\end{equation}

with \(C_T\) the Clausius-Clapeyron slope (8.6E-8 K Pa-1), \(c_p\) the specific heat of water (4184 J K-1 kg-1), and \(\rho_w\) the density of water (1000 kg m-3).

#+BEGIN_SRC bash :tangle no :results verbatim
frink "(8.6E-8 K Pa^(-1)) * (4184 J K^(-1) kg^(-1)) * (1000 kg m^(-3))"
#+END_SRC

#+RESULTS:
: 0.359824

Meaning 0.36 of the pressure reduction energy release is "lost", warming the water to match the increased PTT, and 1-0.36 = 0.64 of the pressure reduction energy release can be used to melt basal ice.

From citet:mankoff_2017_VHD or citet:karlsson_2021:

#+BEGIN_SRC bash
# The effective change in head is...
r.mapcalc "dh = delta_z_head + 0.64 * delta_p_head"
# also "dh = delta_head - 0.36 * delta_p_head"

# Energy (J) is m*g*z
r.mapcalc 'q_z = (1000 * 9.8 * delta_z_head)'
r.mapcalc 'q_p = (1000 * 9.8 * 0.64 * delta_p_head)'
r.mapcalc 'q = (1000 * 9.8 * dh)'

# r.mapcalc "q = (1000 * 9.8 * delta_head) - 0.36 * (917 * 9.8 * delta_p_head)"
r.mapcalc "unit_melt = q / (335 * 1000)" # 335 kJ/kg ice


# Other BMB is m/day equivalent melt, so we should aim for that.
# r.mapcalc "vhd = unit_melt * (10^-3.0) * if(mask@PERMANENT > 50, ru_raw * (mask@PERMANENT/100), null())" --o
## The above is slow (42 s). Let's speed it up.
## Calculate everything except 'ru_raw' which must be done in the loop (elsewhere)
r.mapcalc "scale = unit_melt * (10^-3.0) * if(mask@PERMANENT > 50, (mask@PERMANENT/100), null())" # 1x @ 30 s

#+END_SRC

***** Debug

+ Mask ice should be 1 pixel wider on land and 0 pixels wider for marine terminating glaciers. This is so that for land-terminating glaciers, the pressure head drops to 0 (thickness is 0) at the outlet, but for marine terminating glaciers, the pressure head should remain at ice thickness.

+ Click around... record values at an marine terminating outlet, and then click upstream on or off the stream and check that delta_z_head and delta_p_head make sense. Repeat for land-terminating outlet.  

#+BEGIN_SRC bash :tangle no
g.mapset -c tmp

r.to.vect input=mask output=mask type=area
r.to.vect input=mask_ice output=mask_ice type=area
r.to.vect input=basins_uniq output=basins type=area

d.mon wx0
d.erase

d.rast surface
d.rast bed
d.rast thickness
d.rast pressure
d.rast dir
d.rast delta_head
d.rast delta_p_head
d.rast delta_z_head

d.vect basins fill_color=none color=grey width=3
d.vect mask fill_color=none color=black width=3
d.vect mask_ice fill_color=none color=red
d.vect streams
d.vect outlets_uniq color=cyan
#+END_SRC

*** Apply
:PROPERTIES:
:header-args:bash+: :tangle BMB_MAR.sh
:END:

#+BEGIN_SRC bash
<<init_bash>>
<<init_grass>>
#+END_SRC

**** MAR daily partitioned to ROI

#+BEGIN_SRC bash :results verbatim
g.mapset -c VHD

RCM=MAR
mkdir -p tmp/BMB

dir=${DATADIR}/${RCM}/3.12
f=$(ls ${dir}/MAR-????.nc|head -n1) # debug

if [ -z ${OP+x} ]; then
  f_list=$(ls ${dir}/*.nc) ## initial
  log_info "Initial run. Processing all files"
else
  f_list=$(ls ${dir}/*.nc | tail -n 2) ## operational
  log_warn "OP set to ${OP}. Processing subset of files"
fi

for f in ${f_list}; do
  dates=$(python ./nc_dates.py ${f})
  band=-1
  d=1986-01-01 # debug
  for d in ${dates}; do
    band=$(( ${band} + 2 ))

    log_info "MAR BMB: ${d} (band: ${band})"

    if [[ -e ./tmp/BMB/sector_${d}.bsv ]]; then continue; fi

    r.in.gdal -o input="NetCDF:${f}:ru" band=${band} output=ru_raw --o --q
    r.region map=ru_raw region=RCM

    r.mapcalc "vhd = scale * ru_raw" --o
    r.null map=vhd null=0

    r.univar -t --q map=vhd zones=sectors_e@Zwally_2012 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/BMB/sector_${d}.bsv

    r.univar -t --q map=vhd zones=basins_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/BMB/basin_${d}.bsv

    r.univar -t --q map=vhd zones=regions_e@Mouginot_2019 \
    | cut -d"|" -f1,13 \
    | datamash -t"|" transpose \
    | sed s/^sum/${d}/ \
    > ./tmp/BMB/region_${d}.bsv
    
  done
done
#+END_SRC

*** INFO How much water lost at edge cells?

#+BEGIN_SRC bash :tangle no
grass ./G_MAR/VHD

# unit_melt: Where (and how much) melting occurs from BedMachine
# mask_ice: Where MAR mask is.

r.mapcalc "mask_bedmachine = if(unit_melt)"

r.report -i -h units=k map=mask_bedmachine
r.report -i -h units=k map=mask_ice
r.report -i --q -h units=k map=mask_bedmachine,mask_ice
#+END_SRC



** Create data file
*** Merge BSVs

#+BEGIN_SRC bash :tangle bmb_merge.sh :results verbatim
<<init_bash>>

for ROI in sector region basin; do
  log_info MAR ${ROI}
  head -n1 ./tmp/BMB/${ROI}_2000-01-01.bsv > ./tmp/BMB_VHD_${ROI}.bsv
  tail -q -n1 ./tmp/BMB/${ROI}_*.bsv >> ./tmp/BMB_VHD_${ROI}.bsv
done
#+END_SRC

#+RESULTS:
: 
: $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ > > > > [0;32m[2021-02-03T13:54:26-08:00] [INFO] MAR sector[0m
: [0;32m[2021-02-03T13:54:26-08:00] [INFO] MAR region[0m
: [0;32m[2021-02-03T13:54:26-08:00] [INFO] MAR basin[0m


*** BSV to NetCDF

# #+NAME: bmb_bsv2nc
#+BEGIN_SRC jupyter-python :tangle bmb_bsv2nc.py
import pandas as pd
import xarray as xr
import numpy as np
import datetime
import os

time = pd.date_range(start = "1986-01-01",
                     end = datetime.datetime.utcnow().date() + datetime.timedelta(days = 7),
                     freq = "D")

bmb = xr.Dataset()
bmb["time"] = (("time"), time)

bmb["sector"] = pd.read_csv("./tmp/BMB_GF_sector.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)

rstr = {11:'NW', 12:'NO', 1:'NE', 3:'CE', 9:'CW', 5:'SE', 7:'SW'}
rnum = pd.read_csv("./tmp/BMB_GF_region.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)
bmb["region"] = [rstr[n] for n in rnum]
# bmb["basin"] = pd.read_csv("./tmp/BMB_GF_basin.bsv", delimiter="|", nrows=0, index_col=0).columns.astype(int)

bmb['GF_sector'] = (('sector'),
                    pd.read_csv("./tmp/BMB_GF_sector.bsv",
                                delimiter="|", index_col=0).values.flatten())
bmb['vel_sector'] = (('sector'),
                     pd.read_csv("./tmp/BMB_vel_sector.bsv",
                                 delimiter="|", index_col=0).values.flatten())
bmb['VHD_sector'] = (('time','sector'),
                     pd.read_csv("./tmp/BMB_VHD_sector.bsv",
                                 delimiter="|", index_col=0, parse_dates=True).reindex(time))

bmb['GF_region'] = (('region'),
                    pd.read_csv("./tmp/BMB_GF_region.bsv",
                                delimiter="|", index_col=0).values.flatten())
bmb['vel_region'] = (('region'),
                     pd.read_csv("./tmp/BMB_vel_region.bsv",
                                 delimiter="|", index_col=0).values.flatten())
bmb['VHD_region'] = (('time','region'),
                     pd.read_csv("./tmp/BMB_VHD_region.bsv",
                                 delimiter="|", index_col=0, parse_dates=True).reindex(time))

# bmb['GF_basin'] = (('basin'),
#                     pd.read_csv("./tmp/BMB_GF_basin.bsv",
#                                 delimiter="|", index_col=0).values.flatten())
# bmb['vel_basin'] = (('basin'),
#                      pd.read_csv("./tmp/BMB_vel_basin.bsv",
#                                  delimiter="|", index_col=0).values.flatten())

# df = pd.read_csv("./tmp/BMB_VHD_basin.bsv",
#                  delimiter="|", index_col=0, parse_dates=True).reindex(time)
# df.columns = df.columns.astype(np.int64)
# missing = bmb['basin'].values[~pd.Series(bmb['basin'].values).isin(df.columns)]
# if len(missing) > 0:
#     df[missing] = np.nan
#     df = df.reindex(sorted(df.columns), axis=1)
# bmb['VHD_basin'] = (('time','basin'), df)


# VHD is in mm w.eq / day
# scale from my computed daily VHD to the same units NBK BMB is on, which is [m w.eq]
# for roi in ['sector','region','basin']:
for roi in ['sector','region']:
    v = 'VHD_'+roi
    bmb[v] = (('time',roi), bmb[v] * 1E-3)


#          grid cells    m^3->kg  kg->Gt
bmb = bmb * 1000 * 1000 * 1E3    / 1E12

bmb['GF_sector_err'] = bmb['GF_sector'] * 0.5
bmb['vel_sector_err'] = bmb['vel_sector'] * 0.333
bmb['VHD_sector_err'] = bmb['VHD_sector'] * 0.15
bmb['GF_region_err'] = bmb['GF_region'] * 0.5
bmb['vel_region_err'] = bmb['vel_region'] * 0.333
bmb['VHD_region_err'] = bmb['VHD_region'] * 0.15

fn = './tmp/bmb.nc'
if os.path.exists(fn): os.remove(fn)
bmb.to_netcdf(fn)

print(bmb)
# bmb = xr.open_dataset('./tmp/bmb.nc')
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:         (region: 7, sector: 19, time: 13002)
Coordinates:
  ,* time            (time) datetime64[ns] 1986-01-01 1986-01-02 ... 2021-08-06
  ,* sector          (sector) int64 11 12 13 14 21 22 31 ... 50 61 62 71 72 81 82
  ,* region          (region) <U2 'NE' 'CE' 'SE' 'SW' 'CW' 'NW' 'NO'
Data variables:
    GF_sector       (sector) float64 0.0008361 0.0002763 ... 0.00142 0.0001805
    vel_sector      (sector) float64 0.001486 0.0002814 ... 0.005776 0.0002316
    VHD_sector      (time, sector) float64 4.462e-17 2.422e-17 ... nan nan
    GF_region       (region) float64 0.003765 0.001524 ... 0.001649 0.001317
    vel_region      (region) float64 0.003019 0.003583 ... 0.006068 0.00189
    VHD_region      (time, region) float64 1.771e-16 6.183e-08 ... nan nan
    GF_sector_err   (sector) float64 0.000418 0.0001381 ... 0.0007099 9.023e-05
    vel_sector_err  (sector) float64 0.0004948 9.372e-05 ... 0.001923 7.714e-05
    VHD_sector_err  (time, sector) float64 6.693e-18 3.633e-18 ... nan nan
    GF_region_err   (region) float64 0.001882 0.0007622 ... 0.0008245 0.0006583
    vel_region_err  (region) float64 0.001005 0.001193 ... 0.002021 0.0006293
    VHD_region_err  (time, region) float64 2.656e-17 9.275e-09 ... nan nan
#+end_example


** Check BMB results

From citet:karlsson_2021 (Table 1)

| Sector |  GF |  vel | VHD | TOTAL |
|--------+-----+------+-----+-------|
| CE     | 0.5 |  1.2 | 0.7 |   2.4 |
| CW     | 0.7 |  3.6 | 0.5 |   4.8 |
| NE     | 1.3 |  1.8 | 0.2 |   3.2 |
| NO     | 0.4 |  0.7 | 0.2 |   1.3 |
| NW     | 0.6 |  2.9 | 0.5 |   4.0 |
| SE     | 0.7 |  1.7 | 0.9 |   3.3 |
| SW     | 1.2 |  1.1 | 1.0 |   3.3 |
|--------+-----+------+-----+-------|
| TOTAL  | 5.3 | 13.0 | 4.1 |  22.3 |


#+BEGIN_SRC jupyter-python
import xarray as x
bmb = xr.open_dataset('./tmp/bmb.nc')

bmb = bmb.sel({'time':slice('2010','2019')}).mean(dim='time')
bmb = bmb[['GF_region','vel_region','VHD_region']]
# print(bmb)

df = bmb.to_dataframe()\
        .sort_index()\
        .rename({'GF_region':'GF', 'vel_region':'vel', 'VHD_region':'VHD'}, axis='columns')

df['TOTAL'] = df.sum(axis='columns')
df.loc['TOTAL'] = df.sum(axis='rows')

df.round(3)*365
#+END_SRC

#+RESULTS:
| region |    GF |   vel |   VHD |  TOTAL |
|--------+-------+-------+-------+--------|
| CE     |  0.73 |  1.46 | 1.825 |   3.65 |
| CW     |  0.73 | 2.555 | 1.095 |  4.745 |
| NE     |  1.46 | 1.095 |  0.73 |  3.285 |
| NO     | 0.365 |  0.73 |  0.73 |  1.825 |
| NW     |  0.73 |  2.19 | 1.095 |  4.015 |
| SE     |  0.73 | 2.555 | 1.825 |   5.11 |
| SW     |  1.46 |  1.46 |  3.65 |  6.205 |
| TOTAL  |  5.84 | 11.68 | 10.95 | 28.835 |


* Reconstructed

Adjust SMB and D using 1986 through 2012 overlap

** Load

#+NAME: load_K2015_raw
#+BEGIN_SRC jupyter-python
import pandas as pd

fname = 'Greenland_mass_balance_totals_1840-2012_ver_20141130_with_uncert_via_Kjeldsen_et_al_2015.csv'
k2015 = pd.read_csv('/home/kdm/data/Kjeldsen_2015/' + fname, index_col=0, parse_dates=True)\
          .rename(columns={'discharge from 6 year lagged average runoff' : 'mmb',
                           'discharge 1sigma' : 'mmb_err'})

k2015.index.name = 'time'

k2015['smb'] = k2015['accumulation'] - k2015['runoff']
k2015['smb_err'] = (k2015['accumulation 1sigma']**2 + k2015['runoff 1sigma']**2)**0.5

k2015 = k2015.drop(columns=['accumulation', 'accumulation 1sigma',
                            'melt', 'melt 1sigma', 'retention',
                            'retention 1sigma',
                            # 'runoff', 'runoff 1sigma',
                            'TMB', 'TMB 1sigma'])
#+END_SRC

#+NAME: K2015_adj_prep
#+BEGIN_SRC jupyter-python
<<load_K2015_raw>>
<<load_smb>>
<<load_mmb>>

time = pd.date_range(start = "1986-01-01", end = str(mmb['time'].values[-1]), freq = "D")
mmb = mmb.reindex({'time':time}).bfill(dim='time')

smb = smb[['smb','smb_err']].resample({'time':'YS'}).sum().sel({'time':slice('1986','2012')}).to_dataframe()
mmb = mmb[['mmb','mmb_err']].resample({'time':'YS'}).sum().sel({'time':slice('1986','2012')}).to_dataframe()

k2015_overlap = k2015.loc['1986':'2012']
#+END_SRC

#+NAME: load_k2015
#+BEGIN_SRC jupyter-python
<<k2015_adj_prep>>

import scipy as sp
import scipy.stats as sps

slope, intercept, r_value, p_value, std_err = sps.linregress(smb['smb'].values,
                                                             k2015_overlap['smb'].values)
k2015 = k2015.rename(columns={'smb':'smb_orig'})
k2015['smb'] = (k2015['smb_orig'] - intercept)/slope
k2015['smb_err'] = (k2015['smb_err'] + smb['smb_err'].mean())

slope, intercept, r_value, p_value, std_err = sps.linregress(mmb['mmb'].values,
                                                             k2015_overlap['mmb'].values)
k2015 = k2015.rename(columns={'mmb':'mmb_orig'})
k2015['mmb'] = (k2015['mmb_orig'] - intercept)/slope
k2015['mmb_err'] = (k2015['mmb_err'] + mmb['mmb_err'].mean())

#+END_SRC

#+RESULTS: load_k2015

** Plot overlap

#+BEGIN_SRC jupyter-python
<<load_K2015>>
k2015 = k2015.loc['1986':'2012']

import scipy as sp
import scipy.stats as sps
from adjust_spines import adjust_spines as adj
import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

fig = plt.figure(1, figsize=(3.26,7.1)) # w,h
fig.clf()
fig.set_tight_layout(True)
axSMB = fig.add_subplot(311)
axD = fig.add_subplot(312)
axMB = fig.add_subplot(313)

years = smb.index.year
y2str = [_[2:4] for _ in years.astype(str)]

color = years - min(years); color = color / max(color)
cmap = cm.viridis(color)
cmap = mpl.colors.ListedColormap(cmap[:-3,:-1]) # https://stackoverflow.com/questions/51034408/

kw_errbar = {'fmt':',', 'color':'k', 'alpha':0.33, 'linewidth':0.5}
kw_nums = {'fontsize':9,
           'fontweight':'bold',
           'horizontalalignment':'center','verticalalignment':'center'}
kw_adj = {'linewidth':1, 'marker':(4,0,90), 'markersize':7, 'markevery':[-1,1], 'mfc':'none', 'clip_on':False}

###
### SMB
###
e = axSMB.errorbar(smb['smb'].values, k2015['smb_orig'].values,
                   xerr=smb['smb_err'], yerr=k2015['smb_err'], **kw_errbar)
for i,s in enumerate(y2str):
  axSMB.text(smb['smb'].values[i], k2015['smb_orig'].values[i], s, c=cmap(color)[i], **kw_nums)

def print_stats_in_graph(x0,y0,x1,y1, ax):
  slope, intercept, r_value, p_value, std_err = sps.linregress(x0, y0)
  bias = np.mean(x0 - y0)
  RMSE = np.sqrt(np.mean((x0 - y0)**2))

  slope_adj, intercept_adj, r_value_adj, p_value_adj, std_err_adj = sps.linregress(x1, y1) 
  bias_adj = np.mean(x1 - y1)
  RMSE_adj = np.sqrt(np.mean((x1 - y1)**2))

  kw_text = {'horizontalalignment':'right', 'fontsize':9}
  s = "r$^2$: %.2f → %.2f\nbias: %d → %d\nRMSE: %d → %d\nslope: %.1f → %.1f\nintercept: %d → %d"
  s = "%.2f → %.2f r$^2$\n%d → %d bias\n%d → %d RMSE\n%.1f → %.1f slope\n%d → %d intercept"
  ax.text(1.0, -0.03, s % (round(r_value**2,2), round(r_value_adj**2,2),
                              round(bias), round(bias_adj),
                              round(RMSE), round(RMSE_adj),
                              round(slope,1), round(slope_adj,1),
                              round(intercept), round(intercept_adj)),
          transform=ax.transAxes, **kw_text)

print_stats_in_graph(smb['smb'].values, k2015['smb_orig'].values,
                     smb['smb'].values, k2015['smb'].values, axSMB)
  
for i,s in enumerate(y2str):
  axSMB.plot([smb['smb'].values[i], smb['smb'].values[i]],
             [k2015['smb'].values[i], k2015['smb'].values[i]],
             c=cmap(color[i]), **kw_adj)





###
### MMB
###
e = axD.errorbar(mmb['mmb'].values, k2015['mmb_orig'].values,
                   xerr=mmb['mmb_err'], yerr=k2015['mmb_err'], **kw_errbar)
for i,s in enumerate(y2str):
  axD.text(mmb['mmb'].values[i], k2015['mmb_orig'].values[i], s, c=cmap(color)[i], **kw_nums)
print_stats_in_graph(mmb['mmb'].values, k2015['mmb_orig'].values,
                     mmb['mmb'].values, k2015['mmb'].values, axD)
  
for i,s in enumerate(y2str):
  axD.plot([mmb['mmb'].values[i], mmb['mmb'].values[i]],
           [k2015['mmb'].values[i], k2015['mmb'].values[i]],
           c=cmap(color[i]), **kw_adj)



###
### MB
###
mb = smb['smb'] - mmb['mmb']
k2015['mb_orig'] = k2015['smb_orig'] - k2015['mmb_orig']
k2015['mb'] = k2015['smb'] - k2015['mmb']
k2015['mb_err'] = (k2015['smb_err']**2 + k2015['mmb_err']**2)**0.5
e = axMB.errorbar(mb, k2015['mb_orig'], xerr=mmb['mmb_err'], yerr=k2015['mb_err'], **kw_errbar)

for i,s in enumerate(y2str):
  axMB.text(mb.values[i], k2015['mb_orig'].values[i], s, c=cmap(color)[i], **kw_nums)
print_stats_in_graph(mb.values, k2015['mb_orig'].values,
                     mb.values, k2015['mb'].values, axMB)
  
for i,s in enumerate(y2str):
  axMB.plot([mb.values[i], mb.values[i]],
            [k2015['mb'].values[i], k2015['mb'].values[i]],
            c=cmap(color[i]), **kw_adj)
  



  
axSMB.set_xticks([0,750])
axSMB.set_ylabel('SMB', labelpad=-20)

axD.set_xticks([375,600])
axD.set_ylabel('MMB', labelpad=-20)

# axMB.text(0, 0.9, '+BMB', transform=axMB.transAxes)
axMB.set_xticks([-450, 0, 300])
axMB.set_ylabel('MB$^{*}$', labelpad=-25)
axMB.set_xlabel('This Study', labelpad=-10)

for ax in [axSMB,axD,axMB]:    
  ax.set_yticks(ax.get_xticks())
  ax.set_xlim(ax.get_xticks()[[0,-1]])
  ax.set_ylim(ax.get_xlim())
  ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
  adj(ax, ['left','bottom'])

axMB.set_yticklabels([str(ax.get_yticks()[0]), '', str(ax.get_yticks()[-1])])
axMB.set_xticklabels([str(ax.get_xticks()[0]), '', str(ax.get_xticks()[-1])])


# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

  
s = axSMB.scatter(smb['smb'], k2015['smb'], alpha=0, c=color, cmap=cmap)
# axCB = add_sub_axes(axMB, [1.1, 0.0, 0.075, 1])
axCB = add_sub_axes(axMB, [0.0, -0.50, 1, 0.075])
cb = plt.colorbar(s, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
cb.ax.xaxis.set_ticks_position('top')
cb.ax.xaxis.set_label_position('top')
cb.set_label('Time [year]', labelpad=-10)
axCB.set_xticklabels(k2015.index.year[[0,-1]].values)

plt.savefig('fig/K2015_adjusted.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: <ipython-input-125-0393593e2bbb>:329: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/K2015_adjusted.png', transparent=False, bbox_inches='tight', dpi=300)


* TMB

Generate a TMB netcdf file from SMB, MMB, and BMB

** Load SMB

#+NAME: load_smb
#+BEGIN_SRC jupyter-python
<<py_import>>

smb = xr.open_dataset("./tmp/smb.nc")
smb['region'] = smb['region'].astype(str)
#+END_SRC

#+RESULTS: load_smb

** Load MMB (Mankoff 2020)

#+NAME: load_mmb
#+BEGIN_SRC jupyter-python
import xarray as xr

ds = xr.open_dataset("/home/kdm/data/Mankoff_2020/ice/latest/gate.nc")
# ds = xr.open_dataset("/home/kdm/projects/ice_discharge/out/gate.nc")

# rstr = {'NW':11, 'NO':12, 'NE':1, 'CE':3, 'CW':9, 'SE':5, 'SW':7}
# rnum = [rstr[r] for r in ds['region'].values]
# ds['region'] = (('gate'), rnum)

ds_sector = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat","sector","region","coverage"])\
          .groupby("Zwally_2012")\
          .sum()\
          .rename({"Zwally_2012":"sector",
                   "discharge":"mmb_sector",
                   "err":"err_sector"})

ds_region = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat","sector","Zwally_2012","coverage"])\
          .groupby("region")\
          .sum()\
          .rename({"discharge":"mmb_region",
                   "err":"err_region"})


# ds_basin = ds.drop_vars(["mean_x","mean_y","mean_lon","mean_lat","Zwally_2012","region","coverage"])\
#           .groupby("sector")\
#           .sum()\
#           .rename({"sector":"basin",
#                    "discharge":"mmb_basin",
#                    "err":"err_basin"})

mmb = ds_sector
mmb = mmb.merge(ds_region)
# mmb = mmb.merge(ds_basin)

mmb['mmb'] = (('time'), mmb['mmb_sector'].sum(dim='sector'))
mmb['mmb_err'] = (('time'), mmb['err_sector'].sum(dim='sector'))

# convert from Gt/year @ misc time-steps -> Gt/day @ daily timestep
msave = mmb.copy(deep=True)
mmb = (mmb / 365).resample({"time":"1D"})\
                 .mean()\
                 .interpolate_na(dim="time")

# I want monotonic cubic interpolation for discharge
mmb['mmb'] = (('time'), (msave['mmb']/365).resample({'time':'1D'}).mean().to_dataframe().interpolate(method='pchip').values.flatten())
mmb['region'] = mmb['region'].astype(str)
#+END_SRC

#+RESULTS: load_mmb
*** Forecast MMB for sectors and regions

#+NAME: forecast_mmb
#+BEGIN_SRC jupyter-python
<<load_mmb>>

import numpy as np
import pandas as pd
import xarray as xr
import scipy as sp
import scipy.stats
import scipy.signal

time_first_obs = mmb['time'][0].values
time_last_obs = mmb['time'][-1].values
# time_last_obs_str = np.datetime_as_string(time_last_obs).split('T')[0]
time_end = datetime.datetime.utcnow().date() + datetime.timedelta(days = 7)
time_start = '1986-01-01'
time_all = pd.date_range(start=time_start, end=time_end, freq='D')

df = mmb[['mmb','mmb_err']].to_dataframe()

# select last 3 years for linear trend
df3 = df[-365*3:]

# short term variations
df3['seasonal'] = sp.signal.detrend(df3['mmb'].values)
# trend is computed by subracting the short-term seasonal variations and computing the mean daily change of the long-term signal
trend = (df3['mmb'] - df3['seasonal']).diff().mean()

forecast = pd.DataFrame(index = pd.date_range(start=time_last_obs, end=time_end, freq='D'))
forecast['flat'] = df3.iloc[-1]['mmb']
forecast['trend'] = forecast['flat'] + trend * np.arange(forecast.index.size)

df3['idx'] = np.arange(df3.index.size) % 365 + 1
forecast['seasonal'] = df3['seasonal'].groupby(df3['idx']).mean().values[0:forecast.index.size]
forecast['seasonal'] = forecast['seasonal'] - forecast['seasonal'][0]
forecast['best'] = forecast['trend'] + forecast['seasonal']

# Uncertainty
# 1. Assume that discharge n days in the future may increase by the largest change over n days historically
# 1. Add that to the baseline uncertainty.
forecast['err'] = np.nan
for i in np.arange(forecast.index.size):
    forecast['err'].iloc[i] = df3['seasonal'].rolling(window=i+2).apply(lambda x: x.max()-x.min()).bfill().iloc[i]
    # forecast['err'].iloc[i] = df3['seasonal'].rolling(window=i+2).mean().max()
# forecast['mmb_err'] = df3.iloc[-1]['mmb_err'] + forecast['err'].cumsum()
forecast['mmb_err'] = df3.iloc[-1]['mmb_err'] + forecast['err']

df = df.reindex(time_all).bfill()
df.loc[time_last_obs:, 'mmb'] = forecast[time_last_obs:]['best']
df.loc[time_last_obs:, 'mmb_err'] = forecast[time_last_obs:]['mmb_err']
#+END_SRC

*** INPROGRESS Forecast MMB for GIS: xarray

+ MMB:
  + Estimate +7d MMB from long term trend + seasonal trend
  + pchip interpolate from last observed to +7d value
  + This means no discontinuity from last observed to last obs + 1d
+ MMB err:
  + Take last observed to now+7d window (for ex 25 days)
  + Take that window last 3 years (ex: 75 days)
  + Diff -> abs -> rank
  + Obs+1 err is largest of ranked from past window (75 days)
  + Obs+2 err is largest + 2nd largest of ranked
  + +7d err is (in this case) cumsum of 25 largest.

#+NAME: forecast_mmb
#+BEGIN_SRC jupyter-python
<<load_mmb>>

import numpy as np
import pandas as pd
import xarray as xr
import scipy as sp
import scipy.stats
import scipy.signal

time_start = '1986-01-01'
# time_first_obs = mmb['time'][0].values
time_last_obs = mmb['time'][-1].values
time_forecast_start = pd.Timestamp(time_last_obs) + datetime.timedelta(days=1)
time_forecast_end = datetime.datetime.utcnow().date() + datetime.timedelta(days = 7)

time_index_all = pd.date_range(start=time_start, end=time_forecast_end, freq='D')
time_index_last3y = pd.date_range(start = pd.Timestamp(time_last_obs) - datetime.timedelta(days=365*3), end=time_last_obs, freq='D')
time_index_forecast = pd.date_range(start = time_forecast_start, end=time_forecast_end, freq='D')
time_index_hindcast = np.concatenate([time_index_forecast - datetime.timedelta(days=365*3),
                                      time_index_forecast - datetime.timedelta(days=365*2),
                                      time_index_forecast - datetime.timedelta(days=365*1)])

mmb_all = mmb.reindex({'time':time_index_all})

# https://gist.github.com/rabernat/1ea82bb067c3273a6166d1b1f77d490f
def detrend_dim(da, dim, deg=1):
    # detrend along a single dimension
    p = da.polyfit(dim=dim, deg=deg)
    fit = xr.polyval(da[dim], p.polyfit_coefficients)
    return fit

for v in ['mmb','mmb_region','mmb_sector']:
    # trend is linear long term trend of last 3 years of observations.
    last3_plus_forecast = mmb_all[v].sel({'time':slice(time_index_last3y[0],time_forecast_end)})
    trend = detrend_dim(last3_plus_forecast, 'time')

    # season is the detrended values of forecasted calendar dates for the past 3 years
    season = (last3_plus_forecast - trend).reindex({'time':time_index_hindcast})

    # forecast is the long term trend adjusted to start from the last observed,
    # then cropped to the last timestamp, then seasonality added, then gap-filled
    # with bilinear interpolation for smoothness

    forecast = trend.reindex({'time':time_index_forecast})
    forecast = forecast - forecast.isel(time=0) + mmb[v].isel({'time':-1}) + forecast.diff(dim='time').isel({'time':-1})
    forecast = forecast + season.mean(dim='time').values
    forecast = forecast.where(forecast['time'] == forecast['time'][-1]).dropna(dim='time')
    mmb_all[v] = xr.concat([mmb[v],forecast], dim='time').interp(time=time_index_all)
    
for v in ['mmb_err','err_region','err_sector']:
    last3_hindcast = mmb_all[v].sel({'time':time_index_hindcast})
    diff = np.abs(last3_hindcast.reindex({'time':time_index_last3y}).diff(dim='time').dropna(dim='time'))

    if size(diff.dims) == 1:
        err = diff.sortby(diff, ascending=False).values[0:time_index_forecast.size] # .cumsum()
        e = xr.DataArray(data=err, dims=['time'], coords={'time':time_index_forecast})
    else:
        rank = diff.to_dataframe().unstack()[v]
        err = pd.DataFrame(columns=rank.columns, index=time_index_forecast)
        err.index.name = 'time'
        for c in rank.columns: err[c] = rank[c].sort_values(ascending=False).values[0:time_index_forecast.size]
        e = xr.DataArray(err).squeeze().reindex({'time':time_index_all})
        
    mmb_all[v] = mmb_all[v].ffill(dim='time') + e.cumsum().reindex({'time':time_index_all}).fillna(0)

mmb_all = mmb_all.bfill(dim='time')

from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)
# matplotlib.pyplot.xkcd()

fig = plt.figure(1, figsize=(5,4)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(111)

# mmb_all['mmb'].plot(ax=ax)
# ax.fill_between(mmb_all['time'],
#                 mmb_all['mmb']-mmb_all['mmb_err'],
#                 mmb_all['mmb']+mmb_all['mmb_err'],
#                 color='gray', alpha=0.25)

r = 'NO'
mmb_all['mmb_region'].sel({'region':r}).plot(ax=ax)
ax.fill_between(mmb_all['time'],
                mmb_all['mmb_region'].sel({'region':r}) - mmb_all['err_region'].sel({'region':r}),
                mmb_all['mmb_region'].sel({'region':r}) + mmb_all['err_region'].sel({'region':r}),
                color='gray', alpha=0.25)

#+END_SRC

#+RESULTS: forecast_mmb
: <matplotlib.collections.PolyCollection at 0x7f8acd4d9d90>





*** DONE Forecast MMB for GIS: Pandas

#+NAME: forecast_mmb
#+BEGIN_SRC jupyter-python
<<load_mmb>>

import numpy as np
import pandas as pd
import xarray as xr
import scipy as sp
import scipy.stats
import scipy.signal

time_first_obs = mmb['time'][0].values
time_last_obs = mmb['time'][-1].values
# time_last_obs_str = np.datetime_as_string(time_last_obs).split('T')[0]
time_end = datetime.datetime.utcnow().date() + datetime.timedelta(days = 7)
time_start = '1986-01-01'
time_all = pd.date_range(start=time_start, end=time_end, freq='D')

df = mmb[['mmb','mmb_err']].to_dataframe()

# select last 3 years for linear trend
df3 = df[-365*3:]

# short term variations
df3['seasonal'] = sp.signal.detrend(df3['mmb'].values)
# trend is computed by subracting the short-term seasonal variations and computing the mean daily change of the long-term signal
trend = (df3['mmb'] - df3['seasonal']).diff().mean()

forecast = pd.DataFrame(index = pd.date_range(start=time_last_obs, end=time_end, freq='D'))
forecast['flat'] = df3.iloc[-1]['mmb']
forecast['trend'] = forecast['flat'] + trend * np.arange(forecast.index.size)

df3['idx'] = np.arange(df3.index.size) % 365 + 1
forecast['seasonal'] = df3['seasonal'].groupby(df3['idx']).mean().values[0:forecast.index.size]
forecast['seasonal'] = forecast['seasonal'] - forecast['seasonal'][0]
forecast['best'] = forecast['trend'] + forecast['seasonal']

# Uncertainty
# 1. Assume that discharge n days in the future may increase by the largest change over n days historically
# 1. Add that to the baseline uncertainty.
forecast['err'] = np.nan
for i in np.arange(forecast.index.size):
    forecast['err'].iloc[i] = df3['seasonal'].rolling(window=i+2).apply(lambda x: x.max()-x.min()).bfill().iloc[i]
    # forecast['err'].iloc[i] = df3['seasonal'].rolling(window=i+2).mean().max()
# forecast['mmb_err'] = df3.iloc[-1]['mmb_err'] + forecast['err'].cumsum()
forecast['mmb_err'] = df3.iloc[-1]['mmb_err'] + forecast['err']

df = df.reindex(time_all).bfill()
df.loc[time_last_obs:, 'mmb'] = forecast[time_last_obs:]['best']
df.loc[time_last_obs:, 'mmb_err'] = forecast[time_last_obs:]['mmb_err']
#+END_SRC

** Load BMB (Karlsson 2021)

#+NAME: load_bmb
#+BEGIN_SRC jupyter-python
<<py_import>>

bmb = xr.open_dataset('./tmp/bmb.nc')
bmb['region'] = bmb['region'].astype(str)

bmb['bmb'] = (('time'), (bmb['GF_sector'] \
                         + bmb['vel_sector'] \
                         + bmb['VHD_sector']).sum(dim='sector'))

bmb['bmb_err'] = (('time'), ((bmb['GF_sector_err']**2 \
                              + bmb['vel_sector_err']**2 \
                              + bmb['VHD_sector_err']**2)**0.5).sum(dim='sector'))

bmb['bmb_sector'] = (('time','sector'), bmb['VHD_sector'] + bmb['GF_sector'] + bmb['vel_sector'])
bmb['bmb_region'] = (('time','region'), bmb['VHD_region'] + bmb['GF_region'] + bmb['vel_region'])
# bmb['bmb_basin'] = bmb['VHD_basin'] + bmb['GF_basin'] + bmb['vel_basin']

bmb['bmb_sector_err'] = (('time','sector'), \
                         ((bmb['GF_sector_err'].expand_dims({'time':bmb['time'].size}))**2 \
                          + (bmb['vel_sector_err'].expand_dims({'time':bmb['time'].size}))**2 \
                          + bmb['VHD_sector_err']**2\
                          )**0.5\
                         )

bmb['bmb_region_err'] = (('time','region'), \
                         ((bmb['GF_region_err'].expand_dims({'time':bmb['time'].size}))**2 \
                          + (bmb['vel_region_err'].expand_dims({'time':bmb['time'].size}))**2 \
                          + bmb['VHD_region_err']**2\
                          )**0.5\
                         )

# print(bmb)
bmb[['VHD_sector','GF_sector','vel_sector','bmb']].sum(dim='sector').to_dataframe().resample('MS').sum().head(12)
#+END_SRC

#+RESULTS: load_bmb
| time                | VHD_sector | GF_sector | vel_sector |     bmb |
|---------------------+------------+-----------+------------+---------|
| 1986-01-01 00:00:00 | 0.00161707 |  0.506671 |    1.00549 | 1.51377 |
| 1986-02-01 00:00:00 | 0.00112097 |  0.457638 |   0.908181 | 1.36694 |
| 1986-03-01 00:00:00 | 0.00121233 |  0.506671 |    1.00549 | 1.51337 |
| 1986-04-01 00:00:00 | 0.00110149 |  0.490327 |   0.973051 | 1.46448 |
| 1986-05-01 00:00:00 | 0.00644159 |  0.506671 |    1.00549 |  1.5186 |
| 1986-06-01 00:00:00 |   0.511597 |  0.490327 |   0.973051 | 1.97497 |
| 1986-07-01 00:00:00 |    2.28902 |  0.506671 |    1.00549 | 3.80118 |
| 1986-08-01 00:00:00 |    1.87941 |  0.506671 |    1.00549 | 3.39157 |
| 1986-09-01 00:00:00 |   0.528512 |  0.490327 |   0.973051 | 1.99189 |
| 1986-10-01 00:00:00 | 0.00678598 |  0.506671 |    1.00549 | 1.51894 |
| 1986-11-01 00:00:00 | 0.00239901 |  0.490327 |   0.973051 | 1.46578 |
| 1986-12-01 00:00:00 | 0.00201692 |  0.506671 |    1.00549 | 1.51417 |

+ Monthly changes in VHD seem reasonable
+ Changes in GF and vel (constants) are due to days-in-month.

** Load and adjust Reconstructed K2015
:PROPERTIES:
:ID:       20210406T102219.348249
:END:

#+NAME: load_and_adjust_K2015
#+BEGIN_SRC jupyter-python
<<load_K2015>>

## Add BMB
<<load_bmb>>

# constant or time invariant:
bmb_C = bmb[['GF_sector','vel_sector','GF_sector_err','vel_sector_err']]\
    .to_dataframe()\
    .sum(axis='index')

# VHD is time variable.
v = bmb[['VHD_sector','VHD_sector_err']]\
    .resample({'time':'YS'})\
    .sum()\
    .sum(dim='sector')\
    .to_dataframe()

vr = v.merge(k2015['runoff'], left_index=True, right_index=True)
slope, intercept, r_value, p_value, std_err = sps.linregress(vr['runoff'].values,
                                                             vr['VHD_sector'].values)

print('slope: ', slope)
print('intercept: ', intercept)
print('r^2: ', r_value**2)
print('p: ', p_value)
print('std_err :', std_err)

k2015['bmb_GF'] = bmb_C['GF_sector']*365
k2015['bmb_GF_err'] = bmb_C['GF_sector_err']*365
k2015['bmb_vel'] = bmb_C['vel_sector']*365
k2015['bmb_vel_err'] = bmb_C['vel_sector_err']*365

k2015['bmb_VHD'] = (k2015['runoff']) * slope
k2015['bmb_VHD_err'] = (k2015['runoff 1sigma']) * slope

k2015[[_ for _ in k2015.columns if '_' in _ ]].head()

k2015['bmb'] = k2015[['bmb_GF','bmb_vel','bmb_VHD']].sum(axis='columns')
k2015['bmb_err'] = (k2015['bmb_GF']**2 \
                  + k2015['bmb_vel']**2 \
                  + k2015['bmb_VHD']**2)**0.5

k2015['mb'] = k2015['smb'] - k2015['mmb'] - k2015['bmb']
#+END_SRC

#+RESULTS: load_and_adjust_K2015
: slope:  0.029557388940898793
: intercept:  -3.4633056369036304
: r^2:  0.7469795590445695
: p:  6.253448888175992e-09
: std_err : 0.003440483722315962

  
** Create TMB output
:PROPERTIES:
:header-args:jupyter-python+: :tangle build_TMB_nc.py
:ID:       20210730T084938.500158
:END:

#+BEGIN_SRC jupyter-python
<<load_and_adjust_K2015>>

k2015 = k2015.loc['1840':'1986']
# k2015 = k2015.loc['1840':'1986'].resample('1D').ffill().iloc[:-1] # cut off 1986-01-01
k2015 = k2015.loc['1840':'1986'].iloc[:-1] # cut off 1986-01-01



<<load_smb>>
time = np.append(k2015.index, smb['time'].values)
smb = smb.reindex({'time':time})
smb['smb'] = (('time'),
              np.append(k2015['smb'].values/365, smb['smb'].sel({'time':slice('1986','2200')}).values))
smb['smb_err'] = (('time'),
                  np.append(k2015['smb_err'].values/365, smb['smb_err'].sel({'time':slice('1986','2200')}).values))



<<load_mmb>>
dt_last_obs = mmb['time'].values[-1]
err = []
for dt in pd.date_range(start=dt_last_obs, end=time[-1]):
    std = mmb['mmb'].sel(time=(mmb['time.month'] == dt.month) & (mmb['time.day'] == dt.day)).std().values
    # print(std)
    err_today = max(max(err), 2*std) if dt != dt_last_obs else 2*std
    err.append(err_today)

mmb = mmb.reindex({'time':time})
mmb['mmb'] = (('time'),
              np.append(k2015['mmb'].values/365, mmb['mmb'].sel({'time':slice('1986','2200')}).values))
mmb['mmb_err'] = (('time'),
                  np.hstack((k2015['mmb_err'].values/365,
                             mmb['mmb_err'].sel({'time':slice('1986',dt_last_obs)}).values,
                             err[1:])).ravel())


<<load_bmb>>
bmb = bmb.reindex({'time':time}).fillna(0)
bmb['bmb'] = (('time'),
              np.append(k2015['bmb'].values/365, bmb['bmb'].sel({'time':slice('1986','2200')}).values))
bmb['bmb_err'] = (('time'),
                  np.append(k2015['bmb_err'].values/365, bmb['bmb_err'].sel({'time':slice('1986','2200')}).values))


# add the sectors from SMB to MMB (14 and 21(?) don't have any MMB)
mmb = mmb.reindex({'sector': smb['sector']})# .fillna(0)
# mmb = mmb.reindex({'basin': smb['basin']})# .fillna(0)
mmb = mmb.reindex({'region': smb['region']})# .fillna(0)
mmb['region'] = mmb['region'].astype(str)
mmb = mmb.ffill(dim='time')
mmb = mmb.bfill(dim='time')
#+END_SRC

#+RESULTS:
: slope:  0.029557388940898793
: intercept:  -3.4633056369036304
: r^2:  0.7469795590445695
: p:  6.253448888175992e-09
: std_err : 0.003440483722315962

#+BEGIN_SRC jupyter-python
import subprocess
import os

for roi in ['sector', 'region']: # TODO: 'basin'
    mb = xr.Dataset()
    
    mb["time"] = (("time"), time)
    mb["time"].attrs["cf_role"] = "timeseries_id"
    mb["time"].attrs["standard_name"] = "time"
    mb["time"].attrs["axis"] = "T"

    mb[roi] = ((roi), mmb[roi])
    if roi == 'sector':
        mb[roi].attrs["long_name"] = "Zwally 2012 sectors"
    elif roi == 'region':
        mb[roi].attrs["long_name"] = "Mouginot 2019 regions"
    elif roi == 'basin':
        mb[roi].attrs["long_name"] = "Mouginot 2019 basins"
       

    mb_hist = (k2015['smb'] - k2015['mmb'] - k2015['bmb'])/365
    mb_recent = (smb['smb'] - mmb['mmb'] - bmb['bmb']).to_dataframe('mb').loc['1986':]['mb']
    mb['mb'] = (('time'), mb_hist.append(mb_recent))
    mb['mb_err'] = (('time'), (smb['smb_err']**2 + mmb['mmb_err']**2 + bmb['bmb_err']**2)**0.5)

    v = 'mb'
    mb[v].attrs["long_name"] = "Mass balance"
    mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
    mb[v].attrs["units"] = "Gt d-1"
    mb[v].attrs["coordinates"] = 'time'
    
    mb['mb_ROI'] = (('time',roi), smb['smb_'+roi] - mmb['mmb_'+roi] - bmb['bmb_'+roi])
    mb['mb_ROI_err'] = (('time',roi), (smb['smb_'+roi+'_err']**2 + mmb['err_'+roi]**2 + bmb['bmb_'+roi+'_err']**2)**0.5)
    # mb['mb_ROI'].attrs = mb['mb'].attrs
    # mb['mb_ROI'].attrs["coordinates"] = 'time ROI'

    # if roi == 'region':
    #     from IPython import embed; embed()
    
    mb['smb'] = (('time'), smb['smb'])
    mb['smb_err'] = (('time'), smb['smb_err'])
    mb['smb_ROI'] = (('time',roi), smb['smb_'+roi])
    mb['smb_ROI_err'] = (('time',roi), smb['smb_'+roi+'_err'])
    # mb['smb_HIRHAM'] = (('time',roi), smb['HIRHAM_'+roi]-mmb['mmb_'+roi] )
    # mb['smb_MAR'] = (('time',roi), smb['HIRHAM_'+roi]-mmb['mmb_'+roi])
    # mb['smb_RACMO'] = (('time',roi), smb['HIRHAM_'+roi]-mmb['mmb_'+roi])
    for v in ['smb', 'smb_ROI', 'smb_err', 'smb_ROI_err']:
        ln = 'Surface mass balance'
        if 'err' in v: ln = ln + ' uncertainty'
        mb[v].attrs['long_name'] = ln
        mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
        mb[v].attrs["units"] = "Gt d-1"
        mb[v].attrs["coordinates"] = 'time ' + roi

       
    mb['mmb'] = (('time'), mmb['mmb'])
    mb['mmb_err'] = (('time'), mmb['mmb_err'])
    mb['mmb_ROI'] = (('time',roi), mmb['mmb_'+roi])
    mb['mmb_ROI_err'] = (('time',roi), mmb['err_'+roi])
    for v in ['mmb','mmb_ROI', 'mmb_err', 'mmb_ROI_err']:
        ln = 'Marine mass balance'
        if 'err' in v: ln = ln + ' uncertainty'
        mb[v].attrs['long_name'] = ln
        mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
        mb[v].attrs["units"] = "Gt d-1"
    mb['mmb'].attrs["coordinates"] = 'time'
    mb['mmb_ROI'].attrs["coordinates"] = 'time ' + roi

    
    mb['bmb'] = (('time'), bmb['bmb'])
    mb['bmb_err'] = (('time'), bmb['bmb_err'])
    mb['bmb_ROI'] = (('time',roi), bmb['bmb_'+roi])
    mb['bmb_ROI_err'] = (('time',roi), bmb['bmb_'+roi+'_err'])
    for v in ['bmb','bmb_ROI']:
        ln = 'Basal mass balance'
        if 'err' in v: ln = ln + ' uncertainty'
        mb[v].attrs['long_name'] = ln
        mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
        mb[v].attrs["units"] = "Gt d-1"
    mb['bmb'].attrs["coordinates"] = 'time'
    mb['bmb_ROI'].attrs["coordinates"] = 'time ' + roi
    
        
    # mb['bmb'] = (('time'), bmb['bmb'])
    # mb['bmb_ROI'] = (('time',roi), bmb['bmb_'+roi])
    # mb['bmb_GF_ROI'] = (('time',roi), bmb['bmb_GF_'+roi])
    # mb['bmb_vel_ROI'] = (('time',roi), bmb['bmb_vel_'+roi])
    # mb['bmb_VHD_ROI'] = (('time',roi), bmb['bmb_VHD_'+roi])
    # for v in ['bmb','bmb_ROI']:
    #     mb[v].attrs['long_name'] = 'Basal mass balance'
    #     mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
    #     mb[v].attrs["units"] = "Gt d-1"
    # mb['bmb'].attrs["coordinates"] = 'time'
    # mb['bmb_ROI'].attrs["coordinates"] = 'time ' + roi

    for RCM in ['HIRHAM','MAR','RACMO']:
        mb['mb_'+RCM] = (('time',roi), smb[RCM+'_'+roi]-mmb['mmb_'+roi]-bmb['bmb_'+roi] )
        v = 'mb_'+RCM
        mb[v].attrs['long_name'] = 'Mass balance from ' + v.split('_')[1]
        mb[v].attrs["standard_name"] = "land_ice_mass_tranport"
        mb[v].attrs["units"] = "Gt d-1"
        mb[v].attrs["coordinates"] = 'time ' + roi
        
    # if roi == 'region':
    #     from IPython import embed; embed()
        # smb['smb_'+roi].sel({'region':'CE'}), '\n\n', mmb['mmb_'+roi].sel({'region':'CE'}), '\n\n', (smb['smb_'+roi] - mmb['mmb_'+roi]).sel({'region':'CE'}), '\n\n', mb.sel({'region':'CE'})
        # smb['smb_'+roi].isel({'time':0}), '\n\n', mmb['mmb_'+roi].isel({'time':0}), '\n\n', (smb['smb_'+roi] - mmb['mmb_'+roi]).isel({'time':0}), '\n\n', mb.isel({'time':0})

    
    mb.attrs['featureType'] = 'timeSeries'
    mb.attrs['title'] = 'Greenland ice sheet mass balance from 1840 through next week'
    mb.attrs['summary'] = mb.attrs['title']
    mb.attrs['keywords'] = 'Greenland; Mass; Mass balance'
    # mb.attrs['Conventions'] = 'CF-1.8'
    mb.attrs['source'] = 'git commit: ' + subprocess.check_output(['git', 'describe', '--always']).strip().decode('UTF-8')
    # mb.attrs['comment'] = 'TODO'
    # mb.attrs['acknowledgment'] = 'TODO'
    # mb.attrs['license'] = 'TODO'
    # mb.attrs['date_created'] = datetime.datetime.now().strftime('%Y-%m-%d')
    mb.attrs['creator_name'] = 'Ken Mankoff'
    mb.attrs['creator_email'] = 'kdm@geus.dk'
    mb.attrs['creator_url'] = 'http://kenmankoff.com'
    mb.attrs['institution'] = 'GEUS'
    # mb.attrs['time_coverage_start'] = 'TODO'
    # mb.attrs['time_coverage_end'] = 'TODO'
    # mb.attrs['time_coverage_resolution'] = 'TODO'
    mb.attrs['references'] = '10.22008/promice/mass_balance'
    mb.attrs['product_version'] = 1.0

    comp = dict(zlib=True, complevel=2)
    encoding = {var: comp for var in mb.data_vars} # all

    fn = './TMB/mb_'+roi+'.nc'
    if os.path.exists(fn): os.remove(fn)
    mb.to_netcdf(fn, mode='w', encoding=encoding)


# maybe also some CSV output
mb[['mb','mb_err','smb','smb_err','mmb','mmb_err','bmb','bmb_err']].to_dataframe().to_csv('./TMB/mb_smb_mmb_bmb.csv')
#+END_SRC

#+RESULTS:

* Validation prep
** IO (Mouginot)

#+name: load_mouginot
#+BEGIN_SRC jupyter-python

ds = xr.Dataset()

df = pd.read_excel('/home/kdm/data/Mouginot_2019/pnas.1904242116.sd02.xlsx', sheet_name=1)

## Discharge
c0 = 15 # Column containing 1972
c1 = 61 # Last column
r0 = 8 # sub-table start [LibreOffice is 1-based, Python is 0-based]
r1 = 15 # sub-table stop

ds['time'] = (('time'), pd.to_datetime(df.iloc[r0-1][df.columns[c0:(c1 + 1)]].astype(int).values, format="%Y"))
ds['region'] = (('region'), df.iloc[r0:r1][df.columns[1]])
ds['D'] = (('time','region'), df.iloc[r0:r1][df.columns[c0:(c1 + 1)]].values.T.astype(float))

c2  = 82; c3 = 128
ds['D_err'] = (('time','region'), df.iloc[r0:r1][df.columns[c2:(c3 + 1)]].values.T.astype(float))

r0 = 20; r1=27
ds['SMB'] = (('time','region'), df.iloc[r0:r1][df.columns[c0:(c1 + 1)]].values.T.astype(float))
ds['SMB_err'] = (('time','region'), df.iloc[r0:r1][df.columns[c2:(c3 + 1)]].values.T.astype(float))

r0 = 30; r1=37
ds['MB'] = (('time','region'), df.iloc[r0:r1][df.columns[c0:(c1 + 1)]].values.T.astype(float))
ds['MB_err'] = (('time','region'), df.iloc[r0:r1][df.columns[c2:(c3 + 1)]].values.T.astype(float))

mouginot = ds
# print(mouginot)
#+END_SRC

#+RESULTS: load_mouginot

** VC

Not much to do because VC from Simonsen (2021) provided as MB by ROI

#+BEGIN_SRC bash :results verbatim
ncdump -chs ${DATADIR}/Simonsen_2021/ds1.nc
#+END_SRC

#+RESULTS:
#+begin_example
netcdf ds1 {
dimensions:
	t = 28 ;
variables:
	float Start_time(t) ;
		Start_time:long_name = "Time of first observation" ;
		Start_time:standard_name = "Start_time" ;
		Start_time:units = "decimal year" ;
		Start_time:_Storage = "contiguous" ;
		Start_time:_Endianness = "little" ;
	float time(t) ;
		time:long_name = "Midpoint of SEC obs." ;
		time:standard_name = "time" ;
		time:units = "decimal year" ;
		time:_Storage = "contiguous" ;
		time:_Endianness = "little" ;
	float End_time(t) ;
		End_time:long_name = "Time of last observation" ;
		End_time:standard_name = "End_time" ;
		End_time:units = "decimal year" ;
		End_time:_Storage = "contiguous" ;
		End_time:_Endianness = "little" ;
	float VMB(t) ;
		VMB:long_name = "Rate of mass balance" ;
		VMB:units = "Gt/year" ;
		VMB:_Storage = "chunked" ;
		VMB:_ChunkSizes = 28 ;
		VMB:_DeflateLevel = 4 ;
		VMB:_Shuffle = "true" ;
		VMB:_Endianness = "little" ;
	float VMBer(t) ;
		VMBer:long_name = "Rate of mass balance uncertainty" ;
		VMBer:units = "Gt/year" ;
		VMBer:_Storage = "chunked" ;
		VMBer:_ChunkSizes = 28 ;
		VMBer:_DeflateLevel = 4 ;
		VMBer:_Shuffle = "true" ;
		VMBer:_Endianness = "little" ;
	float VMB_basin_1(t) ;
		VMB_basin_1:long_name = "Rate of mass balance of Zwally basin 1" ;
		VMB_basin_1:units = "Gt/year" ;
		VMB_basin_1:_Storage = "chunked" ;
		VMB_basin_1:_ChunkSizes = 28 ;
		VMB_basin_1:_DeflateLevel = 4 ;
		VMB_basin_1:_Shuffle = "true" ;
		VMB_basin_1:_Endianness = "little" ;
	float VMBer_basin_1(t) ;
		VMBer_basin_1:long_name = "Rate of mass balance uncertainty of Zwally basin 1" ;
		VMBer_basin_1:units = "Gt/year" ;
		VMBer_basin_1:_Storage = "chunked" ;
		VMBer_basin_1:_ChunkSizes = 28 ;
		VMBer_basin_1:_DeflateLevel = 4 ;
		VMBer_basin_1:_Shuffle = "true" ;
		VMBer_basin_1:_Endianness = "little" ;
	float VMB_basin_2(t) ;
		VMB_basin_2:long_name = "Rate of mass balance of Zwally basin 2" ;
		VMB_basin_2:units = "Gt/year" ;
		VMB_basin_2:_Storage = "chunked" ;
		VMB_basin_2:_ChunkSizes = 28 ;
		VMB_basin_2:_DeflateLevel = 4 ;
		VMB_basin_2:_Shuffle = "true" ;
		VMB_basin_2:_Endianness = "little" ;
	float VMBer_basin_2(t) ;
		VMBer_basin_2:long_name = "Rate of mass balance uncertainty of Zwally basin 2" ;
		VMBer_basin_2:units = "Gt/year" ;
		VMBer_basin_2:_Storage = "chunked" ;
		VMBer_basin_2:_ChunkSizes = 28 ;
		VMBer_basin_2:_DeflateLevel = 4 ;
		VMBer_basin_2:_Shuffle = "true" ;
		VMBer_basin_2:_Endianness = "little" ;
	float VMB_basin_3(t) ;
		VMB_basin_3:long_name = "Rate of mass balance of Zwally basin 3" ;
		VMB_basin_3:units = "Gt/year" ;
		VMB_basin_3:_Storage = "chunked" ;
		VMB_basin_3:_ChunkSizes = 28 ;
		VMB_basin_3:_DeflateLevel = 4 ;
		VMB_basin_3:_Shuffle = "true" ;
		VMB_basin_3:_Endianness = "little" ;
	float VMBer_basin_3(t) ;
		VMBer_basin_3:long_name = "Rate of mass balance uncertainty of Zwally basin 3" ;
		VMBer_basin_3:units = "Gt/year" ;
		VMBer_basin_3:_Storage = "chunked" ;
		VMBer_basin_3:_ChunkSizes = 28 ;
		VMBer_basin_3:_DeflateLevel = 4 ;
		VMBer_basin_3:_Shuffle = "true" ;
		VMBer_basin_3:_Endianness = "little" ;
	float VMB_basin_4(t) ;
		VMB_basin_4:long_name = "Rate of mass balance of Zwally basin 4" ;
		VMB_basin_4:units = "Gt/year" ;
		VMB_basin_4:_Storage = "chunked" ;
		VMB_basin_4:_ChunkSizes = 28 ;
		VMB_basin_4:_DeflateLevel = 4 ;
		VMB_basin_4:_Shuffle = "true" ;
		VMB_basin_4:_Endianness = "little" ;
	float VMBer_basin_4(t) ;
		VMBer_basin_4:long_name = "Rate of mass balance uncertainty of Zwally basin 4" ;
		VMBer_basin_4:units = "Gt/year" ;
		VMBer_basin_4:_Storage = "chunked" ;
		VMBer_basin_4:_ChunkSizes = 28 ;
		VMBer_basin_4:_DeflateLevel = 4 ;
		VMBer_basin_4:_Shuffle = "true" ;
		VMBer_basin_4:_Endianness = "little" ;
	float VMB_basin_5(t) ;
		VMB_basin_5:long_name = "Rate of mass balance of Zwally basin 5" ;
		VMB_basin_5:units = "Gt/year" ;
		VMB_basin_5:_Storage = "chunked" ;
		VMB_basin_5:_ChunkSizes = 28 ;
		VMB_basin_5:_DeflateLevel = 4 ;
		VMB_basin_5:_Shuffle = "true" ;
		VMB_basin_5:_Endianness = "little" ;
	float VMBer_basin_5(t) ;
		VMBer_basin_5:long_name = "Rate of mass balance uncertainty of Zwally basin 5" ;
		VMBer_basin_5:units = "Gt/year" ;
		VMBer_basin_5:_Storage = "chunked" ;
		VMBer_basin_5:_ChunkSizes = 28 ;
		VMBer_basin_5:_DeflateLevel = 4 ;
		VMBer_basin_5:_Shuffle = "true" ;
		VMBer_basin_5:_Endianness = "little" ;
	float VMB_basin_6(t) ;
		VMB_basin_6:long_name = "Rate of mass balance of Zwally basin 6" ;
		VMB_basin_6:units = "Gt/year" ;
		VMB_basin_6:_Storage = "chunked" ;
		VMB_basin_6:_ChunkSizes = 28 ;
		VMB_basin_6:_DeflateLevel = 4 ;
		VMB_basin_6:_Shuffle = "true" ;
		VMB_basin_6:_Endianness = "little" ;
	float VMBer_basin_6(t) ;
		VMBer_basin_6:long_name = "Rate of mass balance uncertainty of Zwally basin 6" ;
		VMBer_basin_6:units = "Gt/year" ;
		VMBer_basin_6:_Storage = "chunked" ;
		VMBer_basin_6:_ChunkSizes = 28 ;
		VMBer_basin_6:_DeflateLevel = 4 ;
		VMBer_basin_6:_Shuffle = "true" ;
		VMBer_basin_6:_Endianness = "little" ;
	float VMB_basin_7(t) ;
		VMB_basin_7:long_name = "Rate of mass balance of Zwally basin 7" ;
		VMB_basin_7:units = "Gt/year" ;
		VMB_basin_7:_Storage = "chunked" ;
		VMB_basin_7:_ChunkSizes = 28 ;
		VMB_basin_7:_DeflateLevel = 4 ;
		VMB_basin_7:_Shuffle = "true" ;
		VMB_basin_7:_Endianness = "little" ;
	float VMBer_basin_7(t) ;
		VMBer_basin_7:long_name = "Rate of mass balance uncertainty of Zwally basin 7" ;
		VMBer_basin_7:units = "Gt/year" ;
		VMBer_basin_7:_Storage = "chunked" ;
		VMBer_basin_7:_ChunkSizes = 28 ;
		VMBer_basin_7:_DeflateLevel = 4 ;
		VMBer_basin_7:_Shuffle = "true" ;
		VMBer_basin_7:_Endianness = "little" ;
	float VMB_basin_8(t) ;
		VMB_basin_8:long_name = "Rate of mass balance of Zwally basin 8" ;
		VMB_basin_8:units = "Gt/year" ;
		VMB_basin_8:_Storage = "chunked" ;
		VMB_basin_8:_ChunkSizes = 28 ;
		VMB_basin_8:_DeflateLevel = 4 ;
		VMB_basin_8:_Shuffle = "true" ;
		VMB_basin_8:_Endianness = "little" ;
	float VMBer_basin_8(t) ;
		VMBer_basin_8:long_name = "Rate of mass balance uncertainty of Zwally basin 8" ;
		VMBer_basin_8:units = "Gt/year" ;
		VMBer_basin_8:_Storage = "chunked" ;
		VMBer_basin_8:_ChunkSizes = 28 ;
		VMBer_basin_8:_DeflateLevel = 4 ;
		VMBer_basin_8:_Shuffle = "true" ;
		VMBer_basin_8:_Endianness = "little" ;

// global attributes:
		:Title = "Radar mass balance of the Greenland Ice Sheet" ;
		:institution = "DTU Space - Div. of Geodesy and Earth Observation" ;
		:reference = "Simonsen et al. (2020)" ;
		:contact = "ssim@space.dtu.dk" ;
		:file_creation_date = "2020-09-14" ;
		:region = "Greenland" ;
		:missions_used = "ESA Radar altimeters: ERS-1, ERS-2, Envisat, CryoSat-2 and Sentinel-3" ;
		:time_coverage_start = "1992.0382513661202" ;
		:time_coverage_end = "2019.5341324200913" ;
		:Tracking_id = "47b09966-297e-4eb8-a621-f0c1337ab394" ;
		:netCDF_version = "NETCDF4" ;
		:product_version = "1.0" ;
		:Conventions = "CF-1.7" ;
		:summary = "Annual Mass balance for Greenland ice sheet and major drainage basins." ;
		:_NCProperties = "version=1|netcdflibversion=4.6.1|hdf5libversion=1.10.2" ;
		:_SuperblockVersion = 0 ;
		:_IsNetcdf4 = 1 ;
		:_Format = "netCDF-4" ;
}
#+end_example

*** Load VC

#+NAME: load_vc
#+BEGIN_SRC jupyter-python
import xarray as xr
ds = xr.open_dataset("/home/kdm/data/Simonsen_2021/ds1.nc")
# print(ds)

vc = xr.Dataset()

t = pd.to_datetime([pd.to_datetime(str(int(np.floor(t)))+'-01-01') + pd.to_timedelta((t-np.floor(t))*365, unit='D') for t in ds.time.values])
vc['time'] = (("time"), t)

vc['sector'] = (("sector"), np.arange(1,9))

vc['SEC'] = (("time","sector"), ds[['VMB_basin_1','VMB_basin_2','VMB_basin_3','VMB_basin_4','VMB_basin_5','VMB_basin_6','VMB_basin_7','VMB_basin_8']].to_dataframe().values)
vc['err'] = (("time","sector"), ds[['VMBer_basin_1','VMBer_basin_2','VMBer_basin_3','VMBer_basin_4','VMBer_basin_5','VMBer_basin_6','VMBer_basin_7','VMBer_basin_8']].to_dataframe().values)
#+END_SRC

#+RESULTS:


** GRACE

http://products.esa-icesheets-cci.org/products/downloadlist/GMB/

#+NAME: load_grace
#+BEGIN_SRC jupyter-python
import pandas as pd
from datetime import timedelta, datetime

root = '/home/kdm/data/CCI/GMB/greenland_gravimetric_mass_balance_rl06_dtuspace_v2_0-170820/time_series'
df = pd.read_csv(root + '/GIS00_grace.dat',
                 delim_whitespace=True,
                 header=None,
                 names=['date','mass','err'],
                 index_col=0)
# df = None
# for s in np.arange(1,9):
#     ss = str(s).zfill(2)
#     df_tmp = pd.read_csv(root + '/GIS'+ss+'_grace.dat',
#                          delim_whitespace=True,
#                          header=None,
#                          names=['date','mass','err'],
#                          index_col=0)
#     if df is None:
#         df = df_tmp
#     else:
#         df = df + df_tmp



def convert_partial_year(number):
    year = int(number)
    d = timedelta(days=(number - year)*365)
    day_one = datetime(year,1,1)
    date = d + day_one
    return date

df.index = pd.to_datetime([convert_partial_year(_) for _ in df.index])
grace = df
grace.head()
#+END_SRC

#+RESULTS: load_grace
|                            |    mass |     err |
|----------------------------+---------+---------|
| 2002-04-16 20:23:59.999999 | 1057.32 | 260.241 |
| 2002-05-12 09:35:59.999997 | 1122.55 | 116.317 |
| 2002-08-15 07:11:59.999997 | 801.489 | 65.6365 |
| 2002-09-17 03:36:00.000001 | 685.488 |  491.12 |
| 2002-10-16 08:23:59.999999 | 834.592 | 71.0989 |

** IMBIE

This spreadsheet contains the IMBIE-2019 datasets for Greenland, which includes data on the annual rate of change and cumulative change in Greenland’s ice sheet mass, its surface mass balance and ice discharge anomalies, and their estimated uncertainty. The data are expressed in units of rate of mass change (Gigatons per year – sheet 1, columns B, C, F, G, J and K) mass (Gigatons – sheet 1, columns D, E, H, I, L and M) and in units of equivalent mean global sea level rise (millimetres per year – sheet 2, columns B, C, F, G, J and K, and millimetres – sheet 2, columns D, E, H, I, L and M).

#+NAME: load_imbie
#+BEGIN_SRC jupyter-python
import pandas as pd
imbie = pd.read_excel("/home/kdm/data/IMBIE/imbie_dataset_greenland_dynamics-2020_02_28.xlsx", sheet_name=0, index_col=0, usecols=(0,1,2,3,4,5,6,9,10))\
       .rename(columns={"Rate of ice sheet mass change (Gt/yr)":"mb",
                        "Rate of ice sheet mass change uncertainty (Gt/yr)":"mb_err",
                        "Cumulative ice sheet mass change (Gt)" : "mb_cum",
	                "Cumulative ice sheet mass change uncertainty (Gt)" : "mb_cum_err",
                        "Rate of mass balance anomaly (Gt/yr)":"smb",
                        "Rate of mass balance anomaly uncertainty (Gt/yr)":"smb_err",
                        "Rate of ice dynamics anomaly (Gt/yr)":"D",
                        "Rate of ice dyanamics anomaly uncertainty (Gt/yr)":"D_err"})

imbie['index'] = pd.to_datetime('1980-01-01') + pd.to_timedelta((imbie.index-1980) * 365, unit="D")
# Appears that these fractional dates are supposed to be start-of-month? I think so...
# Let's hard-code this.
imbie['index'] = [pd.to_datetime(y + '-' + m + '-01') for y in imbie.index.astype(int).unique().astype(str) for m in np.arange(1,13).astype(str)]
imbie.index = imbie['index']
imbie = imbie.drop('index', axis='columns')
# imbie.tail(30)
#+END_SRC

#+RESULTS: load_imbie

** PROMICE MB

#+NAME: load_PROMICE_MB
#+BEGIN_SRC jupyter-python
def load_Colgan_2019(sheet=0):
    df_all = pd.read_excel("/home/kdm/data/Colgan_2019/MassBalance_07022019.xlsx", index_col=0, sheet_name=sheet)
    
    df_all = df_all.loc[df_all.index.dropna()]\
                   .drop(index='Total')\
                   .drop(columns='Unnamed: 43')
    if 'Unnamed: 44' in df_all.columns: df_all = df_all.drop(columns='Unnamed: 44')
    
    df_all.index = (df_all.index.astype(float) * 10).astype(int)
    df_all = df_all.T
    df_all = df_all.astype(float)
    df = df_all.iloc[::2]
    
    df.index = pd.to_datetime(df.index, format="%Y")

    df_err = df_all.iloc[1::2]
    df_err.index = df.index
    return df, df_err

p,e = load_Colgan_2019(sheet=3)

promice = xr.Dataset()
promice['time'] = (("time"), p.index)
promice['sector'] = (("sector"), p.columns)
promice['D'] = (("time","sector"), p.values)
promice['D_err'] = (("time","sector"), e.values)

p,e = load_Colgan_2019(sheet=4)
promice['SMB'] = (("time","sector"), p.values)
promice['SMB_err'] = (("time","sector"), e.values)

p,e = load_Colgan_2019(sheet=5)
promice['MB'] = (("time","sector"), p.values)
promice['MB_err'] = (("time","sector"), e.values)

# print(promice)
#+END_SRC

#+RESULTS: load_PROMICE_MB


* Uncertainty
** Reconstructed percent (approx)
:PROPERTIES:
:ID:       20210413T061118.026153
:END:

#+BEGIN_SRC jupyter-python
import xarray as xr

ds = xr.open_dataset('./TMB/mb_region.nc')\
       .sel({'time':slice('1840','1985')})

# print(ds)

df = ds[['smb','smb_err', 'mmb','mmb_err']].to_dataframe()

df['smb_err_pct'] = df['smb_err']/df['smb']*100
df['mmb_err_pct'] = df['mmb_err']/df['mmb']*100

# df[['smb_err_pct','mmb_err_pct']].plot()

df.describe()
#+END_SRC

#+RESULTS:
|       |      smb |   smb_err |       mmb |   mmb_err | smb_err_pct | mmb_err_pct |
|-------+----------+-----------+-----------+-----------+-------------+-------------|
| count |    13005 |     13005 |     13005 |     13005 |       13005 |       13005 |
| mean  | 0.897468 | 0.0807721 |   1.25922 |   0.11988 |           9 |     9.51953 |
| std   |  2.64252 |  0.237827 | 0.0744359 | 0.0118168 | 5.22246e-16 |    0.795106 |
| min   | -12.4337 |  -1.11903 |   1.12975 |   0.10534 |           9 |     9.06072 |
| 25%   | 0.216228 | 0.0194605 |   1.18499 |  0.110438 |           9 |     9.31591 |
| 50%   | 0.981213 | 0.0883091 |   1.27575 |   0.11989 |           9 |     9.44412 |
| 75%   |  2.16235 |  0.194611 |   1.32589 |  0.126881 |           9 |     9.57245 |
| max   |  13.2865 |   1.19578 |   1.40047 |  0.209752 |           9 |     18.0637 |

** Zwally & Mouginot overlap w/ RACMO                     :review:
:PROPERTIES:
:ID:       20210525T143510.416178
:END:

What % of RACMO is not covered by Zwally & Mouginot?

#+BEGIN_SRC bash
# grass -c ./G_RACMO/domain_overlap
grass ./G_RACMO/domain_overlap
g.region raster=mask

r.in.gdal -o input="NetCDF:${DATADIR}/RACMO/Icemask_Topo_Iceclasses_lon_lat_average_1km.nc:Promicemask" output=promicemask

g.list type=raster mapset=* -m
g.list type=vector mapset=* -m

d.mon wx0
# g.gui.mapswipe first=mask_ice@PERMANENT second=mask_ice_shrink@ROI
d.rast mask_ice@PERMANENT
d.vect basins@Mouginot_2019 fillcolor=none # basins is also a raster

r.mapcalc "mouginot = if(basins@Mouginot_2019)"
r.mapcalc "mouginot_e = if(basins_e@Mouginot_2019)"
r.mapcalc "zwally = if(sectors@Zwally_2012)"
r.mapcalc "zwally_e = if(sectors_e@Zwally_2012)"
r.mapcalc "RACMO = if(mask_ice@PERMANENT)"

r.category map=mouginot separator=":" rules=- << EOF
1:M2019
EOF
r.category map=zwally separator=":" rules=- << EOF
1:Z2012
EOF
r.category map=RACMO separator=":" rules=- << EOF
1:RACMO
EOF

r.report
#+END_SRC

#+BEGIN_SRC bash :session *total_mass_balance-shell* :cache yes :results verbatim
# r.report -hi units=k map=mouginot,RACMO
r.report -hi units=k map=RACMO,mouginot --q
echo ""
r.report -hi units=k map=RACMO,zwally --q
#+END_SRC

#+RESULTS[(2021-07-30 08:33:59) d2d0666b0232e30b709e147825c7b2c5546415b2]:
#+begin_example

+-----------------------------------------------------------------------------+
|                      Category Information                        |  square  |
|description                                                     |kilometers|
|-----------------------------------------------------------------------------|
|1|RACMO                                                           | 1,718,959|
| |----------------------------------------------------------------|----------|
| |1|M2019. . . . . . . . . . . . . . . . . . . . . . . . . . . . .| 1,696,419|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .|    22,540|
|------------------------------------------------------------------|----------|
|*|no data                                                         | 2,320,241|
| |----------------------------------------------------------------|----------|
| |1|M2019. . . . . . . . . . . . . . . . . . . . . . . . . . . . .|    20,685|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .| 2,299,556|
|-----------------------------------------------------------------------------|
|TOTAL                                                             | 4,039,200|
+-----------------------------------------------------------------------------+

+-----------------------------------------------------------------------------+
|                      Category Information                        |  square  |
|description                                                     |kilometers|
|-----------------------------------------------------------------------------|
|1|RACMO                                                           | 1,718,959|
| |----------------------------------------------------------------|----------|
| |1|Z2012. . . . . . . . . . . . . . . . . . . . . . . . . . . . .| 1,678,864|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .|    40,095|
|------------------------------------------------------------------|----------|
|*|no data                                                         | 2,320,241|
| |----------------------------------------------------------------|----------|
| |1|Z2012. . . . . . . . . . . . . . . . . . . . . . . . . . . . .|    24,475|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .| 2,295,766|
|-----------------------------------------------------------------------------|
|TOTAL                                                             | 4,039,200|
+-----------------------------------------------------------------------------+
#+end_example

What % of SMB changes occur in the uncovered regions?

#+BEGIN_SRC jupyter-python
import xarray as xr
ds = xr.open_mfdataset('/home/kdm/data/RACMO/daily/smb_rec.201*.nc', combine='by_coords')
ds = ds.resample({'time':'YS'}).sum()
ds.to_netcdf('./tmp/RACMO_2010s.nc')

ds = xr.open_mfdataset('/home/kdm/data/RACMO/daily/smb_rec.2020*.nc', combine='by_coords')
ds = ds.sum(dim='time')
ds.to_netcdf('./tmp/RACMO_2020.nc')
#+END_SRC

#+RESULTS:

2020

#+BEGIN_SRC bash
r.in.gdal -o input="NetCDF:./tmp/RACMO_2020.nc:smb_rec" output=smb_2020 --o  --q
r.region map=smb_2020 region=RCM
r.univar -g map=smb_2020 zones=RACMO | grep sum # 318648388
r.univar -g map=smb_2020 zones=mouginot | grep sum # 338377530
r.univar -g map=smb_2020 zones=zwally | grep sum # 351498574
#+END_SRC

| Domain  | Mass gain [Gt] |
|---------+----------------|
| RACMO   |            319 |
| RACMO M |            338 |
| RACMO Z |            351 |

2010s

#+NAME: RMZ
#+BEGIN_SRC bash :session *total_mass_balance-shell* :cache yes
echo "Year R M Z"
for i in $(seq 10); do
  r.in.gdal -o input="NetCDF:./tmp/RACMO_2010s.nc:smb_rec" band=${i} output=smb --o  --q 2>/dev/null
  r.region map=smb region=RCM --q
  eval $(r.univar -g map=smb zones=RACMO | grep sum | sed 's/sum/R/')
  eval $(r.univar -g map=smb zones=mouginot | grep sum | sed 's/sum/M/')
  eval $(r.univar -g map=smb zones=zwally | grep sum | sed 's/sum/Z/')
  echo $(( 2010 + $(( ${i}-1)) )) ${R} ${M} ${Z}
done
#+END_SRC

#+RESULTS[(2021-07-30 08:34:18) e0760b49600c22820b5ff97f5cfcc4a62ccbf7e4]: RMZ
| Year |                R |                M |                Z |
| 2010 | 138865169.863205 | 164141022.770475 | 188693278.633955 |
| 2011 | 162135323.732203 | 183110164.892631 | 192990105.059272 |
| 2012 | 119050877.731281 | 145874329.623628 | 164998233.714134 |
| 2013 | 392202448.814299 |   405757511.6404 |  408685069.94199 |
| 2014 |  304046429.91578 | 322362180.505164 | 327710495.051068 |
| 2015 | 294980810.845057 | 311374539.451985 | 319797206.757863 |
| 2016 | 233733244.138948 | 257247406.928596 | 266283590.975364 |
| 2017 | 399613619.668526 | 415379433.607018 | 425252437.272332 |
| 2018 |  420151685.34887 | 429386998.485341 | 429046485.925065 |
| 2019 | 45504675.1813576 | 76604095.5701525 | 84186156.4168217 |

#+BEGIN_SRC python :var tab=RMZ :results table output
import pandas as pd
df = pd.DataFrame(tab[:][1:], columns=tab[:][0])
df = df.set_index('Year')
df = df / 1E6

df['M%'] = df['M'] / df['R'] * 100 - 100
df['Z%'] = df['Z'] / df['R'] * 100 - 100

print(df)
print(df.describe())
#+END_SRC

#+RESULTS:
#+begin_example
R           M           Z         M%         Z%
Year                                                          
2010  138.865170  164.141023  188.693279  18.201723  35.882366
2011  162.135324  183.110165  192.990105  12.936626  19.030265
2012  119.050878  145.874330  164.998234  22.531083  38.594723
2013  392.202449  405.757512  408.685070   3.456139   4.202580
2014  304.046430  322.362181  327.710495   6.023998   7.783043
2015  294.980811  311.374539  319.797207   5.557558   8.412885
2016  233.733244  257.247407  266.283591  10.060256  13.926280
2017  399.613620  415.379434  425.252437   3.945264   6.415902
2018  420.151685  429.386998  429.046486   2.198090   2.117045
2019   45.504675   76.604096   84.186156  68.343352  85.005510
                R           M           Z         M%         Z%
count   10.000000   10.000000   10.000000  10.000000  10.000000
mean   251.028429  271.123768  280.764306  15.325409  22.137060
std    131.568677  125.227913  120.826545  19.802262  25.502269
min     45.504675   76.604096   84.186156   2.198090   2.117045
25%    144.682708  168.883308  189.767485   4.348338   6.757687
50%    264.357027  284.310973  293.040399   8.042127  11.169583
75%    370.163444  384.908679  388.441426  16.885449  31.669341
max    420.151685  429.386998  429.046486  68.343352  85.005510
#+end_example


** Lost BMB_VHD (MAR vs. BedMachine)                      :review:
:PROPERTIES:
:ID:       20210526T104310.630293
:END:

What % of MAR is not covered by BedMachine?

#+BEGIN_SRC bash
# grass -c ./G_MAR/BedMachine_overlap
grass ./G_MAR/BedMachine_overlap
g.region raster=mask

g.list type=raster mapset=* -m
g.list type=vector mapset=* -m

# d.mon wx0
# # g.gui.mapswipe first=mask_ice@PERMANENT second=mask_ice_shrink@ROI
# d.rast mask_ice@PERMANENT
# d.rast mask_bedmachine@VHD

r.mapcalc "MAR = if(smb@PERMANENT, 1, null())" --o
r.mapcalc "BedMachine = if(delta_head@VHD, 1, null())" --o

r.category map=MAR separator=":" rules=- << EOF
1:MAR
EOF
r.category map=BedMachine separator=":" rules=- << EOF
1:BedMachine
EOF
#+END_SRC

#+BEGIN_SRC bash :session *total_mass_balance-shell* :cache yes :results verbatim
r.report -hi units=k map=MAR,BedMachine --q
echo ""
echo ""
r.report -hi units=k map=BedMachine,MAR --q
#+END_SRC

#+RESULTS[(2021-07-31 20:43:49) 2c555bc62cbe34c18ac9d67f625bb17cd01633b1]:
#+begin_example
+-----------------------------------------------------------------------------+
|                      Category Information                        |  square  |
|description                                                     |kilometers|
|-----------------------------------------------------------------------------|
|1|MAR                                                             | 1,828,800|
| |----------------------------------------------------------------|----------|
| |1|BedMachine . . . . . . . . . . . . . . . . . . . . . . . . . .| 1,711,200|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .|   117,600|
|------------------------------------------------------------------|----------|
|*|no data                                                         | 2,305,600|
| |----------------------------------------------------------------|----------|
| |1|BedMachine . . . . . . . . . . . . . . . . . . . . . . . . . .|    26,000|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .| 2,279,600|
|-----------------------------------------------------------------------------|
|TOTAL                                                             | 4,134,400|
+-----------------------------------------------------------------------------+


+-----------------------------------------------------------------------------+
|                      Category Information                        |  square  |
|description                                                     |kilometers|
|-----------------------------------------------------------------------------|
|1|BedMachine                                                      | 1,737,200|
| |----------------------------------------------------------------|----------|
| |1|MAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .| 1,711,200|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .|    26,000|
|------------------------------------------------------------------|----------|
|*|no data                                                         | 2,397,200|
| |----------------------------------------------------------------|----------|
| |1|MAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .|   117,600|
| |*|no data. . . . . . . . . . . . . . . . . . . . . . . . . . . .| 2,279,600|
|-----------------------------------------------------------------------------|
|TOTAL                                                             | 4,134,400|
+-----------------------------------------------------------------------------+
#+end_example


What % of MAR runoff occurs in the uncovered regions?

#+BEGIN_SRC jupyter-python
import xarray as xr
ds = xr.open_mfdataset('/home/kdm/data/MAR/3.12/MAR-201*.nc', combine='by_coords')['ru']
ds = ds.sel({'sector':1}).resample({'time':'YS'}).mean()
ds = ds.where(~np.isinf(ds), 0)
ds.to_netcdf('./tmp/MAR_2010s.nc')

ds = xr.open_mfdataset('/home/kdm/data/MAR/3.12/MAR-2020.nc', combine='by_coords')['ru']
ds = ds.sel({'sector':1}).sum(dim='time')
ds.to_netcdf('./tmp/MAR_2020.nc')
#+END_SRC

#+RESULTS:

2020

#+BEGIN_SRC bash
r.in.gdal -o input="NetCDF:./tmp/MAR_2020.nc:ru" output=RU_2020 --o  --q
r.region map=RU_2020 region=RCM
r.mapcalc "RU_2020 = RU_2020 * MAR"
r.univar -g map=RU_2020 zones=MAR | grep sum # 1253668
r.univar -g map=RU_2020 zones=BedMachine | grep sum # 969295
#+END_SRC

2010s

#+NAME: MAR_BedMachine
#+BEGIN_SRC bash :session *total_mass_balance-shell* :cache yes
echo "Year MAR BedMachine"
for i in $(seq 10); do
  r.in.gdal -o input="NetCDF:./tmp/MAR_2010s.nc:ru" band=${i} output=RU --o  --q 2>/dev/null
  r.region map=RU region=RCM --q
  r.mapcalc "RU = RU * MAR" --o --q
  eval $(r.univar -g map=RU zones=MAR | grep sum | sed 's/sum/MAR/')
  eval $(r.univar -g map=RU zones=BedMachine | grep sum | sed 's/sum/BedMachine/')
  echo $(( 2010 + $(( ${i}-1)) )) ${MAR} ${BedMachine}
done
#+END_SRC

#+RESULTS[(2021-08-02 05:18:52) f4030c2ff71f4076318c6c2b9e23f344733420d7]: MAR_BedMachine
| Year |              MAR |       BedMachine |
| 2010 | 4319.52092726302 | 3607.27684702079 |
| 2011 |  3477.6857996143 | 2711.85864046108 |
| 2012 | 5244.02845465436 | 4322.73603488805 |
| 2013 | 2386.84762438336 | 1945.21224011221 |
| 2014 |  3239.0651165512 | 2670.10820556323 |
| 2015 | 3079.28153709474 | 2396.97229131198 |
| 2016 | 3905.23436653402 | 3151.96722148519 |
| 2017 | 2770.57749353875 | 2234.01437787537 |
| 2018 | 2272.97510659068 | 1870.04163381166 |
| 2019 | 4703.14477661515 | 3782.83963157127 |


#+BEGIN_SRC python :var tab=MAR_BedMachine :results table output
import pandas as pd
df = pd.DataFrame(tab[:][1:], columns=tab[:][0])
df = df.set_index('Year')
df = df / 1E6

df['BM%'] = df['BedMachine'] / df['MAR'] * 100 - 100
df['MAR%'] = df['MAR'] / df['BedMachine'] * 100

print(df)
print(df.describe())
#+END_SRC

#+RESULTS:
#+begin_example
MAR  BedMachine        BM%        MAR%
Year                                             
2010  0.004320    0.003607 -16.488960  119.744647
2011  0.003478    0.002712 -22.021172  128.239937
2012  0.005244    0.004323 -17.568410  121.312715
2013  0.002387    0.001945 -18.502873  122.703712
2014  0.003239    0.002670 -17.565467  121.308384
2015  0.003079    0.002397 -22.158066  128.465462
2016  0.003905    0.003152 -19.288654  123.898318
2017  0.002771    0.002234 -19.366472  124.017890
2018  0.002273    0.001870 -17.727140  121.546765
2019  0.004703    0.003783 -19.567868  124.328421
             MAR  BedMachine        BM%        MAR%
count  10.000000   10.000000  10.000000   10.000000
mean    0.003540    0.002869 -19.025508  123.556625
std     0.000991    0.000824   1.883095    2.912486
min     0.002273    0.001870 -22.158066  119.744647
25%     0.002848    0.002275 -19.517519  121.371228
50%     0.003358    0.002691 -18.895763  123.301015
75%     0.004216    0.003493 -17.608092  124.250788
max     0.005244    0.004323 -16.488960  128.465462
#+end_example




* Figures
** Notes

From ESSD:

#+BEGIN_SRC LaTeX
% ONE-COLUMN FIGURES
\begin{figure}[t]
\includegraphics[width=8.3cm]{FILE NAME}
\caption{TEXT}
\end{figure}

% TWO-COLUMN FIGURES
\begin{figure*}[t]
\includegraphics[width=12cm]{FILE NAME}
\caption{TEXT}
\end{figure*}
#+END_SRC


** Overview map
*** ROIs w/o coast

+ Coast is noisy and poorly defined due to different RCM boundaries
+ Generate ROIs without the coast, just interior divisions

#+BEGIN_SRC bash
grass -c ./G_RACMO/ROI
g.region region=sectors -pa

r.mask -r

g.copy vector=sectors_e@Zwally_2012,sector
g.copy vector=regions_e@Mouginot_2019,region

for roi in sector region; do
  v.to.lines input=${roi} output=${roi}_lines
  v.to.rast input=${roi}_lines output=${roi}_lines type=line use=val val=1

  r.grow input=mask_ice radius=-3 output=mask_ice_shrink
  r.mask mask_ice_shrink

  r.thin input=${roi}_lines output=${roi}_thin

  r.to.vect input=${roi}_thin output=${roi}_interior type=line

  v.clean input=${roi}_interior output=${roi}_clean tool=rmdangle threshold=10000

  v.generalize input=${roi}_clean output=${roi}_interior_general method=douglas threshold=5000
  v.generalize input=${roi}_interior_general output=${roi}_interior_smooth method=chaiken threshold=1

  v.out.ogr input=${roi}_interior_smooth output=./tmp/${roi}_interior.gpkg
done
#+END_SRC


*** Greenland outline
#+BEGIN_SRC bash
wget https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip -O ./tmp/countries.zip

(cd tmp; unzip countries.zip)
v.import input=./tmp/ne_10m_admin_0_countries.shp output=countries
v.extract input=countries output=greenland where='name = "Greenland"'
#+END_SRC

*** Other
#+BEGIN_SRC bash
v.import input=~/data/Mankoff_2020/ice/latest/gates.gpkg output=gates
#+END_SRC

*** Graphic

#+BEGIN_SRC bash :session *total_mass_balance-shell* :results verbatim
lightblue="166:206:227"
lightgreen="178:223:138"
pink="251:154:153"

cat << EOF | ps.map -e input=- output=./tmp/overview.eps --o

border n

# scale 1:1000000

paper a3
  end

vareas gates
  color black
  width 3
  label MMB gates
  end

text -94000 -1106944 NO
  color ${lightblue}
  fontsize 24
  end

text -185295 -1605917 NW
  color ${lightblue}
  fontsize 24
  end

text 202794 -1403555 NE
  color ${lightblue}
  fontsize 24
  end

text 39242 -2121523 CW
  color ${lightblue}
  fontsize 24
  end

text 466141 -2124295 CE
  color ${lightblue}
  fontsize 24
  end

text -120000 -2800681 SW
  color ${lightblue}
  fontsize 24
  end

text 110000 -2650000 SE
  color ${lightblue}
  fontsize 24
  end

text -254597 -1154069 11
  color ${pink}
  fontsize 18
  end

text -10655 -1034870 12
  color ${pink}
  fontsize 18
  end

text 144580 -1015465 13
  color ${pink}
  fontsize 18
  end

text 294272 -1021009 14
  color ${pink}
  fontsize 18
  end
     
text 247147 -1281584 21
  color ${pink}
  fontsize 18
  end

text 452281 -1436820 22
  color ${pink}
  fontsize 18
  end

text 424560 -1736204 23
  color ${pink}
  fontsize 18
  end

text 349714 -1941338 31
  color ${pink}
  fontsize 18
  end

text 650000 -2157560 32
  color ${pink}
  fontsize 18
  end

text 380000 -2257354 33
  color ${pink}
  fontsize 18
  end

text 220000 -2451399 41
  color ${pink}
  fontsize 18
  end

text 83595 -2700000 42
  color ${pink}
  fontsize 18
  end

text 60000 -2936512 43
  color ${pink}
  fontsize 18
  end

text 10000 -3105609 50
  color ${pink}
  fontsize 18
  end

text -100000 -2906020 61
  color ${pink}
  fontsize 18
  end

text -100000 -2595547 62
  color ${pink}
  fontsize 18
  end

text 47558 -2262898 71
  color ${pink}
  fontsize 18
  end

text -93817 -2027272 72
  color ${pink}
  fontsize 18
  end

text -265685 -1544931 81
  color ${pink}
  fontsize 18
  end

text -450000 -1312077 82
  color ${pink}
  fontsize 18
  end

vlines sector_interior_smooth
  color $pink
  width 2
  label Sector       
  end
    
vlines region_interior_smooth
  color $lightblue
  width 5
  label Region
  end
    
vareas sectors@Zwally_2012
  color white
  fcolor white
  label Ice
  end

vareas greenland
  color gray
  fcolor 192:192:192
  label Land
  end

# vlegend
#   where 6 13
#   fontsize 18
#   end

# scalebar s
#   length 100
#   units kilometers
#   segment 1
#   where 7 12
#   fontsize 18
#   end
 
EOF
convert -trim ./tmp/overview.eps ./fig/overview.png
o ./fig/overview.png
#+END_SRC

#+RESULTS:
#+begin_example
[Raster MASK present]
[Raster MASK present]
[Raster MASK present]
[Raster MASK present]
> > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > Scale set to 1 : 7313327
...
...
...
...
...
Reading text file ...
ps.map complete. PostScript file './tmp/overview.eps' successfully written.
[Raster MASK present]
[Raster MASK present]
[Raster MASK present]
#+end_example



** SMB/MMB/MB timeseries
*** GIS
#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
import matplotlib.dates as dates
from adjust_spines import adjust_spines as adj

import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

fig = plt.figure(1, figsize=(8,6)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(211)

mb = xr.open_dataset("./TMB/mb_sector.nc")

kw = {'ax':ax, 'legend':False, 'drawstyle':'steps-post'}

smb = mb[['smb','smb_err']]\
    .to_dataframe()\
    .resample('1D')\
    .interpolate(method='time')\
    .resample('AS')\
    .sum()
smb['smb'].plot(color='b', linestyle='-', alpha=0.5, **kw)
ax.fill_between(smb.index.values,
                (smb['smb']-smb['smb_err']).values.flatten(),
                (smb['smb']+smb['smb_err']).values.flatten(),
                color='b', alpha=0.1,
                step='post')

mmbbmb = -mb[['mmb','bmb','mmb_err','bmb_err']]\
    .to_dataframe()\
    .resample('1D')\
    .interpolate(method='time')\
    .resample('AS')\
    .sum()
mmbbmb['sum'] = mmbbmb[['mmb','bmb']].sum(axis='columns')
mmbbmb['err'] = (mmbbmb['mmb_err']**2 + mmbbmb['bmb_err']**2)**0.5

mmbbmb['sum'].plot(color='gray', linestyle='--', **kw)
ax.fill_between(mmbbmb.index.values,
                (mmbbmb['sum']-mmbbmb['err']).values.flatten(),
                (mmbbmb['sum']+mmbbmb['err']).values.flatten(),
                color='k', alpha=0.1,
                step='post')

mb_ann = mb[['mb','mb_err']].to_dataframe()\
                             .resample('1D')\
                             .interpolate(method='time')\
                             .resample('AS')\
                             .sum()

mb_ann['mb'].plot(color='k', linestyle='-', **kw)

ax.fill_between(mb_ann.index.values,
                (mb_ann['mb'] - mb_ann['mb_err']).values.flatten(),
                (mb_ann['mb'] + mb_ann['mb_err']).values.flatten(),
                color='k', alpha=0.5,
                step='post')



adj(ax, ['left','bottom'])
ax.grid(b=True, which='major', axis='y', alpha=0.33)
# ax.xaxis.set_major_locator(dates.YearLocator(20, month=1, day=1))
# ax.xaxis.set_minor_locator(dates.YearLocator())

ticks = [datetime.datetime(i, 1, 1) for i in range(1840, 2021, 20)]
ax.set_xticks(ticks)
ax.set_xticklabels(range(1840,2021,20))
# locator = dates.AutoDateLocator(interval_multiples=True)
# ax.xaxis.set_major_locator(locator)
# ax.set_xticks(ticks)

ax.set_ylabel("Mass gain [Gt yr$^{-1}$]")
ax.set_xlabel("")



ax2 = fig.add_subplot(212)
kw = {'ax':ax2, 'drawstyle':'steps-post'}
mb = mb.sel({'time':slice('2019-01-01','2020-12-31')})

smb = mb[['smb','smb_err']].to_dataframe()
smb['smb'].rename(index='SMB').plot(color='b', linestyle='-', legend=True, alpha=0.5, **kw)
# ax2.fill_between(smb.index,
#                  (smb['smb']-smb['smb_err']).values.flatten(),
#                  (smb['smb']+smb['smb_err']).values.flatten(),
#                  color='b', alpha=0.1,
#                  step='post')

mmbbmb = -mb[['mmb','bmb','mmb_err','bmb_err']].to_dataframe()
mmbbmb['sum'] = mmbbmb[['mmb','bmb']].sum(axis='columns')
mmbbmb['err'] = (mmbbmb['mmb_err']**2 + mmbbmb['bmb_err']**2)**0.5
mmbbmb['sum'].rename(index='MMB+BMB').plot(color='gray', linestyle='--', legend=True, **kw)
# ax2.fill_between(mmbbmb.index.values,
#                  (mmbbmb['sum']-mmbbmb['err']).values.flatten(),
#                  (mmbbmb['sum']+mmbbmb['err']).values.flatten(),
#                  color='k', alpha=0.1,
#                  step='post')



mb = mb[['mb','mb_err']].to_dataframe()
mb['mb'].rename(index='MB').plot(color='k', linestyle='-', legend=True, **kw)
# ax2.fill_between(mb.index.values,
#                  (mb['mb'] - mb['mb_err']).values.flatten(),
#                  (mb['mb'] + mb['mb_err']).values.flatten(),
#                  color='k', alpha=0.5,
#                  step='post')

ax2.set_xlabel("Time [Month; Year]")
ax2.set_ylabel("Mass gain [Gt d$^{-1}$]")
ax2.xaxis.set_minor_locator(plt.NullLocator())


adj(ax2, ['left','bottom'])
ax2.grid(b=True, which='major', axis='y', alpha=0.33)

plt.legend(loc='lower right', framealpha=0, ncol=3)

plt.savefig('fig/mb_ts.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:

*** Sector

Improve the figure generated below with an Inkscape overlay

#+BEGIN_SRC bash :results verbatim
inkscape -z ./fig/overview.svg -e ./fig/overview_w_plots.png
#+END_SRC

#+RESULTS:
: Background RRGGBBAA: ffffff00
: Area 0:0:771.735:862.218 exported to 772 x 862 pixels (96 dpi)
: Bitmap saved as: ./fig/overview_w_plots.png


#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj

import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=10)
rc('text', usetex=False)

plt.close()
fig = plt.figure(1, figsize=(2.5,1.5)) # w,h

mb = xr.open_dataset("./TMB/mb_region.nc")
mb = mb.sel({'time':slice('1986','2099')})

for r in mb['region'].values:

    fig.clf()
    fig.set_tight_layout(True)
    ax = fig.add_subplot(111)
    kw = {'ax':ax, 'legend':False, 'drawstyle':'steps-post'}

    mb_r = mb.sel({'region':r})

    mb_r_smb = mb_r['smb_ROI'].to_dataframe(name='M2021')\
                              .resample('AS')\
                              .sum()\
                              .rename(columns={'M2021':'SMB'})
    mb_r_smb.plot(color='b', linestyle='-', alpha=0.5, **kw)
    # ax.fill_between(mb_r_smb.index,
    #                 mb_r_smb.values.flatten(),
    #                 color='b', alpha=0.25, step='post')
    
    (-1*(mb_r['mmb_ROI'] + mb_r['bmb_ROI'])).to_dataframe(name='M2021')\
                        .resample('AS')\
                        .sum()\
                        .rename(columns={'M2021':'MMB + BMB'})\
                        .plot(color='gray', linestyle='--', **kw)

    mb_r_mb = mb_r['mb_ROI'].to_dataframe(name='M2021')\
                            .resample('AS')\
                            .sum()\
                            .rename(columns={'M2021':'MB'})
    mb_r_mb.plot(color='k', linestyle='-', **kw)

    # mb_r_pos = mb_r_mb.where(mb_r_mb['MB'] > 0, 0)
    # mb_r_neg = mb_r_mb.where(mb_r_mb['MB'] < 0, 0)
    # ax.fill_between(mb_r_pos.index, mb_r_pos.values.flatten(), color='r', alpha=0.1, step='post')
    # ax.fill_between(mb_r_neg.index, mb_r_neg.values.flatten(), color='b', alpha=0.1, step='post')

    # ax.fill_between(mb_r_mb.index,
    #                 mb_r_mb.values.flatten(),
    #                 color='k',
    #                 alpha=0.25,
    #                 step='post')

    # plt.legend(loc='lower left')

    # ax.set_ylabel("Mass gain [Gt yr$^{-1}$]")
    ax.set_ylabel("")
    ax.set_xlabel("")
    ax.set_xticks(ax.get_xlim())
    # ax.set_xticklabels(ax.get_xlim())

    yt = {'NO':50, 'NE':50, 'NW':100, 'CW':100, 'SW':100, 'SE':200, 'CE':100}
    ax.set_yticks([-yt[r], 0, yt[r]])
    
    # if r == 'SE':
        # ax.set_ylabel('Mass gain [Gt yr$^{-1}$]')
        # ax.set_xlabel('Time [Year]')
        # ax.set_xticklabels(['1986','2021'])

    adj(ax, ['left','bottom'])

    ax.grid(b=True, which='major', axis='y', alpha=0.33)

    # plt.setp(ax.xaxis.get_majorticklabels(), rotation=90)
    # plt.ion()
    # plt.show()
    plt.savefig('fig/mb_ts_'+r+'.png', transparent=True, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:

** SMB/MMB/MB vs all others: Reverse accumulated

Improve the figure generated below with an Inkscape overlay

#+BEGIN_SRC bash :results verbatim
inkscape -z ./fig/mb_cumsum_compare_manual.svg -e ./fig/mb_cumsum_compare_manual.png
#+END_SRC

#+RESULTS:
: Background RRGGBBAA: ffffff00
: Area 0:0:752.64:560.32 exported to 753 x 560 pixels (96 dpi)
: Bitmap saved as: ./fig/mb_cumsum_compare_manual.png


#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
from matplotlib.lines import Line2D

from adjust_spines import adjust_spines as adj
import matplotlib.dates as dates
import matplotlib.ticker as tck

import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

# | Color       |   R |   G |   B | hex     |
# |-------------+-----+-----+-----+---------|
# | light blue  | 166 | 206 | 227 | #a6cee3 |
# | dark blue   |  31 | 120 | 180 | #1f78b4 |
# | light green | 178 | 223 | 138 | #b2df8a |
# | dark green  |  51 | 160 |  44 | #33a02c |
# | pink        | 251 | 154 | 153 | #fb9a99 |
# | red         | 227 |  26 |  28 | #e31a1c |
# | pale orange | 253 | 191 | 111 | #fdbf6f |
# | orange      | 255 | 127 |   0 | #ff7f00 |

C_this = 'k'
C_M2019 = 'gray'
C_GMB = '#e31a1c'
C_VC = '#045a8d' # '1f78b4'
C_IMBIE = '#74a9cf'
C_PROMICE = '#fdbf6f'

OFFSET = 00         # where to put 0 for This Study
ADJ = 2150
OFFSET_M2019 = 2800 + ADJ
OFFSET_GRACE = ADJ
OFFSET_VC = 2900 + ADJ
OFFSET_IMBIE = 2900 + ADJ
OFFSET_C2019 = 2100 + ADJ
OFFSET_K2015 = 1900 + ADJ

# plt.close(1)
fig = plt.figure(1, figsize=(8,6)) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
fig.set_tight_layout(True)
# import matplotlib.gridspec as gridspec
# gs = gridspec.GridSpec(1, 1) #w,h
# ax = plt.subplot(gs[:,:])
ax = fig.add_subplot(111)


axins = ax.inset_axes([0.15, 0.08, 0.7, 0.38])


kw = {'clip_on':True, 'linewidth':1, 'legend':False}
kw_err = {'clip_on':True, 'linewidth':1, 'alpha':0.1}

# MB (this)
mb = xr.open_dataset('./TMB/mb_sector.nc')
mb = mb.sel({'time':slice('1971','2100')})

bmb = xr.open_dataset('./tmp/bmb.nc').sum(dim='region').sel({'time':slice('1971','2100')})
# remove un-used sector-level data
bmb = bmb.drop_vars([_ for _ in bmb.keys() if 'sector' in _])\
         .drop_vars(['GF_region','vel_region','VHD_region','sector'])
bmb = bmb.rename_vars({'GF_region_err':'GF_err', 'vel_region_err':'vel_err', 'VHD_region_err':'VHD_err'})

this = mb['mb'].to_dataframe(name='mb').resample('1D').ffill().cumsum()

# this['err'] = mb['mmb_err'].to_dataframe(name='err')[::-1].cumsum()[::-1]

mmb_err = mb['mmb_err'].to_dataframe(name='err')
smb_err = 0

bmb_err_recent = bmb['GF_err'].values + 0*bmb['VHD_err'].rename('err').to_dataframe(name='err')
bmb_err_reconstruct = mb['bmb_err'].sel(time=slice('1971','1985')).to_dataframe(name='err')
bmb_err = (bmb_err_reconstruct*365).append(bmb_err_recent)

this_err = (mmb_err**2 + smb_err**2 + bmb_err**2)**0.5
this['err'] = this_err[::-1].cumsum()[::-1]

this.loc[:'1985'] = this.loc[:'1985'].resample('Y').mean().resample('1D').bfill()

this['mb'] = this['mb'] + OFFSET

tt = (this.index.max() - this.index) / (this.index.max() - pd.Timestamp('2000-01-01'))
tt = tt.where(tt <= 1, 1)

this['bumpy'] = this['mb']
this['smooth'] = this['bumpy'].rolling(365).mean()
this['err_core'] = (this['bumpy'] * (1-tt)) + (this['smooth'] * tt)
for a in [ax,axins]:
    p_this = this['mb'].plot(ax=a, color=C_this, drawstyle='steps-post', **kw)
    a.fill_between(this.index,
                   (this['err_core'] - this['err']).values.flatten(),
                   (this['err_core'] + this['err']).values.flatten(),
                   color=C_this, clip_on=True, alpha=0.05)
    
# Mouginot
if 'mouginot' not in locals():
    <<load_mouginot>>
M2019 = mouginot['MB'].sum(dim='region').to_dataframe('M2019').cumsum()
M2019 = M2019 - np.min(M2019) - OFFSET_M2019
M2019_err = mouginot['D_err'].sum(dim='region').to_dataframe('M2019')[::-1].cumsum()[::-1]
M2019 = M2019[M2019.index.year >= 1971]
M2019_err = M2019_err[M2019_err.index.year >= 1971]
for a in [ax,axins]:
    p_M2019 = M2019.plot(ax=a, label='M2019', color=C_M2019, **kw)
    (M2019-M2019_err).plot(ax=a, color=C_M2019, alpha=0.25, linestyle='--', **kw)
    (M2019+M2019_err).plot(ax=a, color=C_M2019, alpha=0.25, linestyle='--', **kw)



# GRACE
<<load_grace>>
grace_scale = 1 
grace['plot'] = grace_scale * grace['mass'] - OFFSET_GRACE
grace['plot_err'] = grace_scale * grace['err']
for a in [ax,axins]:
    p_gmb = grace['plot'].plot(ax=a, label='GMB', color=C_GMB, **kw)
    a.fill_between(grace['plot'].index,
                   (grace['plot'] + grace['plot_err']),
                   (grace['plot'] - grace['plot_err']),
                   color=C_GMB, **kw_err)

# grace_scale, grace_offset = 0.84, -2800
# grace['plot'] = grace_scale * grace['mass']+grace_offset
# grace['plot_err'] = grace_scale * grace['err']
# p_gmb = grace['plot'].plot(ax=ax, label='GRACE', color=C_GMB, **kw)
# ax.fill_between(grace['plot'].index,
#                 (grace['plot'] + grace['plot_err']),
#                 (grace['plot'] - grace['plot_err']),
#                 color=C_GMB, **kw_err)


# VC
<<load_vc>>
vc_sum = vc['SEC'].sum(dim='sector').to_dataframe(name='VC').cumsum()
vc_sum = vc_sum - np.min(vc_sum) - OFFSET_VC
vc_err = vc['err'].sum(dim='sector').to_dataframe(name='VC')[::-1].cumsum()[::-1]
for a in [ax,axins]:
    p_VC = vc_sum.plot(ax=a, label='VC', color=C_VC, **kw)
    (vc_sum - vc_err).plot(ax=a, linestyle='--', color=C_VC, **kw)
    (vc_sum + vc_err).plot(ax=a, linestyle='--', color=C_VC, **kw)




# IMBIE
<<load_imbie>>
imbie = imbie[['mb_cum','mb_cum_err']].dropna()
imbie['mb_cum'] = imbie['mb_cum'] - np.min(imbie['mb_cum'])  - OFFSET_IMBIE
for a in [ax,axins]:
    lw = kw['linewidth']; kw['linewidth']=3
    p_IMBIE = imbie['mb_cum'].plot(ax=a, label='IMBIE', color=C_IMBIE, **kw)
    kw['linewidth']=lw
    a.fill_between(imbie.index,
                   imbie['mb_cum'] - imbie['mb_cum_err'][::-1].values,
                   imbie['mb_cum'] + imbie['mb_cum_err'][::-1].values,
                   color=C_IMBIE, **kw_err)



# PROMICE MB (Colgan 2019)
<<load_PROMICE_MB>>
P = promice['MB'].sum(dim="sector").to_dataframe('C2019').cumsum()
P = P - np.min(P) - OFFSET_C2019
P_err = ((promice['MB_err'].sum(dim="sector")) * 0 + 35).to_dataframe('C2019')[::-1].cumsum()[::-1]
for a in [ax,axins]:
    p_PROMICE = P.plot(ax=a, label='C2019', color=C_PROMICE, **kw)
    kw_err['alpha'] = 0.33
    a.fill_between(P.index.values, (P-P_err).values.flatten(), (P+P_err).values.flatten(), color = C_PROMICE, **kw_err)


<<load_K2015>>
k2015.loc[k2015.index[-1] + datetime.timedelta(days=365)] = k2015.iloc[-1] # replicate last row for plotting
k2015['mb'] = k2015['smb_orig'] - k2015['mmb_orig']
k2015['mb_err'] = (k2015['smb_err']**2 + k2015['mmb_err']**2)**0.5
K = k2015['mb'].cumsum().loc['1971':]
K = K - np.min(K) - OFFSET_K2015
K_err = (k2015['mb_err']*0 + 36)[::-1].cumsum()[::-1].loc['1971':]
for a in [ax,axins]:
    p_k2015 = K.plot(ax=a, label='K2015', color='cyan', drawstyle='steps-post', **kw)
    kw_err['alpha'] = 0.05
    a.fill_between(K.index.values, (K-K_err).values.flatten(), (K+K_err).values.flatten(),
                   color='cyan', **kw_err)

# hl = ax.hlines(0, pd.datetime(1971, 1, 1), pd.datetime(2100,1,1), color='k', linestyle='--')
hl = ax.hlines(0, pd.datetime(1971, 1, 1), pd.datetime(2100,1,1), color='k', alpha=0.25)

ax.xaxis.set_minor_locator(dates.YearLocator(1, month=1, day=1))

axins.set_xlim(pd.datetime(2010, 1, 1), ax.get_xlim()[1])
axins.set_ylim(-6000,-3000)
axins.patch.set_alpha(0)
axins.set_xlabel('')

ax2 = ax.twinx()

adj(ax, ['left','bottom'])
adj(axins, ['left','bottom'])
adj(ax2, ['right','bottom'])


for label in (axins.get_xticklabels() + axins.get_yticklabels()):
    label.set_fontsize(10)


# SLE (mm) = mass of ice (Gt) x (1 / 361.8)
lims = np.array(ax.get_ylim()) * (-1/361.8)
ax2.set_ylim(lims)
ax2.set_ylabel('Cumulative eustatic sea level change [mm]')

ax2.yaxis.set_minor_locator(tck.AutoMinorLocator())


ax.set_ylabel("Cumulative mass change [Gt]")
ax.set_xlabel("Time [Year]")
# ax.set_ylim([0,7000])
# ax.set_yticklabels(['0','-1000','-2000','-3000','-4000','-5000','-6000','-7000'])
# ax.grid(b=True, which='major', axis='y', alpha=0.33)

color = [C_this, C_M2019, C_GMB, C_VC, C_IMBIE, C_PROMICE, 'cyan']
label = ['This Study', 'Mouginot 2019', 'GMB', 'VC', 'IMBIE', 'Colgan 2019', 'Kjeldsen 2015']
lines = [Line2D([0], [0], color=c) for c in color]
legend = plt.legend(lines, label, framealpha=1, loc='upper right', bbox_to_anchor=(0.95,1), ncol=1, facecolor='w', fontsize=10)


# plt.legend()
plt.savefig('fig/mb_cumsum_compare.png', transparent=False, bbox_inches='tight', dpi=300)
plt.savefig('fig/mb_cumsum_compare.svg', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: <ipython-input-87-7653b0ac2241>:408: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.
:   hl = ax.hlines(0, pd.datetime(1971, 1, 1), pd.datetime(2100,1,1), color='k', alpha=0.25)
: <ipython-input-87-7653b0ac2241>:412: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.
:   axins.set_xlim(pd.datetime(2010, 1, 1), ax.get_xlim()[1])


** This vs. Mouginot (2019): GIS xy

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj
import scipy.stats as sps

import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

fig = plt.figure(1, figsize=(3.26,7)) # w,h
fig.clf()
fig.set_tight_layout(True)
axSMB = fig.add_subplot(311)
axD = fig.add_subplot(312)
axMB = fig.add_subplot(313)

kw_adj = {'marker':(4,0,90), 'markersize':7, 'markevery':[-1,1], 'mfc':'none', 'clip_on':False}

# Mouginot SMB, D, and MB (Mouginot, 2019)
<<load_mouginot>>
mouginot = mouginot.where(mouginot['time'].dt.year >= 1987).dropna(dim='time')
mouginot = mouginot.sum(dim='region')

mb = xr.open_dataset('./TMB/mb_region.nc')\
       .resample({'time':'A-JUN'})\
       .sum()

mb = mb.where((mb['time'].dt.year <= 2018) & (mb['time'].dt.year >= 1987)).dropna(dim='time')

years = mb.time.dt.year.values
color = years - min(years); color = color / max(color)
y2str = [_[2:4] for _ in years.astype(str)]

cmap = cm.viridis(color)     # https://stackoverflow.com/questions/51034408n-matplotlib
cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

# graphics
def my_plot(ax, x, xerr, y, yerr, only_dot=False):
    scatter = ax.scatter(x.values, y.values, alpha=0, c=color, cmap=cmap)

    if only_dot == False:
        e = ax.errorbar(x.values, y.values, xerr=xerr, yerr=yerr, fmt=',',
                        color='k', alpha=0.5, linewidth=0.5)
    else:
        for i,s in enumerate(y2str):
            ax.plot([x.values[i], x.values[i]],
                    [y.values[i], y.values[i]],
                    c=cmap(color[i]), **kw_adj)

    for i,s in enumerate(y2str):
        if only_dot == False:
            ax.text(x.values[i], y.values[i], s,
                    c = cmap(color)[i],
                    fontsize=10,
                    fontweight='bold',
                    horizontalalignment='center',
                    verticalalignment='center')

    if only_dot == True: return
     
    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    ax.text(1.0, -0.03, "r$^2$: %.2f\nbias: %d\nRMSE: %d\nslope: %.1f"
            % (round(r_value**2,2), round(bias), round(RMSE), round(slope,1)),
            transform=ax.transAxes,
            horizontalalignment='right',
            fontsize=9)

    return scatter


my_plot(axSMB, mb['smb'], mb['smb_err'], mouginot['SMB'], mouginot['SMB_err'])
my_plot(axD, mb['mmb'], mb['mmb_err'], mouginot['D'], mouginot['D_err'])
my_plot(axD, mb['mmb']+mb['bmb'], (mb['mmb_err']**2+mb['bmb_err']**2)**0.5, mouginot['D'], mouginot['D_err'], only_dot=True)
my_plot(axMB, mb['mb']+mb['bmb'], mb['mb_err'], mouginot['MB'], mouginot['MB_err'], only_dot=True)
s = my_plot(axMB, mb['mb'], mb['mb_err'], mouginot['MB'], mouginot['MB_err'])


axSMB.text(0, 0.9, 'SMB', transform=axSMB.transAxes)
axSMB.set_xticks([0,600])
axSMB.set_ylabel('Mouginot 2019', labelpad=-25)

axD.text(0, 0.9, 'MMB [+BMB]', transform=axD.transAxes)
axD.set_xticks([375,600])

axMB.text(0, 0.9, 'MB$^{*}$ [+BMB]', transform=axMB.transAxes)
axMB.set_xticks([-450, 0, 150])
axMB.set_xlabel('This Study', labelpad=-10)

axMB.set_xticklabels([str(axMB.get_xticks()[0]), '', str(axMB.get_xticks()[-1])])

for ax in [axSMB,axD,axMB]:
    ax.set_yticks(ax.get_xticks())
    ax.set_xlim(ax.get_xticks()[[0,-1]])
    ax.set_ylim(ax.get_xlim())
    ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
    adj(ax, ['left','bottom'])


# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

# axCB = add_sub_axes(axMB, [1.1, 0.0, 0.075, 1])
axCB = add_sub_axes(axMB, [0.0, -0.50, 1, 0.075])
cb = plt.colorbar(s, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
cb.ax.xaxis.set_ticks_position('top')
cb.ax.xaxis.set_label_position('top')
cb.set_label('Time [year]', labelpad=-10)
axCB.set_xticklabels(mouginot['time'].dt.year[[0,-1]].values)

plt.savefig('fig/mouginot_2019.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: <ipython-input-119-b687a3dfe3f7>:156: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/mouginot_2019.png', transparent=False, bbox_inches='tight', dpi=300)


[[./tmp/mouginot_2019.png]]

** This vs. Colgan (2019): GIS xy

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
import scipy as sp
import scipy.stats as sps
from adjust_spines import adjust_spines as adj
import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

# plt.close(1)
fig = plt.figure(1, figsize=(3.26,7)) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
fig.set_tight_layout(True)
# import matplotlib.gridspec as gridspec
# gs = gridspec.GridSpec(1, 1) #w,h
# ax = plt.subplot(gs[:,:])
axSMB = fig.add_subplot(311)
axD = fig.add_subplot(312)
axMB = fig.add_subplot(313)

# PROMICE MB (Colgan 2019)
<<load_PROMICE_MB>>
promice = promice.sum(dim='sector')

mb = xr.open_dataset('./TMB/mb_region.nc')\
       .resample({'time':'YS'})\
       .sum()\
       .reindex(time=promice['time'])
            
kw_adj = {'marker':(4,0,90), 'markersize':7, 'markevery':[-1,1], 'mfc':'none', 'clip_on':False}

years = mb.time.dt.year.values
color = years - min(years); color = color / max(color)
y2str = [_[2:4] for _ in years.astype(str)]

cmap = cm.viridis(color)
cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

# graphics
def my_plot(ax, x, xerr, y, yerr, only_dot=False):

    scatter = ax.scatter(x.values, y.values, alpha=0, c=color, cmap=cmap)

    if only_dot == False:
        e = ax.errorbar(x.values, y.values, xerr=xerr, yerr=yerr, fmt=',', color='k', alpha=0.5, linewidth=0.5)
    else:
        for i,s in enumerate(y2str):
            ax.plot([x.values[i], x.values[i]],
                    [y.values[i], y.values[i]],
                    c=cmap(color[i]), **kw_adj)
    
    for i,s in enumerate(y2str):
        if only_dot == False:
            ax.text(x.values[i], y.values[i], s,
                    c = cmap(color)[i],
                    fontsize=10,
                    fontweight='bold',
                    horizontalalignment='center',
                    verticalalignment='center')

    if only_dot == True: return
    
    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    # np.sqrt(np.mean((predictions-targets)**2))
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    # ax.plot(x, slope * x + intercept, 'k', linewidth=0.5)
    ax.text(1.0, -0.03, "r$^2$: %.2f\nbias: %d\nRMSE: %d\nslope: %.1f"
            % (round(r_value**2,2), round(bias), round(RMSE), round(slope,1)),
            transform=ax.transAxes,
            horizontalalignment='right',
            fontsize=9)

    return scatter


my_plot(axSMB, mb['smb'], mb['smb_err'], promice['SMB'], promice['SMB_err'])
my_plot(axD,   mb['mmb'], mb['mmb_err'], promice['D'], promice['D_err'])
my_plot(axD,   mb['mmb']+mb['bmb'], (mb['mmb_err']**2+mb['bmb_err']**2)**0.5, promice['D'], promice['D_err'], only_dot=True)
s = my_plot(axMB,  mb['mb'], mb['mb_err'], promice['MB'], promice['MB_err'])
my_plot(axMB,  mb['mb']+mb['bmb'], mb['mb_err'], promice['MB'], promice['MB_err'], only_dot=True)

axSMB.text(0, 0.9, 'SMB', transform=axSMB.transAxes)
axSMB.set_xticks([0,600])
axSMB.set_ylabel('Colgan 2019', labelpad=-25)
axD.text(0, 0.9, 'MMB [+BMB]', transform=axD.transAxes)
axD.set_xticks([300,600])
axMB.text(0, 0.9, 'MB$^{*}$ [+BMB]', transform=axMB.transAxes)
axMB.set_xticks([-450, 0, 150])
axMB.set_xlabel('This Study', labelpad=-10)

axMB.set_xticklabels([str(axMB.get_xticks()[0]), '', str(axMB.get_xticks()[-1])])

for ax in [axSMB,axD,axMB]:
    ax.set_yticks(ax.get_xticks())
    ax.set_xlim(ax.get_xticks()[[0,-1]])
    ax.set_ylim(ax.get_xlim())
    ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
    adj(ax, ['left','bottom'])

# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

# axCB = add_sub_axes(axMB, [1.1, 0.0, 0.075, 1])
axCB = add_sub_axes(axMB, [0.0, -0.50, 1, 0.075])
cb = plt.colorbar(s, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
cb.ax.xaxis.set_ticks_position('top')
cb.ax.xaxis.set_label_position('top')
cb.set_label('Time [year]', labelpad=-10)
axCB.set_xticklabels(promice['time'].dt.year[[0,-1]].values)

plt.savefig('fig/colgan_2019.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: <ipython-input-35-170754f9ea16>:165: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/colgan_2019.png', transparent=False, bbox_inches='tight', dpi=300)


** This vs. IMBIE/GRACE/VC GIS XY

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
import scipy as sp
import scipy.stats as sps
from adjust_spines import adjust_spines as adj
import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

# ONE COLUMN: %\includegraphics[width=8.3cm]{FILE NAME}

fig = plt.figure(1, figsize=(3.26, 7)) # w,h
fig.clf()
fig.set_tight_layout(True)
axGRACE = fig.add_subplot(311)
axVC = fig.add_subplot(312)
axIMBIE = fig.add_subplot(313)

# graphics
def my_plot(ax, x, xerr, y, yerr):

    if type(x) == xr.core.dataarray.DataArray:
        years = x.time.dt.year.values
    else:
        years = x.index.year.values
        
    print(years)
    color = years - min(years); color = color / max(color)
    y2str = [_[2:4] for _ in years.astype(str)]

    # https://stackoverflow.com/questions/51034408/
    cmap = cm.viridis(color)
    cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

    e = ax.errorbar(x.values, y.values, xerr=xerr, yerr=yerr, fmt=',', color='k', alpha=0.5, linewidth=0.5)
    scatter = ax.scatter(x.values, y.values, alpha=0, c=color, cmap=cmap)
    for i,s in enumerate(y2str):
        ax.text(x.values[i], y.values[i], s,
                c = cmap(color)[i],
                fontsize=10,
                fontweight='bold',
                horizontalalignment='center',
                verticalalignment='center')

    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    ax.text(1.0, -0.03,
            "r$^2$: %.2f\nbias: %d\nRMSE: %d\nslope: %.1f"
            % (round(r_value**2,2), round(bias), round(RMSE), round(slope,1)),
            transform=ax.transAxes,
            horizontalalignment='right',
            fontsize=9)


    axCB = add_sub_axes(ax, [0.05, 0.95, 0.3, 0.05])
    cb = plt.colorbar(scatter, cax=axCB, orientation='horizontal', ticks=[0,1])
    cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
    cb.draw_all()
    axCB.set_xticklabels(years[[0,-1]])
    for tick in axCB.xaxis.get_major_ticks():
        tick.label.set_fontsize(9) 

    return scatter

# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])





# this
this_mb = xr.open_dataset('./TMB/mb_sector.nc')
mb = this_mb[['mb','mb_err']].to_dataframe()
mb = mb.resample('MS').sum()

<<load_grace>>
grace = grace.resample('MS').mean().interpolate(method='time').diff()
grace[grace.index.year == 2017] = np.nan
grace = grace.dropna()
grace = grace.iloc[1:-1]
# this_mb_grace = this_mb.sel({'time':grace.index})
this_mb_grace = mb.merge(grace, left_index=True, right_index=True)
this_mb_grace = this_mb_grace.resample('YS').sum()

s = my_plot(axGRACE, this_mb_grace['mb'], this_mb_grace['mb_err'], this_mb_grace['mass'], this_mb_grace['err'])


# this
this_mb = xr.open_dataset('./TMB/mb_sector.nc')\
            .resample({'time':'YS'})\
            .sum()[['mb','mb_err']]
    

<<load_VC>>
vc = vc.sum(dim='sector')\
       .resample({'time':'YS'})\
       .sum()
this_mb_vc = this_mb.sel({'time':slice(vc.time[0], vc.time[-1])})    
    
<<load_imbie>>
# imbie = imbie.resample('1D').mean().interpolate(method='time')
imbie = imbie.resample('YS').mean()
imbie = imbie[imbie.index.year > 1986]
imbie = imbie.dropna()
this_mb_imbie = this_mb.sel({'time':slice(imbie.index[0], imbie.index[-1])})

s = my_plot(axVC, this_mb_vc['mb'], this_mb_vc['mb_err'], vc['SEC'], vc['err'])
s = my_plot(axIMBIE,  this_mb_imbie['mb'], this_mb_imbie['mb_err'], imbie['mb'], imbie['mb_err'])

for ax in [axGRACE, axVC, axIMBIE]:
    ax.set_xticks([-550, 0, 200])
    ax.set_yticks(ax.get_xticks())
    ax.set_xlim(ax.get_xticks()[[0,-1]])
    ax.set_ylim(ax.get_xlim())
    if ax != axGRACE: ax.set_yticklabels(['','',''])
    if ax != axIMBIE: ax.set_xticklabels(['','',''])
    ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
    adj(ax, ['left','bottom'])


axGRACE.set_ylabel('GMB', labelpad=-25)
axVC.set_ylabel('VC')
axIMBIE.set_ylabel('IMBIE')
axIMBIE.set_xlabel('This Study', labelpad=-10)

axIMBIE.set_xticklabels([str(axIMBIE.get_xticks()[0]), '', str(axIMBIE.get_xticks()[-1])])
    
plt.savefig('fig/this_v_grace_vc_imbie.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: [2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015
:  2016 2017 2018 2019 2020]
: [1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005
:  2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019]
: [1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005
:  2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018]
: <ipython-input-37-1f91dffbff96>:207: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/this_v_grace_vc_imbie.png', transparent=False, bbox_inches='tight', dpi=300)

[[./tmp/this_v_grace_vc_imbie.png]]


** HIRHAM MAR RACMO timeseries
#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj

import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

# plt.close(1)
fig = plt.figure(1, figsize=(8,8)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(311)

C = 'k'; L='-'
C_HIRHAM = 'gray'; L_HIRHAM='--'
C_MAR = 'blue'; L_MAR = ':'
C_RACMO = 'red'; L_RACMO = '-'

# MB (this)
mb = xr.open_dataset("./TMB/mb_sector.nc")
mb = mb.sel({'time':slice('1986-01-01','2020-12-31')})
mb = mb.sum(dim='sector')

kw = {'ax':ax, 'legend':False, 'drawstyle':'steps-post'}

mb_ann = mb.to_dataframe()\
           .resample('AS')\
           .sum()\
           .rename(columns={'mb':'This Study',
                            'mb_HIRHAM':'HIRHAM',
                            'mb_MAR':'MAR',
                            'mb_RACMO':'RACMO'})
mb_ann.loc[pd.to_datetime('2021-01-01', format='%Y-%m-%d')] = mb_ann.iloc[-1]
mb_ann['This Study'].plot(color=C, linestyle=L, **kw)
mb_ann['HIRHAM'].plot(color=C_HIRHAM, linestyle=L_HIRHAM, **kw)
mb_ann['MAR'].plot(color=C_MAR, linestyle=L_MAR, **kw)
mb_ann['RACMO'].plot(color=C_RACMO, linestyle=L_RACMO, **kw)

# ax.fill_between(mb_ann.index, mb_ann.values.flatten(), color='k', alpha=0.1, step='post')

# plt.legend(loc='lower left', ncol=2)
plt.legend(loc='lower left', ncol=2, framealpha=0)

ax2 = fig.add_subplot(312)
mb = mb.sel({'time':slice('2019-01-01','2020-12-31')})
kw = {'ax': ax2, 'drawstyle':'steps-post'}

mb['mb'].plot(color=C, linestyle=L, **kw)
mb['mb_HIRHAM'].plot(color=C_HIRHAM, linestyle=L_HIRHAM, **kw)
mb['mb_MAR'].plot(color=C_MAR, linestyle=L_MAR, **kw)
mb['mb_RACMO'].plot(color=C_RACMO, linestyle=L_RACMO, **kw)


ax3 = fig.add_subplot(313)
kw = {'ax': ax3, 'drawstyle':'steps-post'}

(mb['mb_HIRHAM']-mb['mb']).plot(color=C_HIRHAM, linestyle=L_HIRHAM, **kw)
(mb['mb_MAR']-mb['mb']).plot(color=C_MAR, linestyle=L_MAR, **kw)
(mb['mb_RACMO']-mb['mb']).plot(color=C_RACMO, linestyle=L_RACMO, **kw)

ax.set_ylabel("Mass gain [Gt yr$^{-1}$]")
ax.set_xlabel("")
adj(ax, ['left','bottom'])

ax2.set_xlabel("Time [Year]")
ax2.set_ylabel("Mass gain [Gt d$^{-1}$]")
adj(ax2, ['left','bottom'])

ax3.set_xlabel("Time [Year]")
ax3.set_ylabel("Difference [Gt d$^{-1}$]")
adj(ax3, ['left','bottom'])

plt.savefig('fig/mb_3RCM.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:

** This vs. Mouginot (2019): regions xy

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from matplotlib import rc
from adjust_spines import adjust_spines as adj
import scipy.stats as sps

rc('font', size=9)
rc('text', usetex=False)

fig = plt.figure(1, figsize=((12/2.54),(23/2.54))) # w,h
fig.clf()
fig.set_tight_layout(True)
gs = gridspec.GridSpec(ncols=3, nrows=7, figure=fig) #w,h
gs.update(wspace=1, hspace=1)

<<load_mouginot>>

# this
ds = xr.open_dataset('./TMB/mb_region.nc')\
       .resample({'time':'A-JUN'})\
       .sum()

mb = ds['mb_ROI']
smb = ds['smb_ROI']
D = ds['mmb_ROI'] # + ds['bmb_ROI']

mouginot = mouginot.where(mouginot['time'].dt.year >= 1987).dropna(dim='time')
smb = smb.where((smb['time'].dt.year <= 2018) & (smb['time'].dt.year >= 1987)).dropna(dim='time')
D = D.where((D['time'].dt.year <= 2018) & (D['time'].dt.year >= 1987)).dropna(dim='time')
mb = mb.where((mb['time'].dt.year <= 2018) & (mb['time'].dt.year >= 1987)).dropna(dim='time')

region = mb['region'].values

<<round_axes>>

def my_plot(ax, x, y):
    years = x.time.dt.year.values
    color = years - min(years); color = color / max(color)
    y2str = [_[2:4] for _ in years.astype(str)]

    # https://stackoverflow.com/questions/51034408/how-to-make-the-color-of-one-end-of-colorbar-darker-in-matplotlib
    cmap = cm.viridis(color)
    cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

    scatter = ax.scatter(x.values, y.values, alpha=0.75, c=color, cmap=cmap, clip_on=False, s=5)
        
    if x.region.values == 'NW':
        if ax == axSMB: ax.set_title('SMB')
        if ax == axD: ax.set_title('MMB')
        if ax == axMB: ax.set_title('MB$^{*}$')

    if (ax == axSMB):
        ax.set_ylabel(x.region.values)
        if (x.region.values == 'SE'):
            ax.set_ylabel('M2019\nSE', labelpad=-10)
            ax.set_xlabel('This Study')

    if ax == axSMB:
        ax.set_ylim([-90,220])
        ax.set_xlim([-90,220])
    if ax == axD:
        ax.set_ylim([20,180])
        ax.set_xlim([20,180])
    if ax == axMB:
        ax.set_ylim([-120,80])
        ax.set_xlim([-120,80])

    ax.set_xticks(round_axes(ax.get_xlim(), ax.get_ylim()))
    ax.set_yticks(ax.get_xlim())
        
    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    if ax==axD and x.region.values == 'SE':
        s = "%.2f r$^{2}$\n%d bias\n%d RMSE"
    else:
        s = "%.2f\n%d\n%d"
    ax.text(0, 1, s % (round(r_value**2,2), round(bias), round(RMSE)),
            transform=ax.transAxes,
            horizontalalignment='left',
            verticalalignment='top',
            fontsize=8)

    ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
    adj(ax, ['left','bottom'])
    return scatter

# graphics
for i,region in enumerate(['NW','NO','NE','CW','CE','SW','SE']):
    axSMB = fig.add_subplot(gs[i,0])
    axD = fig.add_subplot(gs[i,1])
    axMB = fig.add_subplot(gs[i,2])

    my_plot(axSMB, smb.sel({'region':region}), mouginot['SMB'].sel({'region':region}))
    my_plot(axD,   D.sel({'region':region}), mouginot['D'].sel({'region':region}))
    s = my_plot(axMB,  mb.sel({'region':region}), mouginot['MB'].sel({'region':region}))


# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

axCB = add_sub_axes(axD, [0, -0.85, 1, 0.1])
cb = plt.colorbar(s, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
axCB.set_xticklabels(mb['time'].dt.year[[0,-1]].values)
    
# plt.legend()
plt.savefig('fig/mouginot_2019_regions.png', transparent=False, bbox_inches='tight', dpi=150)
#+END_SRC

#+RESULTS:
: <ipython-input-41-321251559297>:162: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/mouginot_2019_regions.png', transparent=False, bbox_inches='tight', dpi=150)

[[./tmp/colgan_2019_sectors.png]]

** This vs. Colgan (2019): Sectors xy

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
import matplotlib.gridspec as gridspec
import matplotlib.pyplot as plt
from matplotlib import rc
from adjust_spines import adjust_spines as adj
import scipy as sp
import scipy.stats as sps

rc('font', size=9)
rc('text', usetex=False)

# plt.close(1)
fig = plt.figure(1, figsize=((12/2.54),(22/2.54))) # w,h
# get_current_fig_manager().window.move(0,0)
fig.clf()
fig.set_tight_layout(True)
gs = gridspec.GridSpec(ncols=3, nrows=8, figure=fig) #w,h
gs.update(wspace=1, hspace=1)

# PROMICE MB (Colgan 2019)
<<load_PROMICE_MB>>

# this
ds = xr.open_dataset('./TMB/mb_sector.nc')\
       .resample({'time':'YS'})\
       .sum()\
       .reindex(time=promice['time'])

mb = ds['mb_ROI']
smb = ds['smb_ROI']
D = ds['mmb_ROI'] # + ds['bmb_ROI']

# collapse all Wally sectors to just super-sectors
promice['Z'] = (('sector'), (promice.sector.values/10).astype(int))
promice = promice.groupby('Z').sum().rename({'Z':'sector'})
mb['Z'] = (('sector'), (mb.sector.values/10).astype(int))
mb = mb.groupby('Z').sum().rename({'Z':'sector'})
smb['Z'] = (('sector'), (smb.sector.values/10).astype(int))
smb = smb.groupby('Z').sum().rename({'Z':'sector'})
D['Z'] = (('sector'), (D.sector.values/10).astype(int))
D = D.groupby('Z').sum().rename({'Z':'sector'})


sector = mb['sector']

<<round_axes>>

def my_plot(ax, x, y):
    years = x.time.dt.year.values
    color = years - min(years); color = color / max(color)
    y2str = [_[2:4] for _ in years.astype(str)]

    # https://stackoverflow.com/questions/51034408/how-to-make-the-color-of-one-end-of-colorbar-darker-in-matplotlib
    cmap = cm.viridis(color)
    cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

    scatter = ax.scatter(x.values, y.values, alpha=0.75, c=color, cmap=cmap, clip_on=False, s=5)
        
    if x.sector.values == 1:
        if ax == axSMB: ax.set_title('SMB')
        if ax == axD: ax.set_title('MMB')
        if ax == axMB: ax.set_title('MB$^{*}$')        

    if (ax == axSMB):
        ax.set_ylabel(x.sector.values)
        if (x.sector.values == 8):
            ax.set_ylabel('C2019\n8', labelpad=-10)
            ax.set_xlabel('This Study')


    if ax == axSMB:
        ax.set_ylim([-90,190])
        ax.set_xlim([-90,190])
    if ax == axD:
        ax.set_ylim([0,160])
        ax.set_xlim([0,160])
    if ax == axMB:
        ax.set_ylim([-110,60])
        ax.set_xlim([-110,60])

    ax.set_xticks(round_axes(ax.get_xlim(), ax.get_ylim()))
    ax.set_yticks(ax.get_xlim())
    
    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    if ax == axSMB and x.sector.values == 1:
        s = "%.2f r$^{2}$\n%.1f bias\n%.1f RMSE"
    else:
        s = "%.2f\n%.1f\n%.1f"
    ax.text(0, 1, s % (round(r_value**2,2), round(bias), round(RMSE)),
            transform=ax.transAxes,
            horizontalalignment='left',
            verticalalignment='top',
            fontsize=8)

    ax.plot(ax.get_xlim(), ax.get_ylim(), color='k', alpha=0.25, linestyle='--')
    adj(ax, ['left','bottom'])
    return scatter


# graphics
for row in np.arange(8):
    axSMB = fig.add_subplot(gs[row,0])
    axD = fig.add_subplot(gs[row,1])
    axMB = fig.add_subplot(gs[row,2])

    my_plot(axSMB,
            smb.sel({'sector':sector[row]}),
            promice['SMB'].sel({'sector':sector[row]}))
    

    if (sector[row] != 14) & (sector[row] != 22):
        my_plot(axD,
                D.sel({'sector':sector[row]}),
                promice['D'].sel({'sector':sector[row]}))

    s = my_plot(axMB,
                mb.sel({'sector':sector[row]}),
                promice['MB'].sel({'sector':sector[row]}))

# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

axCB = add_sub_axes(axD, [0, -1, 1, 0.1])
cb = plt.colorbar(s, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
axCB.set_xticklabels(promice['time'].dt.year[[0,-1]].values)
    
# plt.legend()
plt.savefig('fig/colgan_2019_sectors.png', transparent=False, bbox_inches='tight', dpi=150)
#+END_SRC

#+RESULTS:
: <ipython-input-42-9dfb1a196fdc>:192: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/colgan_2019_sectors.png', transparent=False, bbox_inches='tight', dpi=150)

[[./tmp/colgan_2019_sectors.png]]

** Zwally and Mouginot RCM coverage
*** HIRHAM

#+BEGIN_SRC bash
grass -c ./G_HIRHAM/coverage

g.list type=raster mapset=* -m

g.region region=sectors

r.mapcalc "RCM = if(smb@PERMANENT) * 0 + 1"
r.mapcalc "regions = if(regions@Mouginot_2019) * 0 + 2"
r.mapcalc "regions_e = if(regions_e@Mouginot_2019) * 0 + 4"

r.null map=RCM null=0
r.null map=regions null=0
r.null map=regions_e null=0

r.mapcalc "coverage_M = RCM + regions + regions_e"
r.null map=coverage_M setnull=0

r.stats coverage_M

r.category map=coverage_M separator=":" rules=- << EOF
1:RCM (peripheral)
2:Mouginot (2019) and no RCM
3:RCM and Mouginot (2019) (peripheral; excluded)
5:RCM and Mouginot (2019) expanded (included)
7:RCM and Mouginot (2019) (included)
EOF

# | light blue  | 166 | 206 | 227 | #a6cee3 |
# | dark blue   |  31 | 120 | 180 | #1f78b4 |
# | light green | 178 | 223 | 138 | #b2df8a |
# | dark green  |  51 | 160 |  44 | #33a02c |
# | pink        | 251 | 154 | 153 | #fb9a99 |
# | red         | 227 |  26 |  28 | #e31a1c |
# | pale orange | 253 | 191 | 111 | #fdbf6f |
# | orange      | 255 | 127 |   0 | #ff7f00 |

cat << EOF | r.colors map=coverage_M rules=-
1 192:192:192
2 253:191:111
3 255:0:0
5 172:223:138
7 166:206:227
EOF

rm -f fig/coverage_hirham.png
d.mon start=png output=fig/coverage_hirham.png
d.rast coverage_M
d.legend -c -n raster=coverage_M at=5,15,40,30 bgcolor=white
d.grid 3
d.mon stop=png

r.out.gdal input=coverage_M output=./tmp/H_cover_M.tif
# display in QGIS
# export in QGIS
# edit in Inkscape
#+END_SRC

** Reconstructed runoff -> VHD

See [[id:20210406T102219.348249][Load and adjust Reconstructed K2015]]

#+BEGIN_SRC jupyter-python
import xarray as xr
import numpy as np
import pandas as pd
from adjust_spines import adjust_spines as adj
import scipy.stats as sps

import matplotlib.pyplot as plt
from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)

fig = plt.figure(1, figsize=(3.26,3.2)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(111)

kw_adj = {'marker':(4,0,90), 'markersize':7, 'markevery':[-1,1], 'mfc':'none', 'clip_on':False}

<<load_and_adjust_K2015>>
# provides vr w/     VHD_sector  VHD_sector_err  runoff    

x = vr['runoff']
xerr = x * 0.15
y = vr['VHD_sector']
yerr = vr['VHD_sector_err']

years = x.index.year.values
color = years - min(years); color = color / max(color)
y2str = [_[2:4] for _ in years.astype(str)]

cmap = cm.viridis(color)     # https://stackoverflow.com/questions/51034408n-matplotlib
cmap = mpl.colors.ListedColormap(cmap[:-3,:-1])

scatter = ax.scatter(x.values, y.values, alpha=0, c=color, cmap=cmap)
e = ax.errorbar(x.values, y.values, xerr=xerr, yerr=yerr, fmt=',',
                color='k', alpha=0.5, linewidth=0.5)

for i,s in enumerate(y2str):
    ax.text(x.values[i], y.values[i], s,
            c = cmap(color)[i],
            fontsize=10,
            fontweight='bold',
            horizontalalignment='center',
            verticalalignment='center')

    slope, intercept, r_value, p_value, std_err = sps.linregress(x.values, y.values)
    bias = np.mean(x.values - y.values)
    RMSE = np.sqrt(np.mean((x.values - y.values)**2))
    ax.text(1.0, 0.0, "r$^2$: %.2f\nslope: %.2f"
            % (round(r_value**2,2), round(slope,2)),
            transform=ax.transAxes,
            horizontalalignment='right',
            fontsize=9)

ax.set_xlabel('Runoff', labelpad=-10)
ax.set_ylabel('BMB$_{\mathrm{VHD}}$', labelpad=-15)

ax.set_xticks(ax.get_xticks()[[0,-1]])
ax.set_xlim(ax.get_xticks()[[0,-1]])
ax.set_yticks(ax.get_yticks()[[0,-1]])
ax.set_ylim(ax.get_yticks()[[0,-1]])
adj(ax, ['left','bottom'])


# https://stackoverflow.com/questions/17478165/
def axis_to_fig(axis):
    fig = axis.figure
    def transform(coord):
        return fig.transFigure.inverted().transform(
            axis.transAxes.transform(coord))
    return transform

def add_sub_axes(axis, rect):
    fig = axis.figure
    left, bottom, width, height = rect
    trans = axis_to_fig(axis)
    figleft, figbottom = trans((left, bottom))
    figwidth, figheight = trans([width,height]) - trans([0,0])
    return fig.add_axes([figleft, figbottom, figwidth, figheight])

# axCB = add_sub_axes(axMB, [1.1, 0.0, 0.075, 1])
axCB = add_sub_axes(ax, [0.0, -0.40, 1, 0.075])
cb = plt.colorbar(scatter, cax=axCB, orientation='horizontal', ticks=[0,1])
cb.set_alpha(1) # https://stackoverflow.com/questions/4478725/
cb.draw_all()
cb.ax.xaxis.set_ticks_position('top')
cb.ax.xaxis.set_label_position('top')
cb.set_label('Time [year]', labelpad=-10)
axCB.set_xticklabels(x.index.year[[0,-1]].values)

plt.savefig('fig/reconstructed_runoff.png', transparent=False, bbox_inches='tight', dpi=300)
#+END_SRC

#+RESULTS:
: slope:  0.02779523106638524
: intercept:  -3.204674758436137
: r^2:  0.7503894036942795
: p:  5.267031413783773e-09
: std_err : 0.003206184032882044
: <ipython-input-20-798d3ad1530e>:261: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.
:   plt.savefig('fig/reconstructed_runoff.png', transparent=False, bbox_inches='tight', dpi=300)

** CRediT


#+NAME: tbl_credit
| Task                   | KDM | XF | PL | MS | KK | NBK | BN | MvdB | AS | WC | JB | SS | MDK | APA | SBA | RSF |
| Conceptualization      |   2 |    |    |    |    |     |    |      |    |    |    |    |     |   2 |     |   2 |
| Data curation          |   3 |  3 |  3 |  3 |  3 |   3 |  3 |    3 |  3 |    |  2 |    |     |     |     |     |
| Implementation         |   3 |  2 |  2 |    |  1 |     |    |      |    |    |    |    |   1 |     |     |     |
| Funding                |     |    |    |    |    |     |    |      |  3 |    |    |    |     |   3 |   2 |   3 |
| Methods: SMB           |     |  3 |  3 |    |    |     |  3 |    1 |    |    |    |    |     |     |     |     |
| Methods: MMB           |   3 |    |    |    |    |     |    |      |  1 |  1 |    |    |     |   1 |     |   1 |
| Methods: MMB forecast  |   2 |    |    |    |    |     |    |      |    |    |    |    |   2 |     |     |     |
| Methods: BMB           |   1 |    |    |    |    |   3 |    |      |    |    |    |    |     |     |     |     |
| Methods: Reconstructed |     |    |    |    |  2 |     |    |      |    |    |  2 |    |     |     |     |     |
| Validation             |   3 |    |    |    |    |     |    |      |    |    |    |    |     |     |     |     |
| Validation: GRACE      |     |    |    |    |    |     |    |      |    |  3 |    |    |     |     |     |     |
| Validation: VC         |     |    |    |    |    |     |    |      |    |  1 |    |  3 |     |     |     |     |
| Project administration |   3 |    |    |    |    |     |    |      |    |    |    |    |     |   1 |   1 |   1 |
| Resources              |   3 |  3 |  3 |  3 |  3 |   3 |  3 |    3 |  3 |    |    |    |     |     |   2 |     |
| Software               |   3 |  3 |  3 |    |    |     |    |      |  3 |    |    |    |   2 |     |     |     |
| Visualization          |   3 |    |    |    |    |     |    |      |    |    |    |    |     |     |     |     |
| Writing (First draft)  |   3 |  3 |  3 |  3 |  3 |   3 |  3 |    3 |    |  3 |  3 |  3 |     |   1 |     |   1 |
| Writing (Editing)      |   2 |  2 |  2 |  2 |  2 |   2 |  2 |    2 |    |  2 |  2 |  2 |   2 |     |     |   2 |
|------------------------+-----+----+----+----+----+-----+----+------+----+----+----+----+-----+-----+-----+-----|
| #ERROR                 |  34 | 19 | 19 | 11 | 14 |  14 | 14 |   12 | 13 | 10 |  9 |  8 |   5 |   8 |   5 |  10 |
#+TBLFM: @>=vsum(@2..@-1)

#+BEGIN_SRC python :var tbl=tbl_credit :session :exports none
import pandas as pd
import numpy as np
df = pd.DataFrame(index = [_[0] for _ in tbl[1:]],
                  columns = tbl[0][1:],
                  data = np.array(tbl)[1:,1:])
df = df.apply(pd.to_numeric)
df.drop(index='#ERROR', inplace=True)
df.to_csv('./tmp/credit.csv')
#+END_SRC

#+RESULTS:
: None


#+BEGIN_SRC jupyter-python :session credit
import seaborn as sns
import numpy as np
import pandas as pd
from matplotlib.colors import LinearSegmentedColormap
from matplotlib import rc
rc('font', size=10)
rc('text', usetex=False)

cmap = sns.color_palette("Blues", 3)

fig = plt.figure(1, figsize=(5,4)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(111)

df = pd.read_csv('./tmp/credit.csv', index_col=0)
df = df.replace(1,3).replace(2,3)

fig = plt.figure(1, figsize=(8,6)) # w,h
fig.clf()
fig.set_tight_layout(True)
ax = fig.add_subplot(111)

sns.set()

im = sns.heatmap(df, vmin=1, vmax=3, cmap=cmap, fmt='d', ax=ax, xticklabels=True, cbar=False)
ax.xaxis.set_ticks_position('top')

plt.xticks(rotation=-90)

# # Manually specify colorbar labelling after it's been generated
# colorbar = ax.collections[0].colorbar
# colorbar.set_ticks([1.33, 2, 2.66])
# colorbar.set_ticklabels(['1', '2', '3'])
# colorbar.set_label('Contribution')

ax.set_ylabel('What')
ax.set_xlabel('Who')

ax.xaxis.set_label_position('top')

plt.savefig('./fig/credit.png', transparent=False, bbox_inches='tight', dpi=150)
#+END_SRC

#+RESULTS:

[[./fig/credit.png]]

* Helper functions
** Imports

#+NAME: py_import
#+BEGIN_SRC jupyter-python
import numpy as np
import xarray as xr
import datetime
import pandas as pd
#+END_SRC

** Adjust Spines

#+BEGIN_SRC jupyter-python :tangle adjust_spines.py
# http://matplotlib.org/examples/pylab_examples/spine_placement_demo.html
def adjust_spines(ax,spines, offset=10):
    for loc, spine in ax.spines.items():
        if loc in spines:
            spine.set_position(('outward', offset)) # outward
            # by 10 points
            #spine.set_smart_bounds(True)
        else:
            spine.set_color('none') # don't
            # draw spine

        # turn off ticks where there
        # is no spine
        if 'left' in spines:
            ax.yaxis.set_tick_params(length=5)
            ax.yaxis.set_tick_params(direction='in', which='major')
            ax.yaxis.set_tick_params(direction='in', which='minor')
            ax.yaxis.set_ticks_position('left')
            ax.yaxis.set_label_position('left')
        elif 'right' in spines:
            ax.yaxis.set_tick_params(length=5)
            ax.yaxis.set_tick_params(direction='in', which='major')
            ax.yaxis.set_tick_params(direction='in', which='minor')
            ax.yaxis.set_ticks_position('right')
            ax.yaxis.set_label_position('right')
        else:
            # no yaxis ticks
            ax.yaxis.set_ticks([])

        if 'bottom' in spines:
            ax.xaxis.set_ticks_position('bottom')
            ax.xaxis.set_tick_params(length=5)
            ax.xaxis.set_tick_params(direction='in', which='major')
            ax.xaxis.set_tick_params(direction='in', which='minor')
            ax.xaxis.set_label_position('bottom')
        elif 'top' in spines:
            ax.xaxis.set_ticks_position('top')
            ax.xaxis.set_tick_params(length=5)
            ax.xaxis.set_tick_params(direction='in', which='major')
            ax.xaxis.set_tick_params(direction='in', which='minor')
            ax.xaxis.set_label_position('top')
        else:
            # no xaxis
            # ticks
            ax.xaxis.set_ticks([])


if __name__ == "__main__":
    import numpy as np            
    x = np.random.random(100)
    fig = plt.figure(100)
    fig.clf()
    ax = fig.add_axes([0.1,0.1,0.8,0.8])
    ax.plot(x)
    adjust_spines(ax,["left","bottom"])

#+END_SRC

** round axes
#+NAME: round_axes
#+BEGIN_SRC jupyter-python
def round_axes(x, y=None):
    x = np.append(x,y) if y is not None else np.array(x)
    # print(x)
    mmin = np.min(x)
    mmax = np.max(x)
    sign = np.sign([mmin,mmax])
    # mmin = 10**np.floor(np.log10(sign[0]*mmin*10)) * sign[0]
    # mmax = 10**np.ceil(np.log10(sign[1]*mmax/10)) * sign[1]
    mmin = np.floor(mmin/10)*10
    mmax = np.ceil(mmax/10)*10
    return(mmin,mmax)
#+END_SRC

** cron jobs

Then "make update"

*** HIRHAM

manual:
#+BEGIN_SRC bash
rsync -ahiu --progress glacio.clean:data/HIRHAM/daily/ ~/data/HIRHAM/daily/
#+END_SRC


Run with =0 5 * * * ~/bin/HIRHAM_daily.sh=

# #+HEADER: :tangle ~/bin/HIRHAM_daily.sh
#+BEGIN_SRC bash 
#!/bin/bash

cd ~/data/HIRHAM/daily/
# wget -a ~/data/HIRHAM/wget.log --user MB --password <PW> -np -nc ftp.dmi.dk:KDM_SMB_files/*
rm -f ../wget.log
wget -a ../wget.log --user MB --password <PW> -np -nc ftp.dmi.dk:KDM_SMB_files/*
#+END_SRC

*** MAR

Run with =0 5 * * * ~/bin/MAR_daily.sh=

# #+HEADER: :tangle ~/bin/MAR_daily.sh
#+BEGIN_SRC bash 
#!/bin/bash

mkdir -p ~/data/MAR/3.12

mkdir -p ~/data/MAR/daily
cd ~/data/MAR/daily

# TODO: This script fails starting in 2030.
Y1=$(date +%Y)
Y0=$(( $Y1 - 1 ))
M=$(date +%m)

rm -f wget.log
if [[ $M == '01' ]]; then wget -a wget.log -np -nc ftp://ftp.climato.be/fettweis/tmp/ken/MAR-${Y0}*.nc; fi
wget -a wget.log -np -nc ftp://ftp.climato.be/fettweis/tmp/ken/MAR-${Y1}*.nc
mv MAR-????.nc ../3.12/
#+END_SRC


** Upload to DV

Goal: Upload and *replace* files for the ice discharge dataset. Requires downloading existing metadata to get the current DOI, then uploading to that DOI (after which a new one gets issued by the dataverse)

*** Install pyDataverse

+ Email: [[mu4e:msgid:1c48e4d2-decc-4747-874a-4f22e824b8bco@googlegroups.com][Re: [Dataverse-Users] Re: Updating a dataset but keeping filenames.]]
  + Public: https://groups.google.com/g/dataverse-community/c/ZN6ekXaCTo0

+ Install pyDataverse dev edition:
#+BEGIN_SRC bash :results verbatim :tangle no
pip install git+https://github.com/aussda/pyDataverse
#+END_SRC

No - doesn't seem to grab latest version. Let's install a specific Git hash from yesterday (https://adamj.eu/tech/2019/03/11/pip-install-from-a-git-repository/)

#+BEGIN_SRC bash :results verbatim :tangle no
python -m pip install --upgrade git+https://github.com/aussda/pyDataverse.git@3b040ff23b665ec2650bebcf4bd5478de6881af0
#+END_SRC

+ Docs: https://github.com/AUSSDA/pyDataverse/blob/develop/src/pyDataverse/api.py#L1805

*** Upload script

#+BEGIN_SRC jupyter-python :session TMB_DV :tangle upload_to_DV.py
from pyDataverse.api import NativeApi
import os
import json

import subprocess
hash = subprocess.check_output(["git", "describe", "--always"]).strip().decode('utf-8')
# hash = subprocess.check_output(["git", "describe", "--always", "--dirty='*'"]).strip().decode('utf-8')
assert("*" not in hash)

# establish connection

base_url = 'https://dataverse01.geus.dk/'
api_token = 'nnnnnn-nnnnnn-nnnnnn-nn-nn-nnnnnn'
api = NativeApi(base_url, api_token)

# get dataverse metadata
identifier = 'doi:10.22008/FK2/OHI23Z'
resp = api.get_dataset(identifier)
files = resp.json()['data']['latestVersion']['files']
for f in files:
    persistentId = f['dataFile']['persistentId']
    description = f['dataFile']['description']
    filename = f['dataFile']['filename']
    fileId = f['dataFile']['id']

    assert(os.path.isfile("./TMB/"+filename))

    description = description.split(".")[0] + ". "
    description = description + "Git hash: " + hash
    
    if 'content' in locals(): del(content)
    if filename[-3:] == ".nc": content = "application/x-netcdf"
    if filename[-3:] == "csv": content = "text/csv"
    if filename[-3:] == "txt": content = "text/plain"
    json_dict={"description":description, 
               "directoryLabel":".", 
               "forceReplace":True, 
               "filename":filename, 
               "label":filename, 
               "contentType":content}

    json_str = json.dumps(json_dict)
    # print(f)

    ## replace
    d = api.replace_datafile(persistentId, "./TMB/"+filename, json_str)
    if d.json()["status"] == "ERROR": 
        print(d.content)
        print("\n")
        continue

    # need to update filenames after uploading because of DataVerse bug
    # https://github.com/IQSS/dataverse/issues/7223
    file_id = d.json()['data']['files'][0]['dataFile']['id']
    d2 = api.update_datafile_metadata(file_id, json_str=json_str, is_filepid=False)
    # print(d2)

resp = api.publish_dataset(identifier, "major")
#+END_SRC

#+RESULTS:


curl -H X-Dataverse-key:$API_TOKEN -X POST -F "file=@$FILENAME" -F 'jsonData={"description":"My description.","directoryLabel":"data/subdir1","categories":["Data"], "restrict":"false"}' "$SERVER_URL/api/datasets/:persistentId/add?persistentId=$PERSISTENT_ID"

* Stats
** Annual GIS
:PROPERTIES:
:ID:       20210803T051000.448219
:END:

#+BEGIN_SRC jupyter-python
import xarray as xr

mb = xr.open_dataset('./TMB/mb_region.nc')\
       .resample({'time':'1D'})\
       .interpolate()\
       .resample({'time':'YS'})\
       .sum()

df_yr = mb[['mb','mb_err','smb','smb_err','mmb','mmb_err','bmb','bmb_err']].to_dataframe()

print("ALL:")
print(df_yr.describe())
print("1986:")
print(df_yr.loc['1986':].describe())
# "global" reported single-value uncertainty comes from post-1986 MB error: 86 Gt yr-1

df_yr.describe()
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
ALL:
               mb      mb_err         smb     smb_err         mmb     mmb_err         bmb  \
count  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000  182.000000   
mean   -80.794193  125.345689  366.788792   90.754311  423.725918   74.450666   23.857068   
std    117.399375   21.079649  100.536354   31.807693   31.837508   16.119800    1.516364   
min   -428.641023   58.316901   85.786621    7.720796  317.088041   32.445140   18.289144   
25%   -161.514548  122.666581  301.154640   94.593748  401.149700   75.120370   22.890307   
50%    -67.874620  133.392796  376.010763  104.156394  422.222093   80.927551   23.873308   
75%      2.098873  138.221561  432.958860  109.691269  442.061328   83.912730   24.716830   
max    137.293490  152.800869  578.714777  123.009131  495.130950   94.757858   29.355867   

          bmb_err  
count  182.000000  
mean    12.864916  
std      3.745037  
min      3.895276  
25%     13.886869  
50%     14.510833  
75%     14.920642  
max     16.293678  
1986:
               mb      mb_err         smb    smb_err         mmb    mmb_err        bmb    bmb_err
count   36.000000   36.000000   36.000000  36.000000   36.000000  36.000000  36.000000  36.000000
mean  -155.246210   86.402255  322.866865  29.058018  455.311455  43.356932  22.801620   5.443801
std    129.099646    9.520848  119.093502  10.718415   35.234490   3.784493   2.038575   0.325342
min   -428.641023   58.316901   85.786621   7.720796  317.088041  32.445140  18.289144   3.895276
25%   -240.945283   81.287165  253.622034  22.825983  433.927963  40.130277  21.839887   5.376715
50%   -167.780829   85.217834  328.052265  29.524704  458.924118  43.920115  22.451248   5.436764
75%    -76.507763   93.740967  404.418727  36.397685  484.441320  46.393970  23.639193   5.536868
max    137.267357  109.461721  578.338093  52.050428  495.130950  52.812337  29.355867   6.220920
#+end_example
|       |         mb |   mb_err |      smb |   smb_err |      mmb |   mmb_err |       bmb |   bmb_err |
|-------+------------+----------+----------+-----------+----------+-----------+-----------+-----------|
| count |  182       | 182      | 182      |  182      | 182      |  182      | 182       | 182       |
| mean  |  -80.7942  | 125.346  | 366.789  |   90.7543 | 423.726  |   74.4507 |  23.8571  |  12.8649  |
| std   |  117.399   |  21.0796 | 100.536  |   31.8077 |  31.8375 |   16.1198 |   1.51636 |   3.74504 |
| min   | -428.641   |  58.3169 |  85.7866 |    7.7208 | 317.088  |   32.4451 |  18.2891  |   3.89528 |
| 25%   | -161.515   | 122.667  | 301.155  |   94.5937 | 401.15   |   75.1204 |  22.8903  |  13.8869  |
| 50%   |  -67.8746  | 133.393  | 376.011  |  104.156  | 422.222  |   80.9276 |  23.8733  |  14.5108  |
| 75%   |    2.09887 | 138.222  | 432.959  |  109.691  | 442.061  |   83.9127 |  24.7168  |  14.9206  |
| max   |  137.293   | 152.801  | 578.715  |  123.009  | 495.131  |   94.7579 |  29.3559  |  16.2937  |
:END:

Years with min & max mb

#+BEGIN_SRC jupyter-python
df_yr = df_yr.loc['1986':]

print(df_yr.iloc[df_yr['mb'].argmin()])
print("")
print(df_yr.iloc[df_yr['mb'].argmax()])
print("")
#+END_SRC

#+RESULTS:
#+begin_example
mb        -250.578015
mb_err      94.662651
mb_std     126.861945
smb        260.996807
smb_err     23.489713
mmb        487.075598
mmb_err     46.711174
bmb         24.499224
bmb_err      5.643037
Name: 2010-01-01 00:00:00, dtype: float64

mb         -65.047037
mb_err      80.945405
mb_std     115.790916
smb        385.629517
smb_err     34.706657
mmb        429.023319
mmb_err     40.192044
bmb         21.653235
bmb_err      5.366423
Name: 1990-01-01 00:00:00, dtype: float64
#+end_example

** Decadal GIS
:PROPERTIES:
:ID:       20210813T110327.480906
:END:

#+BEGIN_SRC jupyter-python
import xarray as xr

mb = xr.open_dataset('./TMB/mb_region.nc')\
       .resample({'time':'1D'})\
       .interpolate()\
       .resample({'time':'AS'})\
       .sum()\
       .resample({'time':'10AS'})\
       .mean()\

mb['mb_std'] = xr.open_dataset('./TMB/mb_region.nc')\
                 .resample({'time':'1D'})\
                 .interpolate()\
                 .resample({'time':'AS'})\
                 .sum()\
                 .resample({'time':'10AS'})\
                 .std()['mb']

df_yr = mb[['mb','mb_err','mb_std','smb','smb_err','mmb','mmb_err','bmb','bmb_err']].to_dataframe().iloc[:-1]

print(df_yr[['mb','mb_std']])
df_yr.describe()
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
                    mb      mb_std
time                              
1840-01-01  -11.030299   87.561444
1850-01-01  -30.652297   65.479303
1860-01-01   49.973645   50.851681
1870-01-01  -13.250539   49.381461
1880-01-01   -4.313202   43.732115
1890-01-01  -10.546295   70.911020
1900-01-01 -121.277076   74.066356
1910-01-01  -34.811835   42.168373
1920-01-01 -128.474188   80.171286
1930-01-01 -203.014191   97.412829
1940-01-01  -87.131045  128.045423
1950-01-01 -134.316386   74.948904
1960-01-01 -169.459645   82.953600
1970-01-01   -7.553647   75.206271
1980-01-01  -32.340083  113.084801
1990-01-01  -65.047037  115.790916
2000-01-01 -175.144532   69.742113
2010-01-01 -250.578015  126.861945
#+end_example
|       |        mb |   mb_err |   mb_std |      smb |   smb_err |      mmb |   mmb_err |      bmb |   bmb_err |
|-------+-----------+----------+----------+----------+-----------+----------+-----------+----------+-----------|
| count |   18      |  18      |  18      |  18      |   18      |  18      |   18      | 18       |  18       |
| mean  |  -79.387  | 125.925  |  80.465  | 368.447  |   91.5452 | 423.946  |   74.8369 | 23.8879  |  12.9551  |
| std   |   83.0348 |  19.7917 |  26.8562 |  56.8732 |   30.5697 |  29.7031 |   15.5009 |  1.11574 |   3.59007 |
| min   | -250.578  |  80.9454 |  42.1684 | 260.997  |   23.4897 | 372.877  |   40.192  | 21.6532  |   5.36642 |
| 25%   | -132.856  | 124.686  |  66.545  | 316.294  |   96.5535 | 406.228  |   75.6907 | 23.1526  |  13.9899  |
| 50%   |  -49.9294 | 133.984  |  75.0776 | 391.848  |  104.868  | 418.442  |   81.4557 | 23.7927  |  14.5241  |
| 75%   |  -11.5854 | 138.089  |  94.95   | 411.47   |  108.939  | 439.636  |   83.5231 | 24.6875  |  14.9481  |
| max   |   49.9736 | 146.359  | 128.045  | 444.943  |  115.906  | 487.076  |   92.0417 | 25.9794  |  15.6157  |
:END:

Decades with min & max SMB

#+BEGIN_SRC jupyter-python
print(df_yr.iloc[df_yr['smb'].argmin()])
print("")
print(df_yr.iloc[df_yr['smb'].argmax()])
print("")
#+END_SRC

#+RESULTS:
#+begin_example
mb        -250.578015
mb_err      94.662651
mb_std     126.861945
smb        260.996807
smb_err     23.489713
mmb        487.075598
mmb_err     46.711174
bmb         24.499224
bmb_err      5.643037
Name: 2010-01-01 00:00:00, dtype: float64

mb          49.973645
mb_err     127.458220
mb_std      50.851681
smb        444.943400
smb_err    101.686285
mmb        372.876755
mmb_err     75.480586
bmb         22.092999
bmb_err     13.900711
Name: 1860-01-01 00:00:00, dtype: float64
#+end_example

Decades with min and max MMB

#+BEGIN_SRC jupyter-python
print(df_yr.iloc[df_yr['mmb'].argmin()])
print("")
print(df_yr.iloc[df_yr['mmb'].argmax()])
print("")
#+END_SRC

#+RESULTS:
#+begin_example
mb          49.973645
mb_err     127.458220
mb_std      50.851681
smb        444.943400
smb_err    101.686285
mmb        372.876755
mmb_err     75.480586
bmb         22.092999
bmb_err     13.900711
Name: 1860-01-01 00:00:00, dtype: float64

mb        -250.578015
mb_err      94.662651
mb_std     126.861945
smb        260.996807
smb_err     23.489713
mmb        487.075598
mmb_err     46.711174
bmb         24.499224
bmb_err      5.643037
Name: 2010-01-01 00:00:00, dtype: float64
#+end_example

Decades with min and max BMB

#+BEGIN_SRC jupyter-python
print(df_yr.iloc[df_yr['bmb'].argmin()])
print("")
print(df_yr.iloc[df_yr['bmb'].argmax()])
print("")
#+END_SRC

#+RESULTS:
#+begin_example
mb         -65.047037
mb_err      80.945405
mb_std     115.790916
smb        385.629517
smb_err     34.706657
mmb        429.023319
mmb_err     40.192044
bmb         21.653235
bmb_err      5.366423
Name: 1990-01-01 00:00:00, dtype: float64

mb        -203.014191
mb_err     146.359427
mb_std      97.412829
smb        289.245138
smb_err    112.658174
mmb        466.279963
mmb_err     92.041689
bmb         25.979365
bmb_err     15.615669
Name: 1930-01-01 00:00:00, dtype: float64
#+end_example


* READMEs
** Dataverse README.txt

#+BEGIN_SRC org :tangle README.txt
,* Greenland ice sheet mass balance from 1986 through next week

This is the data for "Greenland ice sheet mass balance from 1840 through next week" and previous and subsequent versions.

+ Paper: In preparation
+ Data: doi:10.22008/FK2/YG3IWC or https://doi.org/10.22008/FK2/YG3IWC
+ Code: https://github.com/GEUS-PROMICE/mass_balance

,* Early Access

+ Private URL for draft data: https://dataverse01.geus.dk/privateurl.xhtml?token=d09976c4-4f89-43ef-8f91-173d269806a4
+ Contact kdm (at) geus (dot) dk if interested
#+END_SRC

** GitHub README.org

See [[./README.org]]

* Environment
** Software

This project uses software - bash, GRASS, Python, etc. The python environment is reproducible if you have Conda installed. Below I provide the version of the software(s) used to create this document in order to support the goal of bit-matching reproducibility. 

*** Os installed
#+BEGIN_SRC bash :results table
for tool in gdal-bin parallel sed gawk netcdf-bin proj-bin nco cdo bash grass-gui datamash; do dpkg -l | grep "ii  ${tool} " | cut -c5-90; done| sort
#+END_SRC

#+RESULTS:
| bash       | 5.0-6ubuntu1.1     |
| cdo        | 1.9.9~rc1-1        |
| datamash   | 1.4-1              |
| gawk       | 1:5.0.1+dfsg-1     |
| gdal-bin   | 3.0.4+dfsg-1build3 |
| grass-gui  | 7.8.2-1build3      |
| nco        | 4.9.1-1build2      |
| netcdf-bin | 1:4.7.3-1          |
| parallel   | 20161222-1.1       |
| proj-bin   | 6.3.1-1            |
| sed        | 4.7-1              |


*** Org Mode
#+BEGIN_SRC emacs-lisp :eval no-export :exports both
(org-version nil t)
#+END_SRC

#+RESULTS:
: Org mode version 9.4.5 (9.4.5-16-g94be20-elpa @ /home/kdm/.emacs.d/elpa/org-20210412/)

*** Python

The code below produces [[./environment.yml]] when this file is exported. If that file exists, then =conda env create= and =source activate PROJECTNAME= will install all the python packages used in this document.

#+NAME: conda_env 
#+BEGIN_SRC bash :cmdline -i :results verbatim :eval no-export :exports both
conda env export --name TMB | cat | tee environment.yml
#+END_SRC

#+RESULTS: conda_env
#+begin_example
name: TMB
channels:
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_gnu
  - argon2-cffi=20.1.0=py38h497a2fe_2
  - async_generator=1.10=py_0
  - attrs=20.3.0=pyhd3deb0d_0
  - backcall=0.2.0=pyh9f0ad1d_0
  - backports=1.0=py_2
  - backports.functools_lru_cache=1.6.1=py_0
  - bleach=3.3.0=pyh44b312d_0
  - bokeh=2.3.0=py38h578d9bd_0
  - bzip2=1.0.8=h7f98852_4
  - c-ares=1.17.1=h7f98852_1
  - ca-certificates=2020.12.5=ha878542_0
  - certifi=2020.12.5=py38h578d9bd_1
  - cfchecker=4.0.0=py_0
  - cffi=1.14.4=py38ha312104_0
  - cftime=1.3.1=py38h5c078b8_0
  - cfunits=3.3.1=pyhd3deb0d_0
  - click=7.1.2=pyh9f0ad1d_0
  - cloudpickle=1.6.0=py_0
  - curl=7.75.0=h979ede3_0
  - cycler=0.10.0=py_2
  - cytoolz=0.11.0=py38h497a2fe_3
  - dask=2021.1.0=pyhd8ed1ab_0
  - dask-core=2021.1.0=pyhd8ed1ab_0
  - dbus=1.13.6=hfdff14a_1
  - decorator=4.4.2=py_0
  - defusedxml=0.7.1=pyhd8ed1ab_0
  - distributed=2021.1.1=py38h578d9bd_0
  - entrypoints=0.3=pyhd8ed1ab_1003
  - eofs=1.4.0=py_0
  - et_xmlfile=1.0.1=py_1001
  - expat=2.2.10=h9c3ff4c_0
  - fontconfig=2.13.1=hba837de_1004
  - freetype=2.10.4=h0708190_1
  - fsspec=0.8.7=pyhd8ed1ab_0
  - future=0.18.2=py38h578d9bd_3
  - gettext=0.19.8.1=hf34092f_1004
  - glib=2.66.2=h58526e2_0
  - gst-plugins-base=1.14.5=h0935bb2_2
  - gstreamer=1.14.5=h36ae1b5_2
  - hdf4=4.2.13=h10796ff_1004
  - hdf5=1.10.6=nompi_h6a2412b_1114
  - heapdict=1.0.1=py_0
  - icu=67.1=he1b5a44_0
  - importlib-metadata=3.7.3=py38h578d9bd_0
  - importlib_metadata=3.7.3=hd8ed1ab_0
  - importlib_resources=5.1.2=py38h578d9bd_0
  - ipykernel=5.5.0=py38h81c977d_1
  - ipython=7.21.0=py38h81c977d_0
  - ipython_genutils=0.2.0=py_1
  - ipywidgets=7.6.3=pyhd3deb0d_0
  - jdcal=1.4.1=py_0
  - jedi=0.18.0=py38h578d9bd_2
  - jinja2=2.11.3=pyh44b312d_0
  - jpeg=9d=h36c2ea0_0
  - jsonschema=3.2.0=pyhd8ed1ab_3
  - jupyter=1.0.0=py_2
  - jupyter_client=6.1.12=pyhd8ed1ab_0
  - jupyter_console=6.4.0=pyhd8ed1ab_0
  - jupyter_core=4.7.1=py38h578d9bd_0
  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0
  - jupyterlab_widgets=1.0.0=pyhd8ed1ab_1
  - kiwisolver=1.3.1=py38h1fd1430_1
  - krb5=1.17.2=h926e7f8_0
  - lcms2=2.12=hddcbb42_0
  - ld_impl_linux-64=2.35.1=hea4e1c9_2
  - libblas=3.9.0=8_openblas
  - libcblas=3.9.0=8_openblas
  - libclang=10.0.1=default_hde54327_1
  - libcurl=7.75.0=hc4aaa36_0
  - libedit=3.1.20191231=he28a2e2_2
  - libev=4.33=h516909a_1
  - libevent=2.1.10=hcdb4288_3
  - libffi=3.2.1=he1b5a44_1007
  - libgcc-ng=9.3.0=h2828fa1_18
  - libgfortran-ng=9.3.0=hff62375_18
  - libgfortran5=9.3.0=hff62375_18
  - libglib=2.66.2=hbe7bbb4_0
  - libgomp=9.3.0=h2828fa1_18
  - libiconv=1.16=h516909a_0
  - liblapack=3.9.0=8_openblas
  - libllvm10=10.0.1=he513fc3_3
  - libnetcdf=4.7.4=nompi_h56d31a8_107
  - libnghttp2=1.43.0=h812cca2_0
  - libopenblas=0.3.12=pthreads_h4812303_1
  - libpng=1.6.37=h21135ba_2
  - libpq=12.3=h255efa7_3
  - libsodium=1.0.18=h36c2ea0_1
  - libssh2=1.9.0=ha56f1ee_6
  - libstdcxx-ng=9.3.0=h6de172a_18
  - libtiff=4.2.0=hdc55705_0
  - libuuid=2.32.1=h7f98852_1000
  - libwebp-base=1.2.0=h7f98852_2
  - libxcb=1.13=h7f98852_1003
  - libxkbcommon=0.10.0=he1b5a44_0
  - libxml2=2.9.10=h68273f3_2
  - locket=0.2.0=py_2
  - lz4-c=1.9.3=h9c3ff4c_0
  - markupsafe=1.1.1=py38h497a2fe_3
  - matplotlib=3.3.4=py38h578d9bd_0
  - matplotlib-base=3.3.4=py38h0efea84_0
  - mistune=0.8.4=py38h497a2fe_1003
  - msgpack-python=1.0.2=py38h1fd1430_1
  - mysql-common=8.0.21=2
  - mysql-libs=8.0.21=hf3661c5_2
  - nbclient=0.5.3=pyhd8ed1ab_0
  - nbconvert=6.0.7=py38h578d9bd_3
  - nbformat=5.1.2=pyhd8ed1ab_1
  - ncurses=6.2=h58526e2_4
  - nest-asyncio=1.4.3=pyhd8ed1ab_0
  - netcdf4=1.5.6=nompi_py38h1cdf482_100
  - notebook=6.3.0=py38h578d9bd_0
  - nspr=4.30=h9c3ff4c_0
  - nss=3.63=hb5efdd6_0
  - numpy=1.20.1=py38h18fd61f_0
  - olefile=0.46=pyh9f0ad1d_1
  - openpyxl=3.0.6=pyhd8ed1ab_0
  - openssl=1.1.1j=h7f98852_0
  - packaging=20.9=pyh44b312d_0
  - pandas=1.2.1=py38h51da96c_0
  - pandoc=2.12=h7f98852_0
  - pandocfilters=1.4.2=py_1
  - parso=0.8.1=pyhd8ed1ab_0
  - partd=1.1.0=py_0
  - patsy=0.5.1=py_0
  - pcre=8.44=he1b5a44_0
  - pexpect=4.8.0=pyh9f0ad1d_2
  - pickleshare=0.7.5=py_1003
  - pillow=8.1.2=py38ha0e1e83_0
  - pint=0.16.1=py_0
  - pip=21.0.1=pyhd8ed1ab_0
  - prometheus_client=0.9.0=pyhd3deb0d_0
  - prompt-toolkit=3.0.18=pyha770c72_0
  - prompt_toolkit=3.0.18=hd8ed1ab_0
  - psutil=5.8.0=py38h497a2fe_1
  - pthread-stubs=0.4=h36c2ea0_1001
  - ptyprocess=0.7.0=pyhd3deb0d_0
  - pycparser=2.20=pyh9f0ad1d_2
  - pygments=2.8.1=pyhd8ed1ab_0
  - pyparsing=2.4.7=pyh9f0ad1d_0
  - pyqt=5.12.3=py38h578d9bd_7
  - pyqt-impl=5.12.3=py38h7400c14_7
  - pyqt5-sip=4.19.18=py38h709712a_7
  - pyqtchart=5.12=py38h7400c14_7
  - pyqtwebengine=5.12.1=py38h7400c14_7
  - pyrsistent=0.17.3=py38h497a2fe_2
  - python=3.8.6=h852b56e_0_cpython
  - python-dateutil=2.8.1=py_0
  - python_abi=3.8=1_cp38
  - pytz=2021.1=pyhd8ed1ab_0
  - pyyaml=5.4.1=py38h497a2fe_0
  - pyzmq=22.0.3=py38h2035c66_1
  - qt=5.12.9=h1f2b2cb_0
  - qtconsole=5.0.3=pyhd8ed1ab_0
  - qtpy=1.9.0=py_0
  - readline=8.0=he28a2e2_2
  - scipy=1.5.3=py38hb2138dd_0
  - seaborn=0.11.1=ha770c72_0
  - seaborn-base=0.11.1=pyhd8ed1ab_1
  - send2trash=1.5.0=py_0
  - setuptools=49.6.0=py38h578d9bd_3
  - six=1.15.0=pyh9f0ad1d_0
  - sortedcontainers=2.3.0=pyhd8ed1ab_0
  - sqlite=3.35.2=h74cdb3f_0
  - statsmodels=0.12.2=py38h5c078b8_0
  - tabulate=0.8.7=pyh9f0ad1d_0
  - tblib=1.7.0=pyhd8ed1ab_0
  - terminado=0.9.3=py38h578d9bd_0
  - testpath=0.4.4=py_0
  - tk=8.6.10=h21135ba_1
  - toolz=0.11.1=py_0
  - tornado=6.1=py38h497a2fe_1
  - traitlets=5.0.5=py_0
  - typing_extensions=3.7.4.3=py_0
  - udunits2=2.2.27.27=h360fe7b_0
  - uncertainties=3.1.4=py_0
  - wcwidth=0.2.5=pyh9f0ad1d_2
  - webencodings=0.5.1=py_1
  - wheel=0.36.2=pyhd3deb0d_0
  - widgetsnbextension=3.5.1=py38h578d9bd_4
  - xarray=0.16.2=pyhd8ed1ab_0
  - xorg-libxau=1.0.9=h7f98852_0
  - xorg-libxdmcp=1.1.3=h7f98852_0
  - xz=5.2.5=h516909a_1
  - yaml=0.2.5=h516909a_0
  - zeromq=4.3.4=h9c3ff4c_0
  - zict=2.0.0=py_0
  - zipp=3.4.1=pyhd8ed1ab_0
  - zlib=1.2.11=h516909a_1010
  - zstd=1.4.9=ha95c52a_0
  - pip:
    - bottleneck==1.3.2
    - chardet==4.0.0
    - idna==2.10
    - pydataverse==0.2.1
    - pyproj==3.0.0.post1
    - requests==2.25.1
    - urllib3==1.26.4
prefix: /home/kdm/local/miniconda3/envs/TMB

#+end_example

** Python
*** Anaconda environment

**** Create
#+BEGIN_SRC bash
env=TMB
conda create -n ${env} python=3.8 xarray pandas matplotlib jupyter tabulate pint uncertainties scipy cfchecker geopandas
conda activate ${env}
python -m ipykernel install --user --name=${env}

# anconda install matplotlib xarray rasterio simplekml 
# pip install uncertainties

#+END_SRC
**** Share

#+BEGIN_SRC bash :cmdline -i :results drawer verbatim :exports both
conda env export --name TMB | tee environment.yml
#+END_SRC

#+RESULTS:
:results:
name: TMB
channels:
  - conda-forge
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=1_gnu
  - argon2-cffi=20.1.0=py38h25fe258_2
  - async_generator=1.10=py_0
  - attrs=20.3.0=pyhd3deb0d_0
  - backcall=0.2.0=pyh9f0ad1d_0
  - backports=1.0=py_2
  - backports.functools_lru_cache=1.6.1=py_0
  - bleach=3.2.1=pyh9f0ad1d_0
  - bzip2=1.0.8=h516909a_3
  - c-ares=1.16.1=h516909a_3
  - ca-certificates=2020.11.8=ha878542_0
  - certifi=2020.11.8=py38h578d9bd_0
  - cfchecker=4.0.0=py_0
  - cffi=1.14.3=py38h1bdcb99_1
  - cftime=1.3.0=py38h0b5ebd8_0
  - cfunits=3.3.0=pyh9f0ad1d_0
  - curl=7.71.1=he644dc0_8
  - cycler=0.10.0=py_2
  - dbus=1.13.6=hfdff14a_1
  - decorator=4.4.2=py_0
  - defusedxml=0.6.0=py_0
  - entrypoints=0.3=pyhd8ed1ab_1003
  - expat=2.2.9=he1b5a44_2
  - fontconfig=2.13.1=h7e3eb15_1002
  - freetype=2.10.4=h7ca028e_0
  - future=0.18.2=py38h578d9bd_2
  - gettext=0.19.8.1=hf34092f_1004
  - glib=2.66.2=h58526e2_0
  - gst-plugins-base=1.14.5=h0935bb2_2
  - gstreamer=1.14.5=h36ae1b5_2
  - hdf4=4.2.13=hf30be14_1003
  - hdf5=1.10.6=nompi_h1022a3e_1110
  - icu=67.1=he1b5a44_0
  - importlib-metadata=2.0.0=py_1
  - importlib_metadata=2.0.0=1
  - importlib_resources=3.3.0=py38h578d9bd_0
  - ipykernel=5.3.4=py38h81c977d_1
  - ipython=7.19.0=py38h81c977d_0
  - ipython_genutils=0.2.0=py_1
  - ipywidgets=7.5.1=pyh9f0ad1d_1
  - jedi=0.17.2=py38h578d9bd_1
  - jinja2=2.11.2=pyh9f0ad1d_0
  - jpeg=9d=h36c2ea0_0
  - jsonschema=3.2.0=py_2
  - jupyter=1.0.0=py_2
  - jupyter_client=6.1.7=py_0
  - jupyter_console=6.2.0=py_0
  - jupyter_core=4.6.3=py38h578d9bd_2
  - jupyterlab_pygments=0.1.2=pyh9f0ad1d_0
  - kiwisolver=1.3.1=py38h82cb98a_0
  - krb5=1.17.1=hfafb76e_3
  - lcms2=2.11=hcbb858e_1
  - ld_impl_linux-64=2.35.1=hed1e6ac_0
  - libblas=3.9.0=2_openblas
  - libcblas=3.9.0=2_openblas
  - libclang=10.0.1=default_hde54327_1
  - libcurl=7.71.1=hcdd3856_8
  - libedit=3.1.20191231=he28a2e2_2
  - libev=4.33=h516909a_1
  - libevent=2.1.10=hcdb4288_3
  - libffi=3.2.1=he1b5a44_1007
  - libgcc-ng=9.3.0=h5dbcf3e_17
  - libgfortran-ng=9.3.0=he4bcb1c_17
  - libgfortran5=9.3.0=he4bcb1c_17
  - libglib=2.66.2=hbe7bbb4_0
  - libgomp=9.3.0=h5dbcf3e_17
  - libiconv=1.16=h516909a_0
  - liblapack=3.9.0=2_openblas
  - libllvm10=10.0.1=he513fc3_3
  - libnetcdf=4.7.4=nompi_hefab0ff_106
  - libnghttp2=1.41.0=h8cfc5f6_2
  - libopenblas=0.3.12=pthreads_h4812303_1
  - libpng=1.6.37=h21135ba_2
  - libpq=12.3=h5513abc_2
  - libsodium=1.0.18=h36c2ea0_1
  - libssh2=1.9.0=hab1572f_5
  - libstdcxx-ng=9.3.0=h2ae2ef3_17
  - libtiff=4.1.0=h4f3a223_6
  - libuuid=2.32.1=h14c3975_1000
  - libwebp-base=1.1.0=h36c2ea0_3
  - libxcb=1.13=h14c3975_1002
  - libxkbcommon=0.10.0=he1b5a44_0
  - libxml2=2.9.10=h68273f3_2
  - lz4-c=1.9.2=he1b5a44_3
  - markupsafe=1.1.1=py38h8df0ef7_2
  - matplotlib=3.3.3=py38h578d9bd_0
  - matplotlib-base=3.3.3=py38h5c7f4ab_0
  - mistune=0.8.4=py38h25fe258_1002
  - mysql-common=8.0.21=2
  - mysql-libs=8.0.21=hf3661c5_2
  - nbclient=0.5.1=py_0
  - nbconvert=6.0.7=py38h578d9bd_3
  - nbformat=5.0.8=py_0
  - ncurses=6.2=h58526e2_4
  - nest-asyncio=1.4.3=pyhd8ed1ab_0
  - netcdf4=1.5.4=nompi_py38hec8b9af_103
  - notebook=6.1.5=py38h578d9bd_0
  - nspr=4.29=he1b5a44_1
  - nss=3.59=h2c00c37_0
  - numpy=1.19.4=py38hf0fd68c_1
  - olefile=0.46=pyh9f0ad1d_1
  - openssl=1.1.1h=h516909a_0
  - packaging=20.4=pyh9f0ad1d_0
  - pandas=1.1.4=py38h0ef3d22_0
  - pandoc=2.11.1.1=h36c2ea0_0
  - pandocfilters=1.4.2=py_1
  - parso=0.7.1=pyh9f0ad1d_0
  - pcre=8.44=he1b5a44_0
  - pexpect=4.8.0=pyh9f0ad1d_2
  - pickleshare=0.7.5=py_1003
  - pillow=8.0.1=py38h70fbd49_0
  - pint=0.16.1=py_0
  - pip=20.2.4=py_0
  - prometheus_client=0.9.0=pyhd3deb0d_0
  - prompt-toolkit=3.0.8=pyha770c72_0
  - prompt_toolkit=3.0.8=hd8ed1ab_0
  - pthread-stubs=0.4=h14c3975_1001
  - ptyprocess=0.6.0=py_1001
  - pycparser=2.20=pyh9f0ad1d_2
  - pygments=2.7.2=py_0
  - pyparsing=2.4.7=pyh9f0ad1d_0
  - pyqt=5.12.3=py38ha8c2ead_4
  - pyrsistent=0.17.3=py38h25fe258_1
  - python=3.8.6=h852b56e_0_cpython
  - python-dateutil=2.8.1=py_0
  - python_abi=3.8=1_cp38
  - pytz=2020.4=pyhd8ed1ab_0
  - pyzmq=20.0.0=py38h1d1b12f_1
  - qt=5.12.9=h1f2b2cb_0
  - qtconsole=4.7.7=pyh9f0ad1d_0
  - qtpy=1.9.0=py_0
  - readline=8.0=he28a2e2_2
  - scipy=1.5.3=py38hb2138dd_0
  - send2trash=1.5.0=py_0
  - setuptools=49.6.0=py38h924ce5b_2
  - six=1.15.0=pyh9f0ad1d_0
  - sqlite=3.33.0=h4cf870e_1
  - tabulate=0.8.7=pyh9f0ad1d_0
  - terminado=0.9.1=py38h32f6830_1
  - testpath=0.4.4=py_0
  - tk=8.6.10=hed695b0_1
  - tornado=6.1=py38h25fe258_0
  - traitlets=5.0.5=py_0
  - udunits2=2.2.27.6=h4e0c4b3_1001
  - uncertainties=3.1.4=py_0
  - wcwidth=0.2.5=pyh9f0ad1d_2
  - webencodings=0.5.1=py_1
  - wheel=0.35.1=pyh9f0ad1d_0
  - widgetsnbextension=3.5.1=py38h578d9bd_4
  - xarray=0.16.1=py_0
  - xorg-libxau=1.0.9=h14c3975_0
  - xorg-libxdmcp=1.1.3=h516909a_0
  - xz=5.2.5=h516909a_1
  - zeromq=4.3.3=h58526e2_2
  - zipp=3.4.0=py_0
  - zlib=1.2.11=h516909a_1010
  - zstd=1.4.5=h6597ccf_2
  - pip:
    - pyqt5-sip==4.19.18
    - pyqtchart==5.12
    - pyqtwebengine==5.12.1
prefix: /home/kdm/local/miniconda3/envs/TMB

:end:

*** Packages
#+NAME: init_py
#+BEGIN_SRC jupyter-python
import numpy as np
import pandas as pd
import xarray as xr
#+END_SRC

#+RESULTS: init

*** Graphics
#+NAME: init_graphics
#+BEGIN_SRC jupyter-python
import matplotlib.pyplot as plt

from matplotlib import rc
rc('font', size=12)
rc('text', usetex=False)
matplotlib.pyplot.xkcd()
#+END_SRC

#+RESULTS: init_graphics

*** Data Dir

+ I set =DATADIR= as a =bash= environment variable in my login scripts.
+ This is so that Python babel blocks can also easily get that property.

#+NAME: get_DATADIR
#+BEGIN_SRC jupyter-python
import os
DATADIR = os.environ['DATADIR']
#+END_SRC

Example:
#+BEGIN_SRC jupyter-python :tangle no
<<get_DATADIR>>
print(DATADIR)
#+END_SRC

** Bash
#+NAME: init_bash
#+BEGIN_SRC bash :results verbatim
set -o nounset
set -o pipefail

# set -o errexit

### uncomment the above line when doing initial run. When rerunning and
### counting on GRASS failing w/ overwrite issues (speed increase), the
### line above must be commented

red='\033[0;31m'; orange='\033[0;33m'; green='\033[0;32m'; nc='\033[0m' # No Color
log_info() { echo -e "${green}[$(date --iso-8601=seconds)] [INFO] ${@}${nc}"; }
log_warn() { echo -e "${orange}[$(date --iso-8601=seconds)] [WARN] ${@}${nc}"; }
log_err() { echo -e "${red}[$(date --iso-8601=seconds)] [ERR] ${@}${nc}" >&2; }

# trap ctrl_c INT # trap ctrl-c and call ctrl_c()
# ctrl_c() { log_err "CTRL-C. Cleaning up"; }

debug() { if [[ debug:- == 1 ]]; then log_warn "debug:"; echo $@; fi; }

#+END_SRC

** GRASS config

https://grass.osgeo.org/grass74/manuals/variables.html

| GRASS_VERBOSE |                                                                |
|---------------+----------------------------------------------------------------|
|            -1 | complete silence (also errors and warnings are discarded)      |
|             0 | only errors and warnings are printed                           |
|             1 | progress and important messages are printed (percent complete) |
|             2 | all module messages are printed                                |
|             3 | additional verbose messages are printed                        |

#+NAME: init_grass
#+BEGIN_SRC bash :results verbatim :tangle no
export GRASS_VERBOSE=3
# export GRASS_MESSAGE_FORMAT=silent

if [ -z ${DATADIR+x} ]; then
    echo "DATADIR environment varible is unset."
    echo "Fix with: \"export DATADIR=/path/to/data\""
    exit 255
fi

set -x # print commands to STDOUT before running them
#+END_SRC

* Manuscript
** BibTeX library

#+BEGIN_SRC bash
bibexport ms > library.bib
#+END_SRC

Then switch from system to local library.bib in ms.org

** Copernicus LaTeX setup
#+NAME: copernicus-latex-setup
#+BEGIN_SRC emacs-lisp :results none :eval no-export
(add-to-list 'org-latex-classes
               `("copernicus"
                 "\\documentclass{copernicus}
               [NO-DEFAULT-PACKAGES]
               [NO-PACKAGES]
               [EXTRA]"
                 ("\\section{%s}" . "\\section*{%s}")
                 ("\\subsection{%s}" . "\\subsection*{%s}")
                 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
                 ("\\paragraph{%s}" . "\\paragraph*{%s}")
                 ("\\subparagraph{%s}" . "\\subparagraph*{%s}"))
               )

(setq-local org-latex-title-command "")
#+END_SRC


* Makefile
:PROPERTIES:
:CUSTOM_ID: sec:makefile
:END:

This code, and all code files in this project, are derived products tangled from the sob.org source file.

#+BEGIN_SRC makefile :tangle Makefile :tangle-mode (identity #o444)
PROMICE_MB: all
# dist

all: FORCE
	mkdir -p tmp dat

	# set up
	grass -e -c EPSG:4326 G_HIRHAM
	grass ./G_HIRHAM/PERMANENT/ --exec ./HIRHAM.sh
	
	grass -e -c EPSG:3413 G_MAR
	grass ./G_MAR/PERMANENT --exec ./MAR.sh

	grass -e -c EPSG:3413 G_RACMO
	grass ./G_RACMO/PERMANENT --exec ./RACMO.sh

	# BMB setup on the MAR grid
	grass ./G_MAR/PERMANENT --exec ./BMB.sh
	
	make SMB
	make BMB
	make dist

SMB: FORCE
	# partition RCM by Zwally sectors, Mouginot basins, and Mouginot regions
	grass ./G_HIRHAM/PERMANENT --exec ./SMB_HIRHAM_ROI.sh
	grass ./G_MAR/PERMANENT --exec ./SMB_MAR_ROI.sh
	grass ./G_RACMO/PERMANENT --exec ./SMB_RACMO_ROI.sh
	./smb_merge.sh
	python ./smb_bsv2nc.py

BMB: FORCE
	grass ./G_MAR/PERMANENT --exec ./BMB_MAR.sh
	./bmb_merge.sh
	python ./bmb_bsv2nc.py

update: FORCE
	# remove previously forecasted MAR 
	for n in $$(seq -10 10); do d=$$(date --date="$${n} days ago" --iso-8601); rm -f ./tmp/MAR/*_$${d}.bsv; done
	OP=true	make SMB
	for n in $$(seq -10 10); do d=$$(date --date="$${n} days ago" --iso-8601); rm -f ./tmp/BMB/*_$${d}.bsv; done
	OP=true	make BMB
	make dist


validate: FORCE
	grass -e -c EPSG:3413 G
	
dist: FORCE
	mkdir -p TMB
	# create end-user data product
	python ./build_TMB_nc.py
	# python ./upload_to_DV.py

FORCE: # dummy target

clean_all:
	rm -fR G G_RACMO G_HIRHAM G_MAR G_tmp tmp dat PROMICE_MB

clean_SMB:
	rm -fR tmp/HIRHAM tmp/MAR tmp/RACMO 
	
#+END_SRC


